%! TeX root = /Users/trustinnguyen/Downloads/Berkeley/Compsci/CS189/Homework/CS189Hw2/tex/CS189HW2.tex

\documentclass{article}
\usepackage{/Users/trustinnguyen/.mystyle/math/packages/mypackages}
\usepackage{/Users/trustinnguyen/.mystyle/math/commands/mycommands}
\usepackage{/Users/trustinnguyen/.mystyle/math/environments/article}
\graphicspath{{./figures/}}

\title{CS189Hw2}
\author{Trustin Nguyen}

\begin{document}

    \maketitle

\reversemarginpar

\section*{Multivariate Gaussians: A review}
\hrule

Multivariate Gaussian distributions crop up everywhere in machine learning, from priors on model parameters to assumptions on noise distributions. Being able to manipulate multivariate Gaussians also becomes important for analyzing correlations in data and preprocessing it for better regression and classification. We want to make sure to first cover the MVG fundamentals here.

Note that the probability density function of a non-degenerate (i.e. the covariance matrix is positive definite and, thus, invertible) multivariate Gaussian RV with mean vector, $\mu \in \mathbb{R}^{2}$, and covariance matrix, $\Sigma \in \mathbb{R}^{2 \times 2}$, is:
    \begin{equation*}
        f(z) = \dfrac{1}{\sqrt{(2\pi)^{2}\lvert \Sigma \rvert}}\exp\left(-\dfrac{1}{2}(z - \mu)^{T} \Sigma^{-1} (z - \mu)\right)
    \end{equation*}
    \begin{itemize}
        \item [(a)] Consider a two dimensional, zero mean random variable $Z = \begin{bmatrix}
            Z_{1} & Z_{2}   
        \end{bmatrix}^{T} \in \mathbb{R}^{2}$. In order for the random variable to be jointly Gaussian, a necessary and sufficient condition which we call the \textit{first characterization} is that
            \begin{itemize}
                \item $Z_{1}$ and $Z_{2}$ are each marginally Gaussian, and

                \item $Z_{1} \mid Z_{2} = z$ is Gaussian, and $Z_{2} \mid Z_{1} = z$ is Gaussian.
            \end{itemize}
        A \textit{second characterization} of a jointly Gaussian zero mean RV $Z \in \mathbb{R}^{2}$ is that it can be written as $Z = AX$, where $X \in \mathbb{R}^{2}$ is a collection of i.i.d. standard normal RVs and $A \in \mathbb{R}^{2 \times 2}$ is a matrix.

        Let $X_{1}$ and $X_{2}$ be i.i.d. standard normal RVs. Let $U$ denote a binary random variable uniformly that is equal to $1$ with probability $\frac{1}{2}$ and $-1$ with probability $\frac{1}{2}$, independent of everything else.

        For each of the below subproblems, complete the following \textit{two} steps: (1) Using one of the characterizations given above, determine whether the RVs are jointly Gaussian. If using the second characterization, clearly specify the $A$ matrix. (2) Calculate the covariance matrix of $Z$ (regardless of whether the RVs are jointly Gaussian or not).
            \begin{itemize}
                \item [(i.)] $Z_{1} = X_{1}$ and $Z_{2} = X_{2}$
                    \begin{answer}
                        I will be using the first characterization. To show that $Z_{1}$ and $Z_{2}$ are marginally Gaussian, we need to show that:
                            \begin{equation*}
                                p_{Z_{2}}(z) = \int_{-\infty}^{\infty} p(Z_{1} = z^{\prime}, Z_{2} = z) \, \dd{z^{\prime}}
                            \end{equation*}
                        is Gaussian. Since the RVs are independent:
                            \begin{align*}
                                p_{Z_{2}}(z) &= \int_{-\infty}^{\infty} p(Z_{1} = z^{\prime})p(Z_{2} = z) \, \dd{z^{\prime}}  \\
                                             &= p(Z_{2} = z) \int_{-\infty}^{\infty} p(Z_{1} = z^{\prime}) \, \dd{z^{\prime}} \\
                                             &= p(Z_{2} = z)                                                                    
                            \end{align*}
                        so $p_{Z_{2}}$ is Gaussian. The other way is symmetric.

                        To see that $Z_{1} \mid Z_{2} = z$ is Gaussian, we have:
                            \begin{equation*}
                                p(Z_{1} = z_{1} \mid Z_{2} = z) = \dfrac{p(Z_{1} = z_{1}, Z_{2} = z)}{p(Z_{2} = z)}
                            \end{equation*}
                        Since they are independent:
                            \begin{equation*}
                                p(Z_{1} = z_{1} \mid Z_{2} = z) = \dfrac{p(Z_{1} = z_{1})p(Z_{2} = z)}{p(Z_{2} = z)} = p(Z_{1} = z_{1})
                            \end{equation*}
                        So the distribution of $Z_{1} \mid Z_{2} = z$ is the same as that of $Z_{1}$, and is therefore Gaussian. The other case is symmetric.

                        The covariance matrix of $Z$ is the variance of $Z_{1}$ along the diagonal because the distributions are independent.
                    \end{answer}

                \item [(ii.)] $Z_{1} = X_{1}$ and $Z_{2} = X_{1} + 2X_{2}$. If using the first characterization, assume that you already know $(Z_{1} \mid Z_{2} = z)$ is Gaussian.
                    \begin{answer}
                        It is jointly Gaussian because 
                            \begin{equation*}
                                \begin{bmatrix}
                                    Z_{1} \\
                                    Z_{2}   
                                \end{bmatrix} = \begin{bmatrix}
                                    1 & 0 \\
                                    1 & 2   
                                \end{bmatrix} \begin{bmatrix}
                                    X_{1} \\
                                    X_{2}   
                                \end{bmatrix}
                            \end{equation*}
                        by the second classification. From lecture, the covariance matrix is given by $AA^{T}$.
                    \end{answer}

                \item [(iii.)] $Z_{1} = X_{1}$ and $Z_{2} = -X_{1}$.
                    \begin{answer}
                        Yes these are joint Gaussian because $Z = \begin{bmatrix}
                            1  & 0 \\
                            -1 & 0   
                        \end{bmatrix} \begin{bmatrix}
                            X_{1} \\
                            X_{2}   
                        \end{bmatrix}$. Same covariance matrix in the previous question: $AA^{T}$.
                    \end{answer}

                \item [(iv.)] $Z_{1} = X_{1}$ and $Z_{2} = UX_{1}$.
                    \begin{answer}
                        We will use the first characterization here. To show marginal Gaussian:
                            \begin{equation*}
                            p_{X_{1}}(z) = p(X_{1} = z) = \int_{-\infty}^{\infty} p(X_{1} = z, UX_{1} = z^{\prime}) \, \dd{z^{\prime}}
                            \end{equation*}
                        must be Gaussian. We know that it has non-zero values when $z^{\prime} = \pm z$:
                            \begin{equation*}
                                p_{X_{1}}(z) = \sum_{z^{\prime} = \pm z} p(X_{1} = z, UX_{1} = z^{\prime})
                            \end{equation*}
                        We can expand:
                            \begin{equation*}
                                p(X_{1} = z, UX_{1} = z) + p(X_{1} = z, UX_{1} = -z) = p(UX_{1} = z)p(U = 1) + p(X_{1} = z) p(U = -1)
                            \end{equation*}
                        we know that:
                            \begin{equation*}
                                p(U = \pm 1) = 1 / 2
                            \end{equation*}
                        So this turns out to just $p(X_{1} = z)$. So $p_{X_{1}}(z)$ is gaussian.

                        Now for the other marginal:
                            \begin{equation*}
                                p_{UX_{1}}(z) = p(UX_{1} = z) = \int_{-\infty}^{\infty} p(X_{1} = z^{\prime}, UX_{1} = z) \, \dd{z^{\prime}}
                            \end{equation*}
                        Again, nonzero when $X_{1} = \pm z$:
                            \begin{equation*}
                                \sum_{z^{\prime} = \pm z} p(X_{1} = z^{\prime}, UX_{1} = z) = p(X_{1} = z)p(U = 1) + p(X_{1} = -z)p(U = -1)
                            \end{equation*}
                        which is
                            \begin{equation*}
                                 \dfrac{1}{2}(p(X_{1} = z) + p(X_{1} = -z))
                            \end{equation*}
                        Since $X_{1}$ is standard normal, this becomes $p(X_{1} = z)$ which is normal.

                        Now on to the conditional probabilities. We have
                            \begin{equation*}
                                p(X_{1} = x \mid UX_{1} = x^{\prime}) = \dfrac{p(X_{1} = x, UX_{1} = x^{\prime})}{p(UX_{1} = x^{\prime})}
                            \end{equation*}
                        expand:
                            \begin{equation*}
                                \dfrac{p(X_{1} = x, UX_{1} = x^{\prime}) + p(X_{1} = x, UX_{1} = -x^{\prime})}{p(X_{1} = x^{\prime})(p(U = -1) + p(U = 1))}
                            \end{equation*}
                        which is
                            \begin{equation*}
                                \dfrac{p(X_{1} = x)p(X_{1} = x^{\prime})}{p(X_{1} = x^{\prime})} = p(X_{1} = x)
                            \end{equation*}
                        which is gaussian.

                        For the other conditional:
                            \begin{equation*}
                                p(UX_{1} = x \mid X_{1} = x^{\prime}) = \dfrac{p(UX_{1} = x, X_{1} = x^{\prime})}{p(X_{1} = x^{\prime})}
                            \end{equation*}
                        expand:
                            \begin{equation*}
                                \dfrac{p(X_{1} = x^{\prime})(p(UX_{1} = -x) + p(UX_{1} = x))}{p(X_{1} = x^{\prime})} = p(UX_{1} = x) + p(UX_{1} = -x)
                            \end{equation*}
                        so this is gaussian also.

                        Now for the covariance, 
                            \begin{equation*}
                                Cov(UX, X) = \mathbb{E}[(UX - \mathbb{E}(UX))(X)]
                            \end{equation*}
                        or
                            \begin{equation*}
                                Cov(UX, X) = \mathbb{E}[UX^{2} - X] = -\mathbb{E}[X] = 0
                            \end{equation*}
                        So the matrix is 
                            \begin{equation*}
                                \begin{bmatrix}
                                    1 & 0       \\
                                    0 & Var(UX)   
                                \end{bmatrix}
                            \end{equation*}
                        and
                            \begin{equation*}
                                Var(UX) = \mathbb{E}[U^{2}X^{2}] - \mathbb{E}[UX]^{2} = \mathbb{E}[U^{2}X^{2}] = \mathbb{E}[U^{2}]\mathbb{E}[X^{2}] = \mathbb{E}[X^{2}] = 1
                            \end{equation*}
                    \end{answer}
            \end{itemize}

        \item [(b)] Show that two Gaussian random variables can be uncorrelated, but not independent. On the other hand, show that two uncorrelated, jointly Gaussian RVs are independent.
            \begin{answer}
                In part $(d)$ of last example, we computed a covariance matrix of $\begin{bmatrix}
                    1 & 0 \\
                    0 & 1   
                \end{bmatrix}$ but $UX, X$ are not independent. So we see that non correlation does not mean independence.

                If two Gaussians are uncorrelated, then from the joint pdf:
                    \begin{equation*}
                        p_{Z_{1}, Z_{2}}(z) = \dfrac{1}{\sqrt{2\pi\lvert \Sigma \rvert}}\exp\left\{-\dfrac{1}{2}(z - \mu)^{T}\Sigma^{-1}(z - \mu)\right\}
                    \end{equation*}
                $\Sigma$ is a diagonal matrix, so the exponent part factors into
                    \begin{equation*}
                        Var(z_{1}) \cdot (z_{1} - \mu_{1})^{2} + Var(z_{2}) \cdot (z_{2} - \mu_{2})^{2}
                    \end{equation*}
                The determinant in the denominator also factors into: $Var(z_{1})Var(z_{2})$. So the joint pdf factors into their marginal distributions:
                    \begin{equation*}
                        p_{Z_{1}, Z_{2}}(z) = \dfrac{1}{\sqrt{2\pi}\sigma_{1}} \exp\left\{-\dfrac{1}{2}\left(\dfrac{z_{1} - \mu_{1}}{\sigma_{1}}\right)^{2}\right\}\dfrac{1}{\sqrt{2\pi}\sigma_{2}} \exp\left\{-\dfrac{1}{2}\left(\dfrac{z_{2} - \mu_{2}}{\sigma_{2}}\right)^{2}\right\}
                    \end{equation*}
            \end{answer}

        \item [(c)] With the setup in $(a)$, let $Z = VX$, where $V \in \mathbb{R}^{2 \times 2}$, and $Z, X \in \mathbb{R}^{2}$. What is the covariance matrix $\Sigma_{Z}$? If $X$ is not a multivariate Gaussian but has the identity matrix $I \in \mathbb{R}^{2 \times 2}$ as its covariance matrix, is your computed $\Sigma_{Z}$ still the covariance of $Z$?
            \begin{answer}
                Recall from the previous homework that if $Z = AX + b$, then 
                    \begin{equation*}
                        Var(Z) = A \Sigma A^{T}
                    \end{equation*}
                where $\Sigma$ is the covariance matrix of $X$. Then the covariance matrix of $Z$ is 
                    \begin{equation*}
                        V \Sigma V^{T}
                    \end{equation*}
                Here, $\Sigma$ is the identity because $X$ is standard normal. This also implies that if $X$ has an identity matrix as the covariance matrix, the computed covariance does not change.
            \end{answer}

        \item [(d)] Given a jointly Gaussian zero mean RV $Z = \begin{bmatrix}
            Z_{1} & Z_{2}   
        \end{bmatrix}^{T} \in \mathbb{R}^{2}$ with covariance matrix $\Sigma_{Z} = \begin{bmatrix}
            \Sigma_{11} & \Sigma_{12} \\
            \Sigma_{12} & \Sigma_{22}   
        \end{bmatrix}$, derive the conditional distribution of $(Z_{1} \mid Z_{2} = z)$.

        \textit{Hint}: The following identity may be useful
            \begin{equation*}
                \begin{bmatrix}
                    a & b \\
                    b & c   
                \end{bmatrix}^{-1} = \begin{bmatrix}
                    1             & 0 \\
                    -\frac{b}{c} & 1   
                \end{bmatrix} \begin{bmatrix}
                    \left(a - \frac{b^{2}}{c}\right)^{-1} & 0            \\
                    0                                      & \frac{1}{c}   
                \end{bmatrix} \begin{bmatrix}
                    1 & -\frac{b}{c} \\
                    0 & 1               
                \end{bmatrix}
            \end{equation*}
            \begin{answer}
                The joint pdf is:
                    \begin{equation*}
                        \dfrac{1}{\sqrt{2\pi\lvert \Sigma \rvert}} \exp\left\{-\dfrac{1}{2}(Z - \mu)^{T}\Sigma^{-1}(Z - \mu)\right\}
                    \end{equation*}
                And from Bayes's rule:
                    \begin{equation*}
                        p_{Z_{1}}(Z_{1} = z_{1}, Z_{2} = z) = p(Z_{1} \mid Z_{2} = z) p(Z_{2} = z)
                    \end{equation*}
                What we want to solve for is $p(Z_{1} \mid Z_{2} = z)$ on the RHS. So the LHS is:
                    \begin{align*}
                        \text{LHS} &= \dfrac{1}{\sqrt{2\pi\left\lvert \begin{bmatrix}
                            \Sigma_{11} & \Sigma_{12} \\
                            \Sigma_{12} & \Sigma_{22}   
                        \end{bmatrix} \right\rvert}}\exp\left\{-\dfrac{1}{2}\begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix} \begin{bmatrix}
                            \Sigma_{11} & \Sigma_{12} \\
                            \Sigma_{12} & \Sigma_{22}   
                        \end{bmatrix}^{-1}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix}\right\}                                                         \\
                                   &= \dfrac{1}{\sqrt{2\pi(\Sigma_{11}\Sigma_{22} - \Sigma_{12}^{2})}}\exp\left\{-\dfrac{1}{2}\begin{bmatrix}
                                       z_{1} & z   
                                   \end{bmatrix}\begin{bmatrix}
                                       1                                & 0 \\
                                       -\frac{\Sigma_{12}}{\Sigma_{22}} & 1   
                                   \end{bmatrix}\begin{bmatrix}
                                       \frac{\Sigma_{22}}{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^{2}} & 0                     \\
                                       0                                                            & \frac{1}{\Sigma_{22}}   
                                   \end{bmatrix}\begin{bmatrix}
                                       1 & -\frac{\Sigma_{12}}{\Sigma_{22}} \\
                                       0 & 1                                  
                                   \end{bmatrix}\begin{bmatrix}
                                       z_{1} \\
                                       z       
                                   \end{bmatrix}\right\}   
                    \end{align*}
                Recall that we still had the $p(Z_{2} = z)$ term on the RHS which is:
                    \begin{equation*}
                        p(Z_{2} = z) = \dfrac{1}{\sqrt{2\pi\Sigma_{22}}}\exp\left\{-\dfrac{1}{2}\begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            0 & 0                     \\
                            0 & \frac{1}{\Sigma_{22}}   
                        \end{bmatrix}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix}\right\}
                    \end{equation*}
                So when we take $p_{Z_{1}}(Z_{1} = z_{1}, Z_{2} = z) / p(Z_{2} = z)$, we would end up subtracting the exponents in the $\exp()$ term:
                    \begin{align*}
                        \begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            1                                & 0 \\
                            -\frac{\Sigma_{12}}{\Sigma_{22}} & 1   
                        \end{bmatrix}\begin{bmatrix}
                            \frac{\Sigma_{22}}{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^{2}} & 0                     \\
                            0                                                            & \frac{1}{\Sigma_{22}}   
                        \end{bmatrix}\begin{bmatrix}
                            1 & -\frac{\Sigma_{12}}{\Sigma_{22}} \\
                            0 & 1                                  
                        \end{bmatrix}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix} - \begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            0 & 0                     \\
                            0 & \frac{1}{\Sigma_{22}}   
                        \end{bmatrix}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix}
                    \end{align*}
                and notice that:
                    \begin{equation*}
                        \begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            0 & 0                     \\
                            0 & \frac{1}{\Sigma_{22}}   
                        \end{bmatrix}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix} = \begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            1                                & 0 \\
                            -\frac{\Sigma_{12}}{\Sigma_{22}} & 1   
                        \end{bmatrix}\begin{bmatrix}
                            0 & 0                     \\
                            0                                                            & \frac{1}{\Sigma_{22}}   
                        \end{bmatrix}\begin{bmatrix}
                            1 & -\frac{\Sigma_{12}}{\Sigma_{22}} \\
                            0 & 1                                  
                        \end{bmatrix}
                    \end{equation*}
                So subtracting yields:
                    \begin{equation*}
                        \begin{bmatrix}
                            z_{1} & z   
                        \end{bmatrix}\begin{bmatrix}
                            1                                & 0 \\
                            -\frac{\Sigma_{12}}{\Sigma_{22}} & 1   
                        \end{bmatrix}\begin{bmatrix}
                            \frac{\Sigma_{22}}{\Sigma_{11}\Sigma_{22} - \Sigma_{12}^{2}} & 0 \\
                            0                                                            & 0   
                        \end{bmatrix}\begin{bmatrix}
                            1 & -\frac{\Sigma_{12}}{\Sigma_{22}} \\
                            0 & 1                                  
                        \end{bmatrix}\begin{bmatrix}
                            z_{1} \\
                            z       
                        \end{bmatrix}
                    \end{equation*}
                We can simplify the matrix in the middle down to:
                    \begin{equation*}
                        \begin{bmatrix}
                            \frac{\Sigma_{22}}{\lvert \Sigma \rvert}                                 & -\frac{\Sigma_{12}}{\lvert \Sigma \rvert}        \\
                            -\frac{\Sigma_{12}}{\lvert \Sigma \rvert} & \frac{\Sigma_{12}^{2}}{\Sigma_{22}\lvert \Sigma \rvert}   
                        \end{bmatrix}
                    \end{equation*}
                The rest is just algebraic manipulation.
            \end{answer}
    \end{itemize}

\newpage
\section*{Projections and Linear Regression}
\hrule

We are given $X \in \mathbb{R}^{n \times d}$ where $n > d$ and $rank(X) = d$. We are also given a vector $y \in \mathbb{R}^{n}$. Define the orthogonal projection of $y$ onto $range(X) $ as $P_{range(X)}(y)$.

\textbf{\textit{Background on orthogonal projections}}: For any finite-dimensional subspace $W$ (here, $range(X)$) of a vector space $V$ (here, $\mathbb{R}^{n}$), any vector $v \in V$ can be decomposed as
    \begin{equation*}
        v = w + u, \, w \in W, \, u \in W^{\perp}
    \end{equation*}
where $W^{\perp}$ is the orthogonal complement of $W$. Furthermore, this decomposition is unique: if $v = w^{\prime} + u^{\prime}$ where $w^{\prime} \in W, u^{\prime} \in W^{\perp}$, then $w^{\prime} = w$ and $u^{\prime} = u$. These two facts allow us to define $P_{W}$, the orthogonal projection operator onto $W$. Given a vector $v$ with decomposition $v = w + u$, we define
    \begin{equation*}
        P_{W}(v) = w.
    \end{equation*}
It can also be shown using these two facts that $P_{W}$ is linear.
    \begin{itemize}
        \item [(a)] Prove that $P_{range(X)}(y) = \mathop{argmin}_{w \in \mathop{range}(X)} \lVert y - w \rVert^{2}_{2}$
            \begin{answer}
                We know that $y = x + w$ for $x \in \text{range}(X)$ and $w \in X^{\perp}$. So the equation becomes:
                    \begin{equation*}
                        \mathop{argmin}_{x^{\prime} \in \mathop{range}(X)} \lVert x + w - x^{\prime} \rVert^{2}_{2}
                    \end{equation*}
                The norm is:
                    \begin{equation*}
                        (x + w - x^{\prime}) \cdot (x + w - x^{\prime})
                    \end{equation*}
                a dot product:
                    \begin{align*}
                        ((x - x^{\prime}) + w) \cdot ((x - x^{\prime}) + w) &= (x - x^{\prime})\cdot (x - x^{\prime}) + 2 (w \cdot (x - x^{\prime})) + w \cdot w \\
                                                                            &= \lVert x - x^{\prime} \rVert + \lVert w \rVert                                      
                    \end{align*}
                We want the $x^{\prime}$ which minimizes this. So $x^{\prime} = x = P_{\mathop{range}(X)}(y)$.
            \end{answer}

        \item [(b)] An orthogonal projections is a linear transformation. That is, $P_{\mathop{range}(X)}(y) = Py$ for some projection matrix $P$

        Specifically, given $1 \leq d \leq n$, a matrix $P \in \mathbb{R}^{n \times n}$ is said to be a rank-$d$ orthogonal projections matrix if 
            \begin{itemize}
                \item $rank(P) = d$

                \item $P = P^{T}$

                \item $P^{2} = P$
            \end{itemize}
        Prove that $P$ is a rank-$d$ projection matrix if an only if there exists a $U \in \mathbb{R}^{n \times d}$ such that $P = UU^{T}$ and $U^{T}U = I$.
            \begin{answer}
                ($\rightarrow$) From the three statements above, $P = P^{T}P$, which means that $P$ is symmetric, there is an eigenvalue decomposition. The eigenvalues are $1$ for elements in the basis of $X$ and $0$ in the subspace $W / X$. So:
                    \begin{equation*}
                        P = U^{\prime}I U^{\prime T}
                    \end{equation*}
                but $U^{\prime}$ has $d$ non-zero vectors in its column space. So $P = UU^{T}$, $U^{T}U = I$ because $U$ consists of orthonormal vectors.

                ($\leftarrow$) Since $U^{T}U = I$, $U$ is full rank. We know this because the dot products not along the diagonal are $0$, so there cant be any linear dependence. This means that the rank of $P$ is also $d$ because:
                    \begin{equation*}
                        U^{T}U = I \implies UU^{T}U = I \implies PU = U
                    \end{equation*}
                and therefore, the rank of $P$ cannot be any less. It also cannot be any more because $P = UU^{T}$ and is restricted by $rank(U)$. The other two properties of an orthogonal projection matrix can be shown by algebraic manipulation.
            \end{answer}

        \item [(c)] The Singular Value Decomposition theorem states that we can write any matrix $X$ as 
            \begin{equation*}
                X = \sum_{i = 1}^{\min(n, d)}\sigma_{i} u_{i} v_{i}^{T} = \sum_{i: \sigma_{i} > 0}\sigma_{i} u_{i} v_{i}^{T}
            \end{equation*}
        where $\sigma_{i} \geq 0$, and $\{u_{i}\}^{n}_{i = 1}$ and $\{v_{i}\}_{i = 1}^{d}$ are orthonormal bases for $\mathbb{R}^{n}$ and $\mathbb{R}^{d}$ respectively. Some of the singular values $\sigma_{i}$ may equal $0$, indicating that the associated left and right singular vectors $u_{i}$ and $v_{i}$ do not contribute to the sum, but sometimes it is still convenient to include them in the SVD so we have complete orthonormal bases for $\mathbb{R}^{n}$ and $\mathbb{R}^{d}$ to work with. Show that
            \begin{itemize}
                \item [(i)] $\{u_{i} : \sigma_{i} > 0\}$ is an orthonormal basis for the column space of $X$.
                    \begin{answer}
                        
                    \end{answer}

                \item [(ii)] Similarly, $\{v_{i} : \sigma_{i} > 0\}$ is an orthonormal basis for the row space of $X$.
            \end{itemize}

        \item [(d)]

        \item [(e)]

        \item [(f)]
    \end{itemize}

\newpage
\section*{Some MLEs}
\hrule

For this question, assume you observe $n$ (data point, label) pairs $(x_{i}, y_{i})^{n}_{i = 1}$, with $x_{i} \in \mathbb{R}^{d}$ and $y_{i} \in \mathbb{R}$ for all $i = 1, \ldots, n$. We denote $X$ as the data matrix containing all the data points and $y$ as the label vector containing all the labels:
    \begin{equation*}
        X = \begin{bmatrix}
            x_{1}^{T} \\
            \vdots    \\
            x_{n}^{T}   
        \end{bmatrix} \in \mathbb{R}^{ n \times d}, \hspace{30pt}  y = \begin{bmatrix}
            y_{1}  \\
            \vdots \\
            y_{n}    
        \end{bmatrix} \in \mathbb{R}^{n}
    \end{equation*}
    \begin{itemize}
        \item [(a)] Ignoring $y$ for now, suppose we model the data points as coming from a $d$-dimensional Gaussian with diagonal covariance:
            \begin{equation*}
                \forall i = 1, \ldots, n, \, x_{i} \sim N(\mu, \Sigma); \, \Sigma = \begin{bmatrix}
                    \sigma_{1}^{2} & \ldots & 0              \\
                    \vdots         & \ddots & \vdots         \\
                    0              & \ldots & \sigma^{2}_{d}   
                \end{bmatrix}
            \end{equation*}
        If we consider $\mu \in \mathbb{R}^{d}$ and $(\sigma^{2}_{1}, \ldots, \sigma^{2}_{d})$, where each $\sigma^{2}_{i} > 0$, to be unknown, the parameter space here is $2d$-dimensional. When we refer to $\Sigma$ as a parameter, we are referring to the $d$-tuple $(\sigma_{1}^{2}, \ldots, \sigma^{2}_{d})$, but inside a linear algebraic expression, $\Sigma$ denotes the diagonal matrix $diag(\sigma_{1}^{2}, \ldots, \sigma^{2}_{d})$.

        Solve the following problems:
            \begin{itemize}
                \item [(i)] Prove that log-likelihood $l(\mu, \Sigma) = \log{p(X \mid \mu, \Sigma)}$ is equal to
                    \begin{equation*}
                        -\dfrac{n}{2} \left(d \log{2\pi} - \sum_{j = 1}^{d} \log{ \dfrac{1}{\sigma^{2}_{j}}}\right) - \dfrac{1}{2}\sum_{i = 1}^{n}(x_{i} - \mu)^{T}\Sigma^{-1}(x_{i} - pm)
                    \end{equation*}

                \item [(ii)]

                \item [(iii)]

                \item [(iv)]
            \end{itemize}

        \item [(b)]

        \item [(c)]

        \item [(d)]
    \end{itemize}

\end{document}
