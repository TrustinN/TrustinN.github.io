%! TeX root = /Users/trustinnguyen/Downloads/Berkeley/Math/Stat134/Notes/Stat134Notes.tex

\documentclass{report}
\usepackage{/Users/trustinnguyen/.mystyle/math/packages/mypackages}
\usepackage{/Users/trustinnguyen/.mystyle/math/commands/mycommands}
\usepackage{/Users/trustinnguyen/.mystyle/math/environments/report}
\graphicspath{{./figures/}}

\title{Stat134Notes}
\author{Trustin Nguyen}

\begin{document}
\newgeometry{
    total={150mm,235mm},
}
\begin{titlepage}
    \maketitle
\end{titlepage}

\tableofcontents
\restoregeometry

\reversemarginpar

\chapter{Week 1}

\textbf{Last Lecture}: Formalize random experiments by probability space: $(\Omega, \mathcal{F}, \mathbb{P})$. 
    \begin{itemize}
        \item $\Omega:$ sample space (all possible outcomes).

        \item $\mathcal{F}$: collection of subsets in $\Omega$ called events

        \item $P$: probability measure (distribution) $P : \mathcal{F} \rightarrow \mathbb{R}_{\geq 0}$. 
    \end{itemize}
\textbf{Conditions}:
    \begin{itemize}
        \item [(i)] $0 \leq P(A) \leq 1$

        \item [(ii)] $P(\emptyset)  = 0$ and $P(\Omega) = 1$

        \item [(iii)] $A_{n}$ disjoint events, then $P(\cup_{n - 1}^{\infty}A_{n}) = \sum_{n = 1}^{\infty}P(A_{n})$.
    \end{itemize}

\textbf{Corollary 1}: If $A$ and $B$ are two disjoint events, then 
    \begin{equation*}
        P(A \cup B) = P(A) + P(B)
    \end{equation*}
    \begin{proof}
        Take $A_{1} = A$ and $A_{2} = B$, $A_{3} = A_{4} = \cdots = \emptyset$. By $(iii)$, we have $P(A \cup B) = P(A) + P(B)$.
    \end{proof}

\textbf{Corollary 2}: Let $A^{c}$ be complement of $A$:
    \begin{equation*}
        A^{c} = \Omega \backslash A
    \end{equation*}
Then $P(A^{c}) = 1 - P(A)$. 
    \begin{proof}
        Take $B = A^{c}$ and use corollary $2$:
            \begin{equation*}
                P(A) + P(A^{c}) = P(\Omega) = 1
            \end{equation*}
    \end{proof}

\begin{examples}
    \begin{example}
        Loaded die. The bottom part is heavier than top. Suppose that $6$ is twice as likely than any other outcome. Then $P(6) = \frac{2}{7}$ and $P(1) = P(2) = \cdots = P(5) = \frac{1}{7}$.
            \begin{itemize}
                \item Probability of even outcome:
                    \begin{align*}
                        P(\text{even outcome}) &= P(\{2, 4, 6\})                 \\
                                               &= P(\{2\}) + P(\{4\}) + P(\{6\}) \\
                                               &= \dfrac{4}{7}                     
                    \end{align*}

                \item Probability of odd outcome:
                    \begin{equation*}
                        P(\text{odd outcome}) = 1 - P(\text{even outcome}) = \dfrac{3}{7}
                    \end{equation*}
            \end{itemize}
    \end{example}
\end{examples}

There are three methods for creating experiments with random outcomes: ball and urn, dice, coin.

\begin{examples}
    \begin{example}
        There are $3$ ways to retrieve $n$ balls from an urn.
            \begin{itemize}
                \item Sampling with replacement: Take a ball, each $n$ balls have equal probability of being selected, then record the number. Return the ball to the urn. Repeat $k$ times. 

                Outcome: sequence $(s_{1}, \ldots, s_{k})$. 
                    \begin{equation*}
                        \Omega = \{(s_{1}, \ldots, s_{k}): 1 \leq s_{i} \leq n\}
                    \end{equation*}
                Then $\lvert \Omega \rvert = n^{k}$. 

                \item Sampling without replacement:
                    \begin{itemize}
                        \item Order matters: Same urn but we do not return the ball once selected. We still remember the order of which the balls were selected. Repeat $k$ times.

                        Outcome: sequence $(s_{1}, \ldots, s_{k})$ with the restriction that $s_{i}$ distinct. 
                            \begin{equation*}
                                \lvert \Omega \rvert = \dbinom{n}{k}k!
                            \end{equation*}

                        \item Order does not matter: Same urn, take out balls and do not record their order. It only matters which balls are outside the urn at the end of the experiment.

                        Outcome: sets $\{s_{1}, \ldots, s_{k}\}$ of an $n$ element set. 
                            \begin{equation*}
                                \lvert \Omega \rvert = \dbinom{n}{k}
                            \end{equation*}
                    \end{itemize}
            \end{itemize}
    \end{example}
    \begin{example}
        Urn with $\{1, 2, 3, 4, 5\}$, sample $3$ balls with replacement. Then 
            \begin{equation*}
                \Omega = \{1, 2, 3, 4, 5\}^{3}
            \end{equation*}
        and
            \begin{equation*}
                \lvert \Omega \rvert = 125
            \end{equation*}
        so 
            \begin{equation*}
                P(w) = \dfrac{1}{125}
            \end{equation*}
        An example is that $P(153) = P(224)$.
    \end{example}
    \begin{example}
        Same urn with $\{1, 2, 3, 4, 5\}$ and take $3$ balls without replacement. Then 
            \begin{equation*}
                \Omega = \{(s_{1}, s_{2}, s_{3}) : s_{i} \text{ distinct}\}
            \end{equation*}
        and 
            \begin{equation*}
                \lvert \Omega \rvert = \dbinom{5}{3}3!
            \end{equation*}
        so 
            \begin{equation*}
                P(w) = \dfrac{1}{\dbinom{5}{3}3!}
            \end{equation*}
        In this example, $P(153) = P(w)$ while $P(224) = 0$.
    \end{example}
    \begin{example}
        $P(w) = \frac{1}{\binom{n}{k}}$. 
    \end{example}
    \begin{example}
        Same urn with $\{1, 2, 3, 4, 5\}$ and take $3$ balls without replacement and ignoring the order.
            \begin{equation*}
                P(\{153\}) = \dfrac{1}{\dbinom{5}{3}}
            \end{equation*}
        \textbf{Remark}: Same urn can lead to different experiments and probabilities.
    \end{example}
    \begin{example}
        Discussion section with $30$ students all having different names. 
            \begin{itemize}
                \item Choose one student each day.

                Each day a random student is chosen to present a solution with no regard to previous choices. The probability of a given event is $\frac{1}{30^{3}}$.

                \item Three random students are chosen as section president, vice-president, press-secretary.

                What is the probability that $P($Joe is a president, Kamala is VP, Karine is PS$)$. It is $\frac{1}{30 \cdot 29 \cdot 28}$.

                \item A team of $3$ students at random.

                $P($team: Alice, Bob, Charlie$) = \frac{3! \cdot 27!}{30!}$. 
            \end{itemize}
    \end{example}
\end{examples}

Once you set a probability space, you want to compute probabilities of complicated events. Approaches:
    \begin{itemize}
        \item Approach 1: Decomposing the event. Use the formula:
            \begin{equation*}
                P(A) = P(A_{1}) + P(A_{2}) + \cdots + P(A_{n})
            \end{equation*}
            \begin{examples}
                \begin{example}
                    An urn contains $30$ red, $20$ green, and $10$ yellow balls. We take out $2$ balls without replacement. What is the probability that the sample contains exactly one red ball or exactly one yellow. The event can be split into three different ones:
                        \begin{equation*}
                            \{R \, \lor \, Y\} = \{R, Y\} \cup \{R, G\} \cup \{Y, G\}
                        \end{equation*}
                    Compute probabilities as in sampling without replacement. 
                        \begin{align*}
                            P(\{R, Y\}) &= \dfrac{30 \cdot 10}{\dbinom{60}{2}} \\
                            P(\{R, G\}) &= \dfrac{30 \cdot 20}{\dbinom{60}{2}} \\
                            P(\{Y, G\}) &= \dfrac{20 \cdot 10}{\dbinom{60}{2}}   
                        \end{align*}
                    Now we add the probabilities, which will be the answer.
                \end{example}
            \end{examples}

        \item Use $P(A^{c}) = 1 - P(A)$. From $1$, we also get
            \begin{equation*}
                P(B) = P(A \cap B) + P(A^{c} \cap B)
            \end{equation*}
        \textbf{Example}: Birthday problem: If there are $30$ people, what is the chance that $2$ people have birthdays on the same day?
            \begin{equation*}
                \Gamma = \{1, \ldots, 365\}^{30}
            \end{equation*}
        and the probability that there are birthdays on the same day is $1 - $ probability where no birthdays are on the same day:
            \begin{equation*}
                P(W) = \dfrac{\dfrac{365!}{335!}}{365^{30}}
            \end{equation*}
        So the answer is $1 - P(W) \approx 0.7$.

        \item Extension of $1$ to case where $A_{i}$ are not disjoint: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. 

        \textbf{Example}: An urn contains $30$ red, $20$ green, and $10$ yellow balls. We draw $2$ balls and want to find the probability that there is exactly one red or exactly one yellow. Then use the theorem.

        A generalization of theorem $1$. 
            \begin{equation*}
                P(A_{1} \cup \cdots \cup A_{n}) = \sum_{i = 1}^{n}P(A_{i}) - \sum_{i < j}P(A_{i} \cap A_{j}) + \sum_{i < j < u} P(A_{i} \cap A_{j} \cap A_{u}) - \cdots
            \end{equation*}
    \end{itemize}

\chapter{Week 2}

Events: binary - either happens or not.

Random variables: number - computed from random experiment.

\begin{definition}{}
    Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a random variable is a function from $\Gamma$ into $\mathbb{R}$. The value of random variable $X$ at a sample point $w \in \Omega$ is $X(w)$.
\end{definition}

\textbf{Remark}: When $\Omega$ is large (uncountable), we should restrict $f$ to measurable functions.

\begin{examples}
    \begin{example}
        Flip $3$ coins and record results as $0 - 1$ sequences, $a_{1}a_{2}a_{3}$. Set $S = a_{1} + a_{2} + a_{3}$ the number of heads. We can also have random variables: 
            \begin{align*}
                X_{1} &= a_{1} \\
                X_{2} &= a_{2} \\
                X_{3} &= a_{3}   
            \end{align*}
        and $S = X_{1} + X_{2} + X_{3}$. Compute $P(X_{1} = 0)$. Compute $P(X_{1} = 0, S = 1)$.
    \end{example}
    \begin{example}
        We roll a die. For $1, 2, 3$, the player loses $\$1$. For $4$, the player gains $\$1$. For outcome $5, 6$, gain $\$3$. Let $W$ be the variable for the resulting change of wealth. We have 
            \begin{align*}
                W(1) &= W(2) = W(3) = -1 \\
                W(4) &= 1                \\
                W(5) &= W(6) = 3           
            \end{align*}
        $W$ takes three values with probabilities: $\frac{1}{2}, \frac{1}{6}, \frac{1}{3}$. $W$ creates a new probability space $\{-1, 1, 3\}$ with new probability measure, called a distribution of random variable $W$. 
    \end{example}
    \begin{example}
        A constant function
            \begin{equation*}
                C : \Omega \rightarrow \mathbb{R}
            \end{equation*}
        by
            \begin{equation*}
                C(w) = x \hspace{30pt}  \text{for fixed $x$}
            \end{equation*}
        We can say that $\mathbb{P}(C = x) = 1$. There is no randomness left, it is deterministic/degenerate but it is still a random variable.
    \end{example}
\end{examples}

\begin{topic}
    \section{Large Sets of Outcomes}
\end{topic}

\begin{examples}
    \begin{example}
            Pick a uniformly random number $x \in [0, 1]$. Then the probability space is $\Omega = [0, 1]$. How to model uniformly random? Since outcomes are equally likely, then it is probably true that 
            \begin{equation*}
                P([a, b]) = b - a
            \end{equation*}
        What about one element? 
            \begin{equation*}
                P(\{a\}) = P([a, a]) = 0
            \end{equation*}
        Recall that in $iii$, if we have
            \begin{equation*}
                A = \bigcup_{n = 1}^{\infty}A_{n}
            \end{equation*}
        then $P(A) = \sum_{n =1}^{\infty}P(A_{n})$. $\Omega$ is uncountable, as $\lvert \mathbb{N} \rvert \neq \lvert \mathbb{R} \rvert$. 
    \end{example}
    \begin{example}
        Consider a dartboard, which is a disk of radius $9$ inches. The bullseye is a  disk of diameter $\frac{1}{2}$ inch. 
    \end{example}
\end{examples} 

Recall that a random variable $X$ is a function $X : \Omega \rightarrow \mathbb{R}$.

\begin{definition}{Distribution of a Random Variable}
    Distribution of $X$ is a function of a set $\mathcal{B} \subseteq \mathbb{R}$ given by 
        \begin{equation*}
            P(X \in \mathcal{B}) = P(X^{-1}(\mathcal{B}))
        \end{equation*}
\end{definition}

\textbf{Lemma}: Distribution of $X$ is a probability measure on $\mathbb{R}$.

\begin{examples}
    \begin{example}
        Let $X$ be uniform on $[0, 1]$. Set $Y = 2X$. 
            \begin{align*}
                Y    &: [0, 1] \rightarrow \mathbb{R} \\
                Y(a) &= 2a                              
            \end{align*}
        Compute $p(Y \in [-5, 5]) = 1$. $P(Y \in [0, 2]) = 1$. What about $P(Y \in [3, 4]) = 0$. Also $P(Y \in [a, b]) = \frac{b - a}{2}$.
    \end{example}
\end{examples}

\begin{definition}{Degenerate Random Variables}
    A random variable is degenerate if for some $a \in \mathbb{R}$, $P(X = a) = 1$. This can happen if $X$ is constant:
        \begin{align*}
            X    &: \Omega \rightarrow\mathbb{R} \\
            X(\omega) &= a                              
        \end{align*}
\end{definition}

\textbf{Exercise}: Construct a degenerate random variable that is not constant.

\begin{definition}{Discrete Random Variable}
    A discrete random variable is one where there is a finite or countable collection of points on $\mathbb{R}$, $\{K_{1}, K_{2}, \ldots\}$ such that $\sum_{i} P(X = K_{i}) = 1$. The $K_{i}$ such that $P(X = K_{i}) > 0$ are called possible values of discrete random variable $X$.
\end{definition}

\begin{examples}
    \begin{itemize}
        \item Any degenerate random variable is discrete. 

        \item Sum of two dice. 
            \begin{equation*}
                \Omega = \{1, 2, \ldots, 6\}^{2}
            \end{equation*}
        and
            \begin{align*}
                X              &:       \Omega \rightarrow \mathbb{R} \\
                (a_{1}, a_{2}) &\mapsto a_{1} + a_{2}                   
            \end{align*}
        its range of possible values in $\mathbb{R}$ is $2, \ldots, 12$, so it is a discrete random variable. 

        \item $Y$ from example $3$ is not discrete. Since $P(Y = 0) = 0$, then $P(\{K_{1}, \ldots\}) = \sum_{i \geq 1} P(K_{i}) = 0$
    \end{itemize}
\end{examples}

\begin{definition}{Probability mass function}
    A probability mass function of a discrete random variable is defined as 
        \begin{equation*}
            P_{X}(K) = P(X = K)
        \end{equation*}
\end{definition}

From additivity, $P(X \in \mathcal{B}) = \sum_{K \in \mathcal{B}} P_{X}(K)$.

\begin{examples}
    \begin{example}
        $Y = 2X$ = uniform on $[0, 2]$. Take any set $A \subseteq [0, 2]$ define indicator function:
            \begin{align*}
                I_{A}(Y) &= \begin{cases}
                    1                &\text{ if } Y \in A \\
                    0 &\text{ if } \text{otherwise}   
                \end{cases}   
            \end{align*}
        $A = [0, 1]$. Define $Z = I_{A}(Y)$. Then $P_{Z}(1) = .5$ and $P_{Z}(0) = .5$.
    \end{example}
\end{examples}















\end{document}
