%! TeX root = Downloads/Berkeley/Math/Math128a/Homework/Math128aHw12/Math128aHw12.tex

\documentclass{article}
\usepackage{/Users/trustinnguyen/.mystyle/math/packages/mypackages}
\usepackage{/Users/trustinnguyen/.mystyle/math/commands/mycommands}
\usepackage{/Users/trustinnguyen/.mystyle/math/environments/article}
\graphicspath{{./figures/}}

\title{Math128aHw12}
\author{Trustin Nguyen}

\begin{document}

    \maketitle

\reversemarginpar

\section*{Exercise Set 6.1}
\hrule

\textbf{Exercise 6}: Use the Gaussian Elimination Algorithm to solve the following linear systems, if possible, and determine whether row interchanges are necessary:
    \begin{itemize}
        \item [d.] 
            \begin{align*}
                x_{1} + x_{2} + x_{4}            &= 2  \\
                2x_{1} + x_{2} - x_{3} + x_{4}   &= 1  \\
                -x_{1} + 2x_{2} + 3x_{3} - x_{4} &= 4  \\
                3x_{1} - x_{2} - x_{3} + 2x_{4}  &= -3   
            \end{align*}
    \end{itemize}
    \begin{answer}
        Write it as a matrix:
            \begin{equation*}
                \begin{bmatrix}
                    1  & 1  & 0  & 1  & 2  \\
                    2  & 1  & -1 & 1  & 1  \\
                    -1 & 2  & 3  & -1 & 4  \\
                    3  & -1 & -1 & 2  & -3   
                \end{bmatrix}
            \end{equation*}
        Then
            \begin{equation*}
                \begin{bmatrix}
                    1  & 1 & 0 & 1  & 2 \\
                    0  & 5 & 5 & -1 & 9 \\
                    -1 & 2 & 3 & -1 & 4 \\
                    0  & 5 & 8 & -1 & 9   
                \end{bmatrix}
            \end{equation*}
        which becomes:
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 0 & 1  & 2 \\
                    0 & 5 & 5 & -1 & 9 \\
                    0 & 3 & 3 & 0  & 6 \\
                    0 & 0 & 3 & 0  & 0   
                \end{bmatrix}
            \end{equation*}
        or
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 0 & 1  & 2 \\
                    0 & 1 & 1 & 0  & 2 \\
                    0 & 0 & 1 & 0  & 0 \\
                    0 & 5 & 5 & -1 & 9   
                \end{bmatrix}
            \end{equation*}
        and
            \begin{equation*}
                \begin{bmatrix}
                    1 & 1 & 0 & 1  & 2  \\
                    0 & 1 & 0 & 0  & 2  \\
                    0 & 0 & 1 & 0  & 0  \\
                    0 & 0 & 0 & -1 & -1   
                \end{bmatrix}
            \end{equation*}
        So we get $x_{1} = -1, x_{2} = 2, x_{3} = 0, x_{4} = 1$.
    \end{answer}

\textbf{Exercise 9}: Given the linear system
    \begin{align*}
        2x_{1} - 6 \alpha x_{2} &= 3            \\
        3\alpha x_{1} - x_{2}   &= \dfrac{3}{2}   
    \end{align*}
    \begin{itemize}
        \item [a.] Find value(s) of $\alpha$ for which the system has no solutions.
            \begin{answer}
                Multiply both equations by:
                    \begin{align*}
                        6\alpha x_{1} - 18\alpha^{2} x_{2} &= 9\alpha \\
                        -6\alpha x_{1} + 2x_{2}             &= -3         
                    \end{align*}
                So we get the matrix:
                    \begin{equation*}
                        \begin{bmatrix}
                            6\alpha & -18\alpha^{2} & 9\alpha \\
                            0       & 2 -18\alpha^{2}      & 9\alpha - 3
                        \end{bmatrix}
                    \end{equation*}
                There are no solutions when $2 - 18\alpha^{2} = 0$ or $\alpha = \pm \frac{1}{3}$. But $9\alpha - 3$ should also be nonzero, so no solutions when $\alpha = -\frac{1}{3}$.
            \end{answer}

        \item [b.] Find value(s) of $\alpha$ for which the system has an infinite number of solutions.
            \begin{answer}
                From the previous problem, there are infinite solutions when $\alpha = \pm \frac{1}{3}$, or when both $2 - 18\alpha^{2} = 0$ and $9\alpha - 3 = 0$.
            \end{answer}

        \item [c.] Assuming a unique solution exists for a given $\alpha$, find the solution.
            \begin{answer}
                Reduce the matrix from part $a$:
                    \begin{equation*}
                        \begin{bmatrix}
                            1 & -3\alpha & 3/2                                   \\
                            0 & 1        & \dfrac{9\alpha - 3}{2 - 18\alpha^{2}}   
                        \end{bmatrix}
                    \end{equation*}
                So we get:
                    \begin{align*}
                        x_{1} &= \dfrac{3}{2} - 3\alpha\left(\dfrac{9\alpha - 3}{2 - 18\alpha^{2}}\right) \\
                        x_{2} &= \dfrac{9\alpha - 3}{2 - 18\alpha^{2}}                                      
                    \end{align*}
            \end{answer}
    \end{itemize}

\textbf{Exercise 12}: A Fredholm integral equation of the second kind is an equation of the form
    \begin{equation*}
        u(x) = f(x) + \int_{a}^{b} K(x, t) u(t) \, \dd{t},
    \end{equation*} 
    where $a$ and $b$ and the functions $f$ and $K$ are given. To approximate the function $u$ on the interval $[a, b]$, a partition $x_{0} = a < x_{1} < \cdots < x_{m - 1} < x_{m} = b$ is selected, and the equations
        \begin{equation*}
            u(x_{i}) = f(x_{i}) + \int_{a}^{b} K(x_{i}, t)u(t) \, \dd{t}
        \end{equation*}
    are solved for $u(x_{0}), u(x_{1}), \ldots, u(x_{m})$. The integrals are approximated using quadrature formulas based on the nodes $x_{0}, \ldots, x_{m}$. In our problem, $a = 0, b = 1, f(x) = x^{2}$, and $K(x, t) = e^{\lvert x - t \rvert}$.
    \begin{itemize}
        \item [a.] Show that the linear system
            \begin{align*}
                u(0) &= f(0) + \dfrac{1}{2}[K(0, 0)u(0) + K(0, 1)u(1)] \\
                u(1) &= f(1) + \dfrac{1}{2}[K(1, 0)u(0) + K(1, 1)u(1)]   
            \end{align*}
        must be solved when the Trapezoidal rule is used.
            \begin{answer}
                In the trapezoidal rule, we take $1/2$ of the sum of the endpoints. So for the first node $x_{0} = a = 0$, we have:
                    \begin{equation*}
                        \int_{a}^{b} K(x_{0}, t)u(t) \, \dd{t} \approx \dfrac{1}{2}[K(x_{0}, 0)u(0) + K(x_{0}, 1)u(1)]
                    \end{equation*}
                and for $x_{1} = b = 0$:
                    \begin{equation*}
                        \int_{a}^{b} K(x_{1}, t)u(t) \, \dd{t} \approx \dfrac{1}{2}[K(x_{1}, 0)u(0) + K(x_{1}, 1)u(1)]
                    \end{equation*}
                putting this with $u(x)$, we get the equations above.
            \end{answer}

        \item [b.] Set up and solve the linear system that results when the Composite Trapezoidal rule is used with $n = 4$.

        \item [c.] Repeat part (b) using the composite Simpson's rule.
    \end{itemize}

\textbf{Exercise 13}: Show that the operations
    \begin{itemize}
        \item [a.] $(\lambda E_{i}) \rightarrow (E_{i})$.

        \item [b.] $(E_{i} + \lambda E_{j}) \rightarrow (E_{i})$

        \item [c.] $(E_{i}) \iff (E_{j})$
    \end{itemize}
do not change the solution set of a linear system.
    \begin{itemize}
        \item [a.] $(\lambda E_{i}) \rightarrow (E_{i})$.
            \begin{answer}
                If $E_{i}$ represents the coefficients at row $i$, we have the relation:
                    \begin{equation*}
                        a_{i, 0}x_{i, 0} + \cdots + a_{i, n}x_{i, n} = b_{i}
                    \end{equation*}
                So if $x_{i, 0}, \ldots, x_{i, n}$ is our solution, we see that it still remains the solution in:
                    \begin{equation*}
                        \lambda(a_{i, 0}x_{i, 0} + \cdots + a_{i, n}x_{i, n}) = \lambda b_{i}
                    \end{equation*}
                for any $\lambda$. 

                We also see that if $y$ is not a solution, then $y$ is not a soluiton under the new coefficients also.
            \end{answer}

        \item [b.] $(E_{i} + \lambda E_{j}) \rightarrow (E_{i})$
            \begin{answer}
                Recall
                    \begin{equation*}
                        a_{i, 0}x_{i, 0} + \cdots + a_{i, n}x_{i, n} = b_{i}
                    \end{equation*}
                and let the product of the coefficients with $x_{i, 0}, \ldots, x_{i, n}$ be the linear transformation. Then we have:
                    \begin{equation*}
                        T_{i}(x) + \lambda T_{j}(x)
                    \end{equation*}
                where $T_{i}$ represents a dot product with $x$. Then by the properties of a dot product:
                    \begin{equation*}
                        T_{i}(x) + \lambda T_{j}(x) = (T_{i} + \lambda T_{j})(x) = b_{i} + \lambda b_{j}
                    \end{equation*}
                and therefore we see that $x$ remains a solution under the new coefficients.

                It is also clear that if $y$ is a vector that is not in the solution set, it is also not a solution under the new coefficients.
            \end{answer}

        \item [c.] $(E_{i}) \iff (E_{j})$
            \begin{answer}
                If $x$ is a solution to $E_{1}, \ldots, E_{n}$, permuting the $E_{i}$ does not change the fact that $x$ remains a solution. And if $y$ does not satisfy one of the $E_{i}$, permuting the $E_{i}$ does not change the fact that $y$ does not satisfy one of the $E_{i}$.
            \end{answer}
    \end{itemize}
do not change the solution set of a linear system.

\newpage
\section*{Exercise Set 6.2}
\hrule

\textbf{Exercise 2}: Find the row interchanges that are required to solve the following linear systems using Algorithm $6.1$.
    \begin{itemize}
        \item [a.] 
            \begin{align*}
                13x_{1} + 17x_{2} + x_{3} &= 5 \\
                x_{2} + 19x_{3}           &= 1 \\
                12x_{2} - x_{3}           &= 0   
            \end{align*}
    \end{itemize}
    \begin{answer}
        I got $x_{1} = 0.3743$, $x_{2} = 0.0046$, $x_{3} = 0.0553$. Here is my code:
        \inputminted{matlab}{./code/GE/GEBackward.m}
        \inputminted{matlab}{./code/script1.m}
    \end{answer}

\textbf{Exercise 4}: Repeat Exercise $2$ using Algorithm $6.2$.
    \begin{answer}
        Here is the code:
        \inputminted{matlab}{./code/GE/GEPP.m}
        \inputminted{matlab}{./code/script2.m}
    \end{answer}

\textbf{Exercise 6}: Repeat Exercise $2$ using Algorithm $6.3$.

\newpage
\section*{Exercise Set 6.3}
\hrule

\textbf{Exercise 6}: Determine which of the following matrices are nonsingular and compute the inverse of those matrices:
    \begin{itemize}
        \item [a.] 
            \begin{equation*}
                \begin{bmatrix}
                    1  & 2 & -1 \\
                    0  & 1 & 2  \\
                    -1 & 4 & 3    
                \end{bmatrix}
            \end{equation*}
            \begin{answer}
                Row reduce
                    \begin{equation*}
                        \begin{bmatrix}
                            1 & 2 & -1 \\
                            0 & 1 & 2  \\
                            0 & 6 & 2    
                        \end{bmatrix}
                    \end{equation*}
                and
                    \begin{equation*}
                        \begin{bmatrix}
                            1 & 2 & -1  \\
                            0 & 1 & 2   \\
                            0 & 0 & -10   
                        \end{bmatrix}
                    \end{equation*}
                so it is invertible.
            \end{answer}
    \end{itemize}

\textbf{Exercise 9}:
    \begin{itemize}
        \item [a.] The product is as shown because of the linearity of matrix products. We can take multiply matrices block wise to get $AB$.

        \item [b.] No because the dimensions do not match up for the matrix multiplication.

        \item [c.] The vertical split of $A$ must match up with the size of the horizontal split of $B$.
    \end{itemize}

\end{document}
