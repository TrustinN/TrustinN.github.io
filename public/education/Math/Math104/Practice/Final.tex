%! TeX root = /Users/trustinnguyen/Downloads/Berkeley/Math/Math104/Practice/Final.tex

\documentclass{article}
\usepackage{/Users/trustinnguyen/.mystyle/math/packages/mypackages}
\usepackage{/Users/trustinnguyen/.mystyle/math/commands/mycommands}
\usepackage{/Users/trustinnguyen/.mystyle/math/environments/article}
\graphicspath{{./figures/}}

\title{Math104FinalPractice}
\author{Trustin Nguyen}

\begin{document}

    \maketitle

\reversemarginpar

\textbf{Exercise 1}: \begin{itemize}
    \item Suppose that $A \subseteq \mathbb{R}$ is a finite set. Prove that $A$ cannot be dense in $\mathbb{R}$.
        \begin{proof}
            Suppose that $A$ if finite, for contradiction. Then there exists a supremum of the set $S$. So for all $a \in A$, we have $a \leq S$. Then we have
                \begin{equation*}
                    A \cap (S, \infty) = \emptyset
                \end{equation*}
            demonstrates and empty intersection with an open set. So $A$ is not dense. 
        \end{proof}

    \item Suppose that $A \subseteq \mathbb{R}$ satisfies the property:
        \begin{equation*}
            A \text{ is bounded above, and $\sup A  \notin A$.}
        \end{equation*}
    First, give an example of a set $A$ which satisfies $(*)$. Then, prove that any set $A$ which satisfies $(*)$ must have infinitely many elements.
        \begin{proof}
            One such set is $A = (0, 1)$. We have $\sup A = 1 \notin (0, 1)$. Suppose that we have a set that is bounded above and that $\sup A \notin A$. Then for all $a \in A$, we have that $a \leq \sup A$. Also, if we have $M$ such that $a \leq M$ for all $a \in A$, then $\sup A \leq M$. Notice that our set must be non-empty. Then there exists an element such that $a_{0} < \sup A$. Since $a_{0}$ is not the supremum, then there exists an element not in $A$ larger. So $a_{0} < a_{1} < \sup A$. But we can continue this process forever. So the size of the set is infinite.
        \end{proof}
\end{itemize}

\textbf{Exercise 2}: \begin{itemize}
    \item Using the $\varepsilon - \delta$ definition of continuity, prove that the function $f: \mathbb{R} \rightarrow\mathbb{R}$ defined by $f(x) = 6x - 5$ is continuous on $\mathbb{R}$.
        \begin{proof}
            We need to show that $\forall \varepsilon >0$, we have that $\exists \delta >0$ such that if 
                \begin{equation*}
                    \lvert x - y \rvert < \delta
                \end{equation*}
            then
                \begin{equation*}
                    \lvert f(x) - f(y) \rvert < \varepsilon
                \end{equation*}
            So we want to find when
                \begin{align*}
                    \lvert 6x - 5 - (6y - 5) \rvert &< \varepsilon            \\
                    \lvert 6x - 6y \rvert           &< \varepsilon            \\
                    6\lvert x - y \rvert            &< \varepsilon            \\
                    \lvert x - y \rvert             &< \dfrac{\varepsilon}{6}   
                \end{align*}
            So there is a $\delta = \frac{\varepsilon}{6}$. And we are done.
        \end{proof}

    \item Suppose $g : \mathbb{R} \rightarrow \mathbb{R}$ is a continuous function. Must the set $g([0 , 1]) \subseteq\mathbb{R}$ be bounded? Must it be closed? Justify your answers.
        \begin{proof}
            We know that $g([0, 1])$ is bounded. Suppose wlog that $g([0, 1])$ is unbounded above. Then we have that there is a sequence $g(x_{n})$ such that 
                \begin{equation*}
                    g(x_{1}) < g(x_{2}) < \cdots
                \end{equation*}
            diverges to $\infty$. Or we have $(a_{n}) < (g(x_{n}))$, where $a_{n} = n$.
        \end{proof}
\end{itemize}

\textbf{Exercise 3}:
    \begin{itemize}
        \item Prove that the series $\sum_{n = 1}^{\infty} \frac{2 + \sin{n}}{n^{3} + 5}$ converges.
            \begin{proof}
                By comparison test, we have 
                    \begin{equation*}
                        \dfrac{2 + \sin{n}}{n^{3} + 5} \leq \dfrac{3}{n^{3}}
                    \end{equation*}
                we also see that 
                    \begin{equation*}
                        \dfrac{3}{n^{3}} \leq \dfrac{1}{n^{2}}
                    \end{equation*}
                when $n \geq 3$. Since $\sum \frac{1}{n^{2}}$ converges, we have that $\sum_{n = 1}^{\infty}\frac{2 + \sin{n}}{n^{3} + 5}$.
            \end{proof}

        \item Let $(a_{n})_{n = 1}^{\infty}$ be a sequence of strictly positive real numbers which satisfies $\lim_{n \to \infty}a_{n} = 2$. Determine if the series
            \begin{equation*}
                \sum_{n = 1}^{\infty}\dfrac{1}{n^{2} \cdot a_{n}}
            \end{equation*}
        converges (always, sometimes, or never). Justify your answer.
            \begin{proof}
                The sequence always converges. Because we have that $\lim_{n \to \infty} a_{n} = 2$, this means that there is an $N$ such that $\forall n > N$, we have 
                    \begin{equation*}
                        \lvert a_{n} - 2 \rvert < 1
                    \end{equation*}
                This means that 
                    \begin{equation*}
                        1 < a_{n} < 3
                    \end{equation*}
                So we have that 
                    \begin{equation*}
                        \left\lvert \dfrac{1}{n^{2} \cdot a_{n}} \right\rvert \leq \dfrac{1}{n^{2}}
                    \end{equation*}
                and that the RHS converges. Then so does the LHS by the comparison test. Since it converges when we sum for terms above $N$ and for terms below $N$, we have that the entire sum converges.
            \end{proof}
    \end{itemize}

\textbf{Exercise 4}: Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be continuous on $\mathbb{R}$. 
    \begin{itemize}
        \item Prove that
            \begin{equation*}
                \lim_{h \to 0^{+}}\dfrac{1}{h} \int_{0}^{h} f(x) \, \dd{x}  = f(0)
            \end{equation*}
                \begin{proof}
                    Since $f$ is continuous, then it is integrable, and we have
                        \begin{equation*}
                            \lim\limits_{h \to 0^{+}}\dfrac{1}{h}(F(h) - F(0)) = \lim\limits_{h \to 0^{+}}F^{\prime}(h) = f(0)
                        \end{equation*}
                    So this is the answer.
                \end{proof}

        \item Prove that if 
            \begin{equation*}
                \lim_{h \to 0^{+}} \dfrac{1}{h}\int_{0}^{1 + h} f(x) \, \dd{x}  \text{exists (i.e. is some real number), }
            \end{equation*}
        then $\int_{0}^{1} f(x) \, \dd{x} = 0$. 
            \begin{proof}
                
            \end{proof}
    \end{itemize}

\textbf{Exercise 5}:
    \begin{itemize}
        \item Consider the power series $\sum_{n = 0}^{\infty}(\frac{1}{5})^{n}x^{n}$. Find all points $x \in\mathbb{R}$ for which this series converges.
            \begin{proof}
                This converges when 
                    \begin{equation*}
                        \limsup \left\lvert \left(\dfrac{1}{5}\right)^{n}x^{n} \right\rvert^{\frac{1}{n}} < 1
                    \end{equation*}
                So we require
                    \begin{equation*}
                        \lvert x \rvert < 5
                    \end{equation*}
                Now, checking the endpoints, we have
                    \begin{equation*}
                        \sum_{n = 0}^{\infty}1^{n}
                    \end{equation*}
                and 
                    \begin{equation*}
                        \sum_{n = 0}^{\infty}(-1)^{n}
                    \end{equation*}
                which both diverge. So the radius of convergence is $(-5, 5)$.
            \end{proof}

        \item Let $f(x) = \sum_{n = 0}^{\infty}(\frac{1}{5})^{n}x^{n}$ for all $x$ values for which the right-hand side is well-defined. Is $f$ differentiable at $x =  0$? If so, explain why and find $f^{\prime}(0)$.
            \begin{proof}
                $f$ is differentiable at $x = 0$ because the sum converges at $x = 0$. We need to verify that by the definition:
                    \begin{equation*}
                        \lim_{x \to 0} \dfrac{f(x) - f(0)}{x - 0}
                    \end{equation*}
                converges. Then this is 
                    \begin{equation*}
                        \lim_{x \to 0} \dfrac{\sum_{n = 0}^{\infty}(\dfrac{1}{5}x)^{n} - 1}{x} = \lim_{x \to 0} \dfrac{\sum_{n = 1}^{\infty}(\dfrac{1}{5}x)^{n}}{x} = \lim_{x \to 0} \sum_{n = 1}^{\infty}(\dfrac{1}{5})^{n}x^{n - 1} = \dfrac{1}{5}
                    \end{equation*}
                So this is the limit and the derivative.
            \end{proof}
    \end{itemize}

\textbf{Exercise 6}: For $n \in\mathbb{N}$, let $f_{n} : [0, 1] \rightarrow \mathbb{R}$ be the function which satisfies 
    \begin{equation*}
        f_{n}(x) = \begin{cases}
            n                                   &\text{ if } x \in(0, \dfrac{1}{n}) \\
            0 &\text{ if } x \in \{0\} \cup [\dfrac{1}{n}, 1].   
        \end{cases}
    \end{equation*}
    \begin{itemize}
        \item Find the function $f : [0 , 1] \rightarrow \mathbb{R}$ so that $f_{n} \rightarrow f$ pointwise in $[0, 1]$, and justify.
            \begin{proof}
                The function converges to $0$. We need to show that for all $x \in [0, 1]$, we have for all $\varepsilon > 0$:
                    \begin{equation*}
                        \lvert f_{n}(x) \rvert < \varepsilon
                    \end{equation*}
                If we pick $n = \varepsilon/2$, we see that the function has a value of either $\varepsilon/2$ or $0$ which is less than $\varepsilon$.
            \end{proof}

        \item Determine if $f_{n} \rightarrow f$ uniformly on $[0, 1]$, and explain.
            \begin{proof}
                It converges uniformly because $n$ does not depend on $x$.
            \end{proof}
    \end{itemize}

\textbf{Exercise 7}:
    \begin{itemize}
        \item Show that if $f$ is continuous with $f \geq0$ and $\int_{0}^{1} f(x) \, \dd{x}  = 0$, then $f \equiv 0$.

        \item Give an example which shows the statement in the previous part may not be true if we do not assume $f$ is continuous, and justify. 
    \end{itemize}

\textbf{Exercise 8}: Suppose that $(x_{n})_{n = 1}^{\infty}$ is convergent sequence in $\mathbb{R}$.
    \begin{itemize}
        \item Show that the set $\{x_{n} : n \in\mathbb{N}\}$ is compact. The set should be $\{x_{n} : n \in\mathbb{N}\} \cup \{\lim_{n \to \infty}x_{n}\}$.
            \begin{proof}
                We have that the set is bounded because it is a convergent sequence. This means that there is an $N$ such that $\forall n > N$, we have
                    \begin{equation*}
                        \lvert x_{n} - L \rvert < 1
                    \end{equation*}
                Then we have that 
                    \begin{equation*}
                        - 1 + L < x_{n} < 1 + L
                    \end{equation*}
                So we see that $(x_{n})$ is bounded above by $\max(1 + L, x_{0}, x_{1}, \ldots, x_{N})$ and below by $\min(-1 + L, x_{0}, x_{1}, \ldots, x_{N})$. To show that it is closed, we need to prove that all the limits of any sequence in $M = \{x_{n} : n \in \mathbb{N}\} \cup \{\lim_{n \to \infty}x_{n}\}$ lies within the set. Suppose for contradiction that we had some other convergent series $(y_{n})$ where $y_{n} \in M$. Then we have 
                    \begin{equation*}
                        \lim_{n \to \infty} y_{n} = L^{\prime} \neq L
                    \end{equation*}
                Since there is infinitely many $y_{n}$, we must have some $x_{m} = y_{n}$ for any $m > N$. Consider the difference $\lvert L^{\prime} - L \rvert$. First, we consider distances between $y_{i}$ by the cauchy criterion, there is a $B$ such that for all $a, b > B$, we have 
                    \begin{equation*}
                        \lvert y_{a} - y_{b} \rvert < \lvert L^{\prime} - L \rvert / 3
                    \end{equation*}
                Now we also have a $B^{\prime}$ such that for all $b^{\prime} > B^{\prime}$, 
                    \begin{equation*}
                        \lvert y_{b^{\prime}} - L^{\prime} \rvert < \lvert L^{\prime} - L \rvert /3
                    \end{equation*}
                and finally, there is a $B^{\prime\prime}$ such that for some $b^{\prime\prime} > B^{\prime\prime}$, we have
                    \begin{equation*}
                        \lvert y_{b^{\prime\prime}} - L \rvert < \lvert L^{\prime} - L \rvert / 3
                    \end{equation*}
                Now take $B^{\prime\prime\prime} = \max(B, B^{\prime}, B^{\prime\prime})$, and we see that this situation is impossible, for some $y_{b^{\prime\prime\prime}}$ where $b^{\prime\prime\prime} > B^{\prime\prime\prime}$.
            \end{proof}

        \item Show that the set $\{x_{n} : n \in\mathbb{N}\}$ is not connected. The set should be assumed to have at least two distinct elements.
            \begin{proof}
                Suppose for contradiction that it was connected. Then we have that $J = \{x_{n} : n \in \mathbb{N}\}$ is a union of two open sets $A_{1}, A_{2}$ such that 
                    \begin{itemize}
                        \item $(A_{1} \cap J) \cup (A_{2} \cap J) = J$,

                        \item $A_{1} \cap J, A_{2} \cap J \neq \emptyset$, 

                        \item $(A_{1} \cap J) \cap (A_{2} \cap J) = \emptyset$. 
                    \end{itemize}
            \end{proof}
    \end{itemize}

\textbf{Exercise 9}: Suppose $f : \mathbb{R} \rightarrow \mathbb{R}$ is twice continuously differentiable (i.e. $f^{\prime}$ and $f^{\prime\prime}$ exist and are continuous on all of $\mathbb{R}$) with $f(0) = 0$ and $f^{\prime}(0) <0$. 
    \begin{itemize}
        \item Prove that there exists $\varepsilon > 0$ such that 
            \begin{equation*}
                \dfrac{f(x)}{x} < 0
            \end{equation*}
        for all $x \in (-\varepsilon, \varepsilon) \backslash \{0\}$.
            \begin{proof}
                We see that 
                    \begin{equation*}
                        f^{\prime}(0) = \lim\limits_{x \to 0}\dfrac{f(x) - f(0)}{x - 0} = \dfrac{f(x)}{x}
                    \end{equation*}
                Since $\frac{f(x)}{x}$ converges to some negative value, we have for $\varepsilon = -\frac{f^{\prime}(0)}{2}$ there is a $\delta$ such that for 
                    \begin{equation*}
                        \lvert x \rvert < \delta
                    \end{equation*}
                we have 
                    \begin{equation*}
                        \left\lvert \dfrac{f(x)}{x} - f^{\prime}(0) \right\rvert < \varepsilon
                    \end{equation*}
                so this shows that $\frac{f(x)}{x}$ is less than $0$ in an interval.
            \end{proof}

        \item Suppose $f(1) = 1$ (recall also from above that $f(0) = 0$ and $f^{\prime}(0) < 0$). Show that for some $x \in [0, 1]$ we must have $f^{\prime\prime}(x) > 0$.
            \begin{proof}
                By the MVT, we can see that there is an $x_{0}$ in $(0, 1)$ such that $f^{\prime}(x_{0}) = 1$. Now applying the mean value theorem again, there exists an $x_{0}^{\prime}$ between $(0, x_{0})$ such that 
                    \begin{equation*}
                        f^{\prime\prime}(x_{0}^{\prime}) = \dfrac{f^{\prime}(x_{0}) - f^{\prime}(0)}{x_{0} - 0}
                    \end{equation*}
                Then 
                    \begin{equation*}
                        f^{\prime\prime}(x_{0}^{\prime}) = \dfrac{1 - f^{\prime}(0)}{x_{0}}
                    \end{equation*}
                and because the numerator and denominator are positive, we have $f^{\prime\prime}(x_{0}^{\prime}) > 0$.
            \end{proof}
    \end{itemize}





\end{document}













