%! TeX root = 	

\documentclass{article}
\usepackage{/Users/trustinnguyen/MyStyle/mystyle}

\title{Final}
\author{Trustin Nguyen}


\begin{document}

\maketitle
\reversemarginpar

\begin{topic}
	\section*{Final}
\end{topic}

\textbf{Exercise 1}: Let $T \in \mathcal{L}(\mathbb{R}^{n}), n \geq 2$. Prove that $\mathbb{R}^{n}$ has a 2-dimensional $T$-invariant subspace. 
\begin{proof}
	Suppose that $p_{\text{min}}$, when fully factored, contains a quadratic term, say $(z^{2} - r_{n})$. Then we use the fact that $p(T)$ is the 0 operator and that each factor sends a specific vector to 0. 
	\begin{align*}
		(T^{2} - r_{n})v = 0 \\
		T^{2}v - r_{n}v = 0 \\
		T^{2}v = r_{n}v
	\end{align*}
	This tells us that $\Span\{v\}$ is invariant under $T^{2}$ and that $Tv$, if added to the span, $\Span\{v, Tv\}$ makes an invariant subspace under $T$. The idea is that $v$ is not quite an eigenvector, but there is an intermediate subspace that $v$ is sent to, and this, when looked at together with $v$ makes a 2 dimensional invariant subspace. Suppose now that $p_{\text{min}}$ contains no quadratic factors. Then $T$ can be written as a diagonal matrix with respect to some basis, lets say $v_{1}, \ldots, v_{n}$. So we now just take $\{v_{1}, v_{2}\}$ to be the invariant subspace because that is how we read off of diagonal matrices. The first vector is an eigenvector and the second can be written as a linear combination of itself with the first. 
\end{proof}

\textbf{Exercise 2}: Let $U_{j}, j \in \mathbb{N}$, be a family of finite-dimensional nested subspaces of a vector space $V$, i.e., 
\begin{equation*}
	U_{1} \subseteq U_{2} \subseteq U_{3} \subseteq \cdots \subseteq U_{k} \subseteq \cdots
\end{equation*}
Prove that 
\begin{enumerate}
	\item [(a)] $U := \bigcup_{j = 1}^{\infty}U_{j}$ is a subspace of $V$;
		\begin{proof}
			We will proceed by induction.

			Base Case: For $n = 1$, since $U_{1}$ is a subspace of $V$, the $U_{1}$ is a subspace of $V$. 

			Inductive Step: Now suppose that $\bigcup_{j = 1}^{n - 1}U_{j}$ is a subspace of $V$. We will show that $\bigcup_{j = 1}^{n}U_{j}$ is a subspace consequently. We check that $0$ is in $\bigcup_{j = 1}^{n}$ which is indeed true since $0$ is in $U_{1}$. Suppose that $v, w \in \bigcup_{j = 1}^{n}$. If both are in $U_{1} \subseteq \ldots \subseteq U_{n - 1}$ then we are done or if both are in $U_{n}$ we are also done, as they are subspaces. Now if wlog $v$ is in $U_{1} \subseteq \ldots \subseteq U_{n-1}$ and $w \in U_{n}$, we are also done as we know that $w \in U_{n}$. So $v + w \in \bigcup_{j = 1}^{n}U_{j}$. Now suppose $\lambda \in \mathbb{F}$ and that $v \in \bigcup_{j = 1}^{n}U_{j}$. Then if $v \in \bigcup_{j = 1}^{n - 1}U_{j}$, then we are done. If $v \in U_{n}$, we are also done as $U_{n}$ is a subspace. Therefore, $\bigcup_{j = 1}^{n}U_{j}$ is a subspace of $V$.
		\end{proof}

	\item [(b)] $\dim{U} \geq \dim{U_{k}}$ for all $k \in \mathbb{N}$.
		\begin{proof}
		We can use the fact that the dimension of a subspace of a vector space does not exceed the dimension of that vector space. So considering that $U_{k} \subseteq \bigcup_{j = 1}^{n}U_{j} \subseteq U$, we can say that $\dim{U_{k}} \leq \dim{\bigcup_{j = 1}^{n}U_{j}} \leq \dim{U}$.
		\end{proof}
\end{enumerate}
\textbf{Exercise 3}: Let $\varphi_{1}, \varphi_{2}, \ldots, \varphi_{m}$ be linearly independent linear functionals on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$. Define the map $T : V \rightarrow \mathbb{F}^{m}$ by the formula
\begin{equation*}
	T(v) := (\varphi_{1}(v), \varphi_{2}(v), \ldots, \varphi_{m}(v)).
\end{equation*}
\begin{enumerate}
	\item [(a)] Prove that $T$ is a linear map.
		\begin{proof}
			Notice that $\mathbb{F}^{m}$ is a vector space, so this is a valid map. Now to prove linearity, we check that if $v, w \in V$, 
			\begin{align*}
				T(v) + T(w) &= (\varphi_{1}(v), \ldots, \varphi_{m}(v)) + (\varphi_{1}(w), \ldots, \varphi_{m}(w)) \\
					    &= (\varphi_{1}(v) + \varphi_{1}(w), \ldots, \varphi_{m}(v) + \varphi_{m}(w)) \\
					    &= (\varphi_{1}(v + w), \ldots, \varphi_{n}(v + w)) \\
					    &= T(v + w)
			\end{align*}
			Where we used the fact that $\varphi$ is linear. Now for multiplication, suppose that $\lambda \in \mathbb{F}$ and that $v \in V$:
			\begin{align*}
				T(\lambda v) &= (\varphi_{1}(\lambda v), \ldots, \varphi_{n}(\lambda v))\\
					     &= \lambda(\varphi_{1}(v), \ldots, \varphi_{n}(v)) \\
					     &= \lambda T(v)
			\end{align*}
			Which concludes the proof.
		\end{proof}
	
	\item [(b)] Determine, with proof, $\dim{\ker{T}}$ and $\dim{\Im{T}}$. When is $T$ invertible?
		\begin{proof}
			Since $\varphi_{1}, \ldots, \varphi_{m}$ are linearly independent, we have some independent list $v_{1}, \ldots, v_{m}$ of $V$ such that
			\begin{align*}
				\varphi_{i}(v_{j}) = \begin{cases}
					1 & \text{if $j = i$} \\
					0 & \text{if otherwise}
				\end{cases}
			\end{align*}
			So we find that for $(\varphi_{1}(v), \ldots, \varphi_{m}(v)) = 0$, we must have that 
			\begin{equation*}
				\varphi_{1}(v), \ldots, \varphi_{m}(v) = 0
			\end{equation*}	
			So this means that for the basis $v_{1}, \ldots, v_{m}, \ldots, v_{n}$ which is an extended basis to $V$. So if 
			\begin{align*}
				v = a_{1}v_{1} + \cdots + a_{n}v_{n}
			\end{align*}
			we have that 
			\begin{align*}
				\varphi_{1}(v) &= a_{2}v_{2} + \cdots + a_{n}v_{n} \\
					       &\vdots \\
				\varphi_{m}(v) &= a_{1}v_{1} + \cdots + a_{m - 1}v_{m - 1} + a_{m + 1}v_{m + 1} + \cdots a_{n}v_{n}
			\end{align*}
			So if all the $\varphi_{i}(v) = 0$, then that means that $T(v)$ is exactly 0 whenever $v$ is written as a linear combination of only the vectors in $\{v_{1}, \ldots, v_{m}\}$. This tells us that $\dim{\ker{T}} = m$ and by rank nullity, $\dim{\Im{T}} = n - m$.
		\end{proof}
\end{enumerate}
\textbf{Exercise 4}: Let $V$ be a finite-dimensional complex vector space, let $T \in \mathcal{L}(V)$ and let $\lambda \in \mathbb{C}$. Using the Jordan normal form of $T$, prove or disprove:
\begin{equation*}
	\dim{\ker{(T - \lambda I)^{3}}} - \dim{\ker{(T - \lambda I)^{2}}} \leq \dim{\ker{(T - \lambda I)^{2}}} - \dim{\ker{(T - \lambda I)^{1}}}.
\end{equation*}
\begin{proof}
	The expression is true. The LHS gives how many Jordan blocks there are that have size greater than $1 \times 1$ while the RHS gives the number of Jordan blocks that have size greater than $2 \times 2$. Clearly, the collection of Jordan blocks that have size greater than $2 \times 2$ is a subset of the collection of Jordan blocks that have a size greater than $1 \times 1$. Therefore, 
	\begin{equation*}
		\dim{\ker{(T - \lambda I)^{3}}} - \dim{\ker{(T - \lambda I)^{2}}} \leq \dim{\ker{(T - \lambda I)^{2}}} - \dim{\ker{(T - \lambda I)^{1}}}.
	\end{equation*}
\end{proof}
\textbf{Exercise 5}: Let $V$ be the real vector space of polynomials in $x$ and $y$ of (total) degree at most $2$, and let $T \in \mathcal{L}(V)$ be defined as follows (you do not need to verity that $T \in \mathcal{L}(V)$; it is so):
\begin{equation*}
	(Tf)(x,y) := (y + 1)\pdv{x}f(x, y) + (x + 1)\pdv{y}f(x, y).
\end{equation*}
Find a basis of $V$ that diagonalizes $T$ and the resulting diagonal matrix representation $\mathcal{M}(T)$.
\begin{proof}
	Note that a basis for $V$ we could start with is $\{1, x, y, xy, x^{2}, y^{2}\}$. We know that $x + 1$ is in the final basis since:
	\begin{align*}
		(Tx)(x,y) := (y + 1)0 + (x + 1)(1) = x + 1
	\end{align*}
	So the shortest dependence so far is $T(x + 1) = x + 1$ which implies that our $p_{\text{min}}$ has factor $(z - 1)$. Also notice that this factor annihilates the basis vector $y + 1$ also. So our next basis is $\{x + 1, y + 1\}$. If we take the vector $1$, we get $(T1) = 0$ so our next factor is $x$: $p_{\text{min}} = z(z - 1)$. Now we can guess the last vectors. Try $(y + 1)^{2}$:
	\begin{align*}
		T(x + 1)^{2} = (2x + 2)(x + 1) \\
		T(x + 1)^{2} - 2(x + 1)^{2} = 0 
	\end{align*}
	Now we guess one for replacing $xy$ which could be $(x + 1)(y + 1)$:
	\begin{gather*}
		T(x + 1)(y + 1) = (y + 1)(x + 1) + (x + 1)(y + 1) \\
		Tv - 2v = 0
	\end{gather*}
	So our minimal polynomial is $p_{min} = z(z - 1)(z - 2)$. Our basis is now $\{1, x + 1, y + 1, (x + 1)(y + 1), (x + 1)^{2}, (y + 1)^{2}\}$.
\end{proof}
\textbf{Exercise 6}: Find a function $f \in  \Span\{1, \cos{x}, \sin{x}\}$ which minimizes the integral
\begin{equation*}
	\int_{0}^{2\pi} \lvert x + 1 - f(x) \rvert^{2} \,\dd{x}.
\end{equation*}
\begin{proof}
	We observe that this can be minimized by projecting the vector $x + 1$ onto the space spanned by $\{1, \cos{x}, \sin{x}\}$. So we start by matching inner products:
	\begin{align*}
		\langle x + 1, 1 \rangle &= \langle a_{0} + a_{1}\cos{x} + a_{2}\sin{x} \rangle \\
		\langle x + 1, \cos{x} \rangle &= \langle a_{0} + a_{1}\cos{x} + a_{2}\sin{x} \rangle \\
		\langle x + 1, \sin{x} \rangle &= \langle a_{0} + a_{1}\cos{x} + a_{2}\sin{x} \rangle
	\end{align*}
	After all that computation, we get the system of equations:
	\begin{align*}
		2\pi^{2} + 2\pi &= 2\pi a_{0} \\
		0 &= a_{1}\pi \\
		-2\pi &= a_{2}\pi 
	\end{align*}
	We therefore, have solved for the values $a_{0}, a_{1}, a_{2}$ that minimize the function:
	\begin{equation*}
		( \pi + 1 ) + 0\cos{x} - 2\sin{x}
	\end{equation*}
\end{proof}
\textbf{Exercise 7}: Let $T$ be a self-adjoint operator and let $S$ be a positive operator on a complex finite-dimensional inner product space. Prove that all eigenvalues of $ST$ are real.

\textbf{Exercise 8}: Consider the complex inner product space 
	\begin{align*}
		V = \Span\{1, \cos{x}, \sin{x}\}
	\end{align*}
with the inner product
\begin{equation*}
	\langle f, g \rangle = \int_{-\pi}^{\pi} f(x)\overline{g(x)} \,\dd{x}
\end{equation*}
and the operator $T = I + D^{2}: f(x) \mapsto f(x) + f^{\prime\prime}(x)$.
\begin{enumerate}
	\item [(a)] Is $T$ self-adjoint? Explain.
		\begin{proof}
			$T$ is indeed self-adjoint. We will look at the the matrix representation of $T$ by looking at its action on the basis vectors:
			\begin{align*}
				1 & \mapsto 1 \\ 
				\cos{x} & \mapsto \cos{x} + (-\cos{x}) \\
				\sin{x} & \mapsto \sin{x} + (-\sin{x})
			\end{align*}
			therefore, $T$ is 
			\begin{align*}
				\mathcal{M}(T) = 
				\begin{bmatrix}
					1 & 0 & 0 \\
					0 & 0 & 0 \\
					0 & 0 & 0 \\
				\end{bmatrix}
			\end{align*}
			which is self-adjoint as $T = \overline{T}^{\perp}$
		\end{proof}

	\item [(b)] Determine the singular value decomposition of $T$.
		\begin{proof}
			We can find the singular values of $T$ through the matrix representation of $T^{*}T$:
			\begin{align*}
				\mathcal{M}(T^{*}T) = 
					\begin{bmatrix}
						1 & 0 & 0 \\
						0 & 0 & 0 \\
						0 & 0 & 0 \\
					\end{bmatrix}
			\end{align*}
			Therefore, our singular values are $\sigma_{1} = 1$, $\sigma_{2} = 0$, and $\sigma_{3} = 0$. Now the goal is to represent the image of $T$ as 
			\begin{align*}
				Tv = \sigma_{1}\langle v, e_{1} \rangle f_{1} + \sigma_{2}\langle v, e_{2} \rangle f_{2} + \sigma_{3}\langle v, e_{3} \rangle f_{3}
			\end{align*}
			for some $f_{i} \in \Span\{1, \cos{x}, \sin{x}\}$ and $e_{1}, e_{2}, e_{3}$ orthonormal basis vectors. We can orthonormalize the vectors in $\Span\{1, \cos{x}, \sin{x}\}$ which we have done before:
			\begin{equation*}
				\left\{\dfrac{1}{\sqrt{2\pi}}, \dfrac{\cos{x}}{\pi}, \dfrac{\sin{x}}{\pi}\right\}
			\end{equation*}
			Now we just take 
			\begin{align*}
				f_{1} &= \dfrac{T(\frac{1}{\sqrt{2\pi}})}{1}
			\end{align*}
			which is all that is needed because $T$ sends the other basis vectors to 0. Our singular value decomposition is
			\begin{equation*}
				Tv = 1 \left\langle v, \dfrac{1}{\sqrt{2\pi}} \right\rangle \dfrac{1}{\sqrt{2\pi}}
			\end{equation*}
		\end{proof}
\end{enumerate}

\textbf{Exercise 9}: Decide if the following implications hold in the settings below. No need to justify your answers. You will receive 2pts for each correct answer, 1 pt for each black answer, opts for each incorrect answer. Please circle the best answer.
\begin{enumerate}
	\item [(a)] $\{v_{1}, \ldots, v_{k}\}^{\perp} = (\Span\{v_{1}, \ldots, v_{k}\})^{\perp}$ for any $v_{1}, \ldots, v_{k} \in V$.
		\begin{center}
			\begin{tabular}{c c c}
				$\boxed{\text{ALWAYS TRUE}}$ & TRUE ONLY IN FINITE DIMENSION & FALSE
			\end{tabular}
		\end{center}
	\item [(b)] $S, T \in \mathcal{L}(V), (\dim{V} < \infty)$ commute if and only if their matrix representations commute.
		\begin{center}
			\begin{tabular}{c c c}
				ALWAYS TRUE & $\boxed{\text{TRUE ONLY IF USING SAME BASIS}}$ & FALSE
			\end{tabular}
		\end{center}
	\item [(c)] If $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, W)$ is invertible, the $\dim{V} = \dim{W}$.
		\begin{center}
			\begin{tabular}{c c c}
				$\boxed{\text{TRUE OVER $\mathbb{C}$ and $\mathbb{R}$}}$ & TRUE OVER $\mathbb{R}$ BUT NOT $\mathbb{C}$ & FALSE
			\end{tabular}
		\end{center}
	\item [(d)] Any normal operator on a finite-dimensional space is diagonalizable.
		\begin{center}
			\begin{tabular}{c c c}
				ALWAYS TRUE & $\boxed{\text{TRUE OVER $\mathbb{C}$ BUT NOT $\mathbb{R}$}}$ & FALSE
			\end{tabular}
		\end{center}
	\item [(e)] If $T \in \mathcal{L}(V)$ is invertible, then $T^{\prime}$ is.
		\begin{center}
			\begin{tabular}{c c c}
			    $\boxed{\text{ALWAYS TRUE}}$ & TRUE ONLY IN FINITE DIMENSION & FALSE
			\end{tabular}
		\end{center}

\end{enumerate} 




\end{document}

