%! TeX root = Downloads/Berkeley/Math/Algebra_Chapter_0/Exercises.tex

\documentclass{report}
\usepackage{/Users/trustinnguyen/.mystyle/math/packages/mypackages}
\usepackage{/Users/trustinnguyen/.mystyle/math/commands/mycommands}
\usepackage{/Users/trustinnguyen/.mystyle/math/environments/report}

\title{Algebra Notes}
\author{Trustin Nguyen}


\begin{document}
\newgeometry{
	total={150mm,235mm},
}

\begin{titlepage}
    \maketitle
\end{titlepage}
\tableofcontents
\restoregeometry
\reversemarginpar

\chapter{Set Theory and Categories}

\begin{topic}
    \section{Naive Set Theory}
\end{topic}

\begin{definition}[\label{def:1.1.1}]{Equivalence Relation}
    An equivalence relation on a set $S$ is any relation $\sim $ satisfying these three properties:
    \begin{itemize}
        \item \textit{reflexivity:} $(\forall a \in S) a \sim a$;

        \item \textit{symmetry:} $(\forall a \in S) a \sim b \implies b \sim a$;

        \item \textit{transitivity}: $(\forall a \in S)(\forall b \in S)(\forall c \in S), (a \sim b \text{ and } b \sim c) \implies a \sim c$.
    \end{itemize}
\end{definition}

A partition of $S$ can be obtained through equivalence classes which are defined as
    \begin{equation*}
        [a]_{\sim } := \{b \in S : b \sim a\}
    \end{equation*}
This leads to the definition that shows that partitions of a set $S$:

\begin{definition}[\label{def:1.1.2}]{Quotient Set}
    The \textit{quotient} of the set $S$ with respect to the equivalence relation $\sim $ is the set
    \begin{equation*}
        S/\sim := \mathcal{P}_{\sim }
    \end{equation*}
    of equivalence classes of elements of $S$ with respect to $\sim $.
\end{definition}

\begin{examples}
    \begin{example}[\label{exm:1.1.3}]
        Take $S = \mathbb{Z}$, and let $\sim $ be the relation defined by
            \begin{equation*}
                a \sim b \iff a - b \text{ is even }
            \end{equation*}
        Then $\mathbb{Z}/ \sim $ consists of two equivalence classes:
            \begin{equation*}
                \mathbb{Z} /\sim = \{[0]_{\sim }, [1]_{\sim }\}
            \end{equation*}
    \end{example}
\end{examples}

\begin{exercises}{\textbf{Problem Sets}}
    \textbf{Exercise 1}: Locate a discussion of Russell's paradox, and understand it.

    \begin{answer}
        Russell's paradox states that any set theory which contains the proposition that there exists a set for every proposition, that is
        \begin{align*}
            \forall p_{1}, p_{2}, \ldots, p_{n} \exists S \forall s (s \in S \iff \varphi(x, p_{1}, p_{2}, \ldots, p_{n}))
        \end{align*}
        would contain a contradiction. Specifically, it was pointed out that the set of all sets that do not contain themselves was impossible to construct. If we follow from the definition, suppose that $R$ is the set of sets that do not contain themselves. We have two cases. If $R \notin R$, then we would observe that $R$ must contain itself so $R \in R$. Which is a contradiction. If we have the latter case that $R \in R$, then $R$ is in the set of sets that do not contain itself. Therefore, $R \notin R$ which is also a contradiction.
    \end{answer}

    \textbf{Exercise 2}: Prove that if $\sim $ is a relation on a set $S$, then the corresponding family $\mathcal{P}_{\sim }$ is a partition of $S$: that is, its elements are nonempty, disjoint, and their union is $S$.

    \begin{answer}
        Since we define each equivalence to be reflexive, that means that for a given element $s \in S$, its equivalence class is nonempty as $s \sim s$. Now suppose that we have two equivalence classes: 
        \begin{equation*}
            s_{1} \in [a], \, [b]
        \end{equation*}
        and that $[a] \neq [b]$. Suppose that $s_{1} \in [b]$ also. We will show that $[a] \cap [b] = \emptyset$ otherwise, $[a] = [b]$. Since $s_{1} \in [b]$, following from the definition of equivalence class:
        \begin{equation*}
            [b] = {s \in S : s \sim b}
        \end{equation*}
        we find that $s_{1} \sim b$. Now let $b_{i} \in [b]$ be arbitrary. Then $s_{1} \sim b$, $b_{i} \sim b \implies b \sim b_{i}$. By transitivity, we have that $s_{1} \sim b \sim b_{i}$. So an arbitrary element of $[b]$ must be in $[a]$: $[b] \subseteq [a]$. Since $[b] \neq \emptyset$ we have that some $b \in [b]$ is in $[a]$. But the same argument now, we conclude that $[a] = [b]$. Therefore, the equivalence classes are disjoint, otherwise they are the same. We can disregard these duplicates as $\mathcal{P}_{\sim }$ is a set. We can show that the equivalence classes partition $S$ since every element must belong to some equivalence class. Take an arbitrary $s \in S$. Then $s \in [s]$. We are done.
    \end{answer}

    \textbf{Exercise 3}: Given a partition $\mathcal{P}$ on a set $S$, show how to define a relation $\sim $ on $S$ such that $\mathcal{P}$ is the corresponding partition.

    \begin{answer}
        Given a partition of a set $S$, we note that it is of the form $S = \bigcup_{i = 1}^{n}S_{i}$ where $S_{i} \cap S_{j} = \emptyset$ for $i \neq j$. Now we say that $a \sim b \iff (a \in S_{i} \iff  b \in S_{i})$. This says that $a, b$ are related if only if they belong in the same set. This is an equivalence relation, which we will prove. Observe that in a partition, $a \in S_{i} \iff a \in S_{i}$. Therefore, $a \sim a$. It satisfies the reflexive property. As for the symmetric, we note that if $a \sim b$, then that means that $a \in S_{i} \iff b \in S_{i}$. But that means that $b \in S_{i} \iff a \in S_{i}$. Therefore, $b \sim a$ which is the symmetric property. Now for transitive, suppose that $a_{1} \sim a_{2} \land a_{2} \sim a_{3}$. Then $a_{1} \in S_{i} \iff a_{2} \in S_{i}$ and $a_{2} \in S_{j} \iff a_{3} \in S_{j}$. By the fact that the partitions are disjoint, that means that if $a_{1} \in S_{i}$, then $a_{2} \in S_{i}$. But that means that $a_{3} \in S_{i}$. And if $a_{3} \in S_{i}$, then it follows that $a_{1} \in S_{i}$ since we can argue the same backwards. So $a_{1} \sim a_{3}$. We are done as this is a proper equivalence relation.
    \end{answer}

    \textbf{Exercise 4}: How many different equivalence relations may be defined on the set $\{1, 2, 3\}$?

    \begin{answer}
        Given an equivalence class, observe that we can map it to a partition, as done in Exercise 2. In Exercise 3, we defined a partition that maps to an equivalence class. There is a clear bijection between the partitions of a set and the equivalence classes of a set, namely, we map a set of partitions
        \begin{align*}
            {S_{1}, S_{2}, \ldots, S_{n}}
        \end{align*}
        to which we have $s_{i} \in S_{i}$ and we map $s_{i} \in [s_{i}]$. Observe that each partition gets mapped to an equivalence class. This mapping is injective and surjective. So the number of ways to partition a set is the number of equivalence classes there are. Therefore, since $\{1, 2, 3\}$ has partitions:
        \begin{gather*}
            \{1\} \cup \{2\} \cup \{3\}, \{1, 2\} \cup \{3\} \\
            \{1, 3\} \cup \{2\}, \{2, 3\} \cup \{3\} \\
            \{1, 2, 3\}
        \end{gather*}
        which count to 5, there are 5 equivalence relations.
    \end{answer}

    \textbf{Exercise 5}: Give an example of a relation that is reflexive and symmetric but not transitive. What happens if you attempt to use this relations to define a partition on a set?

    \begin{answer}
        When you do this, elements that are in the same equivalence class might not be related. For example, we might have that $a_{1}, a_{2} \in [a]$:
        \begin{align*}
            [a] = \{s \in S : s \sim a\}
        \end{align*}
        so we have that $a_{1} \sim a, a_{2} \sim a$ but not necessarily that $a_{1} \sim a_{2}$. In fact, this may belong in a different equivalence class also. So our relation might not represent a partition of the set. We might define a relation such that $\lvert a - b \rvert < 2 \iff a \sim b$. Notice that $a \sim a$, if $a \sim b$, then $\lvert a - b \rvert < 2 \implies \lvert b - a \rvert < 2 \implies b \sim a$. As for the transitivity, we have that $4 \sim 3$, $3 \sim 2$, but $4 \not\sim 2$ since $\lvert 4 - 2 \rvert \not< 2$.
    \end{answer}

    \textbf{Exercise 6}: Define a relation $\sim $ on the set $\mathbb{R}$ of real numbers by setting $a \sim b \implies b - a \in \mathbb{Z}$. Prove that this is an equivalence relation, and find a `compelling' description for $\mathbb{R} / \sim $. Do the same for the equivalence relation $\equiv $ on the plane $\mathbb{R} \times \mathbb{R}$ defined by declaring $(a_{1}, a_{2}) \equiv (b_{1}, b_{2}) \implies b_{1} - a_{1} \in \mathbb{Z}$ and $b_{2} - a_{2} \in \mathbb{Z}$.

    \begin{answer}
        (Part I) We just need to check the properties. Notice that $a - a = 0$ so $a \sim a$. Also, if $a - b = z\in \mathbb{Z}$, then we conclude that $b - a = -z \in \mathbb{Z}$. Therefore, the symmetry property is satisfied. Suppose that $a - b = z_{1} \in \mathbb{Z}$ and that $b - c = z_{2} \in \mathbb{Z}$. Then $a - c = z_{1} + z_{2} \in \mathbb{Z}$ showing the transitive property. The equivalence classes would look something like $[a]$ for $\lvert a \rvert < 1$ since elements that are related are ones that have the same decimal part. 

        (Part II) We will check the properties for this also. Notice that it is reflexive as $(a_{1}, a_{2}) - (a_{1}, a_{2}) = (0, 0) \in \mathbb{Z} \times \mathbb{Z}$. Now note that it is also symmetric since we would take the negative of both parts when reversing the relation which is also in $\mathbb{Z}$. To prove transitivity, suppose $(a_{1}, a_{2}) \approx (b_{1}, b_{2})$ and $(b_{1}, b_{2}) \approx (c_{1}, c_{2})$. Then we have that $b_{1} - a_{1} = z_{1} \in \mathbb{Z} \land c_{1} - b_{1} = z_{2} \in \mathbb{Z}$. This means that $c_{1} - a_{1} = z_{2} + z_{1} \in \mathbb{Z}$. The same argument works for the second component. Therefore, $(a_{1}, a_{2}) \approx (c_{1}, c_{2})$.
    \end{answer}
\end{exercises}

\newpage
\begin{topic}
    \section{Functions Between Sets}
\end{topic}

\begin{definition}[\label{def:1.2.2}]{Functions}
    A function is one which takes an element from one set, mapping it to an element of another set. The definition will be based on the idea of a \textit{graph} of $f$, our function:
    \begin{equation*}
        \Gamma_{f} := \{(a,b) \in A \times B : b = f(a)\} A \times B
    \end{equation*}
\end{definition}

The restrictions on such a construction is that
    \begin{equation*}
        (\forall a \in A)(\exists !b \in B) \, (a, b) \in \Gamma_{f},
    \end{equation*}
So each element of $A$ is sent to one element in $B$. We also have the identity function, mapping every element to itself:
    \begin{equation*}
        \text{id}_{A}: A \rightarrow A
    \end{equation*}
This naturally leads to the idea that if we have a subset of $A$, $S$, the identity would be sending every element of $s \in S$ to itself. If $S$ is a subset of $f$, then we have
    \begin{equation*}
        f(S) := \{b \in B : (\exists a \in A) \, b = f(a)\} 
    \end{equation*}
This is the set of elements in the image of the mapping $f : S \rightarrow B$. In other words, this is a domain restriction which we denote by
    \begin{equation*}
        (\forall s \in S) : \, f \mid_{S}(s) = f(s)
    \end{equation*}

\begin{examples}
    \begin{itemize}
        \item Multisets: A multiset has multiple elements in one set. It can be defined by a function from a set to $\mathbb{N}_{> 0}$ which maps an element to the number of occurrences of that element in that set.

        \item Indices: We can consider indices as functions also where if we consider $a_{1}, a_{2}, \ldots, a_{n}$, what we are meaning is a function 
            \begin{equation*}
                a : \{1, \ldots, n\} \rightarrow \mathbb{Z}\ldots
            \end{equation*}
            Where the RHS is where we have our elements of $a_{i}$. This means that we can have some $a_{i} = a_{j}$ or as $a_{i}$'s distinct.
    \end{itemize}
\end{examples}

\begin{definition}[\label{def:1.2.3}]{Composition of functions}
    Functions can be composed if $f : A \rightarrow B$ and $g : B \rightarrow C$ are functions. There are operations $f \circ g$ such as:
        \begin{equation*}
            (\forall a \in A) \, (g \circ g)(a) := g(f(a))
        \end{equation*}
    that is, we use $f$ to go from $A$ to $B$ and then $g$ to go from $B$ to $C$. Here is a graphical representation:
        \begin{center}
            \begin{tikzcd}
                A\ar[r, "f"]\ar[rr, "g \circ f"', bend right = 20] & B\ar[r, "g"] & C & \text{or} & A\ar[r, "f"]\ar[dr, "g \circ f"'] & B\ar[d, "g"] \\
                                                                   &              &   &           &                                   & C               
            \end{tikzcd}
        \end{center}
\end{definition}

We say that diagrams such as one drawn above commute if we start from $A$ and travel to $C$ in any of the two ways applying the functions along the way give rise to the same result.

Also note that composition is associative, where if we have $f: A \rightarrow B$, $g: B \rightarrow C$, and $h: c \rightarrow D$ which are functions, then $h \circ (g \circ f) = (h \circ g) \circ f$. This is the diagram:
    \begin{center}
        \begin{tikzcd}
            A\ar[r, "f"]\ar[rr, "g \circ f"', bend right = 20] & B\ar[r, "g"]\ar[rr, "h \circ g", bend left = 20] & C\ar[r, "h"] & D   
        \end{tikzcd}
    \end{center}
which commutes. This diagram says that $(g \circ f) \circ h = f \circ g \circ h = f \circ (g \circ h)$.

The identity function has two representations with respect to composition. Let $f : A \rightarrow B$, then we have $\text{id}_{B}\circ f = f$ and $f \circ \text{id}_{A} = f$. This is represented by the diagrams:
    \begin{center}
        \begin{tikzcd}
            A\ar[r, "f"]\ar[rr, "f"', bend right = 20] & B\ar[r, "\text{id}_{B}"] & B, & A\ar[r, "\text{id}_{A}"]\ar[rr, "f"', bend right = 20] & A\ar[r, "f"] & B \\
                                                       &                           &    &                                                        &              &     
        \end{tikzcd}
    \end{center}
which commute.

\begin{definition}[\label{def:1.2.4}]{Injections, Surjections, Bijections}
    We have special types of functions:
        \begin{itemize}
            \item A function $f : A \rightarrow B$ is \textit{injective} if 
                \begin{equation*}
                    a^{\prime} \neq a^{\prime\prime} \implies f(a^{\prime}) \neq f(a^{\prime\prime})
                \end{equation*}
                that is, $f$ sends different elements to different elements.

            \item A function $f : A \rightarrow B$ is \textit{surjective} if
                \begin{equation*}
                    (\forall b \in B)(\exists a \in A) \, b = f(a)
                \end{equation*}
                of that $f$ covers the whole of $B$.
        \end{itemize}
    For injections, we draw $\hookrightarrow$ and for surjections, we draw $\twoheadrightarrow$.

    If $f$ is both injective and surjective, we say that it is bijective and we write $f : A \tilde{\rightarrow} B$, or 
        \begin{equation*}
            A \cong B
        \end{equation*}
    and we say that $A, B$ are isomorphic sets.

    This allows us to consider `disjoint unions' which are the `copies' $A^{\prime}, B^{\prime}$ of the sets $A, B$, which we should consider as isomorphic sets to $A, B$. For example, we can take a copy defined by a bijective function:
        \begin{equation*}
            f : A \rightarrow {0} \times A
        \end{equation*}
    defined by 
        \begin{equation*}
            (\forall a \in A) \, f(a) = (0, a)
        \end{equation*}
\end{definition}

\begin{definition}[\label{def:1.2.5}]{Injections, Surjections, Bijections: Second Viewpoint}
    We can also note that if $f : A \rightarrow B$ is a bijection, we can flip the graph and define
        \begin{equation*}
            g : B \rightarrow A
        \end{equation*}
    where we have $a = g(b)$ when $b = f(a)$. Since $f$ is injective, we guarantee that no element in $B$ gets mapped to two elements by $g$. Since the map is surjective, the mapping given by $g$ is defined over the entire domain $B$. So $g$ is a function. 
\end{definition}

The graphical representation of $g$ is interesting:
        \begin{center}
            \begin{tikzcd}
                A\ar[r, "f"]\ar[rr, "\text{id}_{A}"', bend right = 20] & B\ar[r, "g"] & A, & B\ar[r, "g"]\ar[rr, "\text{id}_{B}"', bend right = 20] & A\ar[r, "f"] & B   
            \end{tikzcd}
        \end{center}
    which both commute, so we actually have bidirectional arrows and $g \circ f = \text{id}_{A}$ and $f \circ g = \text{id}_{B}$. The first identity shows that $g$ is a left-inverse of $f$ while the second shows that it is also a right-inverse. We way that it is the inverse of $f$ which is denoted as $f^{-1}$. Is the converse true? That if a function has an inverse that it is a bijection?

\textbf{Proposition 2.1}: \textit{Assume} $A \neq \emptyset$, and let $f : A \rightarrow B$ be a function. Then
    \begin{enumerate}
        \item $f$ has a left-inverse if and only if it is injective.

        \item $f$ has a right-inverse if and only if it is surjective. 
    \end{enumerate}

\begin{proof}
    (Part I) $(\rightarrow )$ If $f : A \rightarrow B$ has a left-inverse, then there exists a $g : B \rightarrow A$ such that $g \circ f = \text{id}_{A}$. Now suppose that $a^{\prime} \neq a^{\prime\prime}$ are arbitrary different elements of $A$. Then
        \begin{equation*}
            g(f(a^{\prime})) = \text{id}_{A}(a^{\prime}) = a^{\prime} \neq a^{\prime\prime} = \text{id}_{A}(a^{\prime\prime}) = g(f(a^{\prime\prime}))
        \end{equation*}
        So as $g$ sends $f(a^{\prime})$ and $f(a^{\prime\prime})$ to different elements, $f(a^{\prime}) \neq f(a^{\prime\prime})$. 

    ($\leftarrow $) Now suppose that $f : A \rightarrow B$ is injective. The we construct a function $g : B \rightarrow A$ by assigning a unique value $g(b) \in A$ for each element $b \in B$. Choose a fixed element in $s \in A$
        \begin{equation*}
            g(b) := 
                \begin{cases}
                    a & \text{if } b = f(a) \text{ for some $a \in A$} \\
                    s & \text{if } b \notin \Im{f}.
                \end{cases}
        \end{equation*}
        So if $b$ is the image of some element in $A$, then we send it back to that element. Otherwise, you send it to some fixed $A$. So every $b \in B$ is sent to a well-defined element in $A$. It can also be seen that $g$ is a left inverse of $g$. The proof of (2) is left as an exercise (Exercise 2.2).
\end{proof}

\textbf{Corollary 2.2}: A function $f : A \rightarrow B$ is a bijection if and only if it has a two-sided inverse.

    A function that is injective will have many left-inverses and a function that is surjective will have many right-inverses. These are called sections.

    The proposition shows something deep. The definition of injective and surjective maps looks at elements of sets, and how these functions are organized based on our sets. The definition of elements could be vague but ideas about injectivity and surjectivity could be deduced as properties of functions. They could be seen as a way of comparing sets.

    One extra note on bijections, we denote the inverse of $f$ to be $f^{-1}$ where $f : A \rightarrow B$ and if $T \subseteq B$, then we write 
        \begin{equation*}
            f^{-1}(T) = \{a \in A : f(a) \in T\}
        \end{equation*}
    If $T = \{q\}$ is a singleton set, we can write $f^{-1}(T) = f^{-1}(q)$ which is called a \textit{fiber} of $f$ over $q$. So a function $f : A \rightarrow B$ is a bijection if it has non-empty fibers over all elements of $B$ and that each of these fibers are singletons.

\begin{definition}[\label{def:1.2.6}]{Monomorphism}
    A function $f: A \rightarrow B$ is a \textit{monomorphism} if the following holds:
    \begin{gather*}
        \text{\textit{for all sets $Z$ and all functions $\alpha^{\prime}, \alpha^{\prime\prime}: Z \rightarrow A$}} \\
        f \circ \alpha^{\prime} = f \circ \alpha^{\prime\prime} \implies \alpha^{\prime} = \alpha^{\prime\prime}
    \end{gather*}
\end{definition}

We can look at this definition and notice that it looks like the injective requirement for a function.

\textbf{Proposition 2.3}: A function is injective if and only if it is a monomorphism.
    \begin{proof}
        $(\rightarrow )$ If it is injective, it has a left inverse $g: B \rightarrow A$. Therefore, we have that 
        \begin{align*}
            g \circ (f \circ \alpha^{\prime}) &= g \circ (f \circ \alpha^{\prime\prime}) \\
            \text{id}_{A} \circ \alpha^{\prime} &= \text{id}_{A} \circ \alpha^{\prime\prime} \\
            \alpha^{\prime} &= \alpha^{\prime\prime}
        \end{align*}

    $(\leftarrow )$ Now suppose that $f$ is a monomorphism. To show injectivity, we only need to look at one element. So choosing $Z = {p}$, we have that 
        \begin{align*}
            f \circ \alpha^{\prime}(p) &= f \circ \alpha^{\prime\prime}(p) \\
            \alpha^{\prime}(p) &= \alpha^{\prime\prime}(p) \\
            a^{\prime} &= a^{\prime\prime}
        \end{align*}
    which is by definition of an injective function.
    \end{proof}

It is to be expected that there would be a definition as such for a surjective function. This is known as an \textit{epimorphism} which will be proved in Exercise 2.5.

\begin{examples}
    \begin{example}[\label{exm:1.2.4}]
        If $A, B$ are sets, then the natural projections 
            \begin{center}
                \begin{tikzcd}
                      & A \times B \ar[dl, "\pi_{A}"']\ar[dr, "\pi_{B}"] &   \\
                    A &                                                  & B   
                \end{tikzcd}
            \end{center}
        are defined by
        \begin{equation*}
            \pi_{A}((a, b)) := a, \, \pi_{B}((a, b)) := b
        \end{equation*}
        for all $(a, b) \in A \times B$. These maps are definitely surjective.
    \end{example}

    \begin{example}[\label{exm:1.2.5}]
        There are also injections from $A$ and $B$ to the disjoint union.
        \begin{center}
            \begin{tikzcd}
                A\ar[dr, ""] &             & B \ar[dl, ""] \\
                             & A \coprod B &                 
            \end{tikzcd}
        \end{center}
        obtained by sending $a \in A$ to the element in the isomorphic copy in $A \coprod B$.
    \end{example}

    \begin{example}[\label{exm:1.2.6}]
        If $\sim $ is an equivalence relation on a set $A$, there is a canonical projection
        \begin{equation*}
            A \rightarrow A / \sim 
        \end{equation*}
        by sending the element $a$ to its equivalence class: $[a]$.
    \end{example}

    \begin{example}[\label{exm:1.2.7}]
        Canonical decomposition. Injective and surjective functions allow us to construct any function. We note that every function $f : A \rightarrow B$ determines an equivalence relation: for all $a^{\prime}, a^{\prime\prime} \in A$,
        \begin{equation*}
            a^{\prime} \sim a^{\prime\prime} \iff f(a^{\prime}) = f(a^{\prime\prime})
        \end{equation*}
    \end{example}
\end{examples}

\begin{theorem}[\label{thm:1.2.7}]{First Isomorphism Theorem}
    Let $f : A \rightarrow B$ be any function, and define $\sim $ as above. Then $f$ decomposes as follows
    \begin{center}
        \begin{tikzcd}
            A\ar[r, ""]\ar[rrr, "f", bend left = 20] & A/\sim \ar[r, "\sim", "f"'] & \Im{f}\ar[r, ""] & B   
        \end{tikzcd}
    \end{center}
\end{theorem}

We have that the first function is the canonical projection $A \rightarrow A/\sim $. The third one is the identity from $\Im{f} \subseteq B$. Out goal is to quotient out all the non-injective domain values, that is each equivalence class to one value in $b$. We represent each equivalence class by one member, and therefore, out middle mapping is defined by
\begin{equation*}
    \tilde{f}([a]_{\sim }) := f(a)
\end{equation*}
for all $a \in A$. Observe that this formula for $\tilde{f}$ is ambiguous, as we need to make sure that we get the same output for any representative of out equivalence class. We also need to prove that this formula is a function and defines a bijection.
\begin{proof}
    Observe that by the defined equivalence class, for $a^{\prime}, a^{\prime\prime} \in A$, we need to prove that to show that equivalence representatives do not matter
    \begin{equation*}
        [a^{\prime}]_{\sim } = [a^{\prime\prime}]_{\sim } \implies f(a^{\prime}) = f(a^{\prime\prime})
    \end{equation*}
    Since $[a^{\prime}]_{\sim } = [a^{\prime\prime}]_{\sim }$ means that $a^{\prime} \sim a^{\prime\prime}$, we have that $f(a^{\prime}) = f(a^{\prime\prime})$. Therefore, our choice of representatives does not matter.

    Now we have to show that $\tilde{f}$ is a bijection: $\tilde{f} : A/ \sim \rightarrow \Im{f}$. For the injective part, we reason that if $\tilde{f}([a]) = \tilde{f}([b])$, then we must have that they are the same equivalence class. We note that $\tilde{f}([a])$ is just $f(a)$ and $\tilde{f}([b])$ is $f(b)$. Now using the equivalence relation we defined, we have that $a \sim b$. Therefore, $[a] = [b]$.

    For surjectivity, we recall our function: $\tilde{f} : A/\sim \rightarrow \Im{f}$. Let $b \in \Im{f}$ be arbitrary. Notice that we only lose elements in $A /\sim $ which map to the same element. So we should in theory still have a surjective map that is `minimal'. Namely, we note that $f(a) = b$. Therefore, we take $f([a]) = b \in \Im{f}$.
\end{proof}

The proof is very interesting as it shows that we can define an equivalence relation on the elements in $A$ that map to the same element in $B$. And this actually gives us a bijection if we remove these elements. To conclude, we can decompose any function into one that is surjective function, a bijection, then an inclusion map/injection.

On a side note, notice that the definition of a disjoint union can have several choices for our copies of $A^{\prime}, B^{\prime}$. This tells us that $A \coprod B$ is not will-defined, yet we will realize that $A \coprod B$ is well-defined up to isomorphism: Any two choices for $A^{\prime}, B^{\prime}$ will lead to isomorphic candidates for $A \coprod B$. This applies to quotients and products.

\begin{exercises}{\textbf{Problem Sets}}
    \textbf{Exercise 1}: How many different bijections are there between a set $S$ with $n$ elements and itself?

    \begin{answer}
        We start by labeling the elements $1, \ldots, n$. Then we have $n!$ choices.
    \end{answer}

    \textbf{Exercise 2}: Prove statement $(2)$ in Proposition 2.1, that is that $f : A \rightarrow B$ has a right-inverse if and only if it is surjective. 
    
    \begin{answer}
        (Part I) Suppose that $f$ has a right-inverse, $f^{-1} : B \rightarrow A$. We take an arbitrary element of $B$, called $b$. Now observe that
            \begin{align*}
                f \circ f^{-1} (b) &= b \\
                f (a) &= b
            \end{align*}
        Therefore, $f$ is surjective.

        (Part II) Now suppose that $f$ is surjective. Consider the decomposition equivalence relation such that 
            \begin{equation*}
                a^{\prime} \sim a^{\prime\prime} \iff f(a^{\prime}) = f(a^{\prime\prime})
            \end{equation*}
        Consider the mapping $f^{\prime} : B \rightarrow A/\sim $ by:
            \begin{equation*}
                f^{\prime} := b \mapsto [a] : f(a) = b
            \end{equation*}
        Now we consider the mapping $f^{\prime\prime} : A/\sim \rightarrow A$ by:
            \begin{equation*}
                f^{\prime\prime} := [a] \mapsto a
            \end{equation*}
        Notice that now, our function $f^{\prime\prime}f^{\prime}$ is our $f^{-1}$ function. Take an arbitrary $b \in B$ where $f(a) = b$. Now plug in:
            \begin{equation*}
                ff^{\prime\prime}f^{\prime}(b) = ff^{\prime\prime}([a]) = f(a) = b
            \end{equation*}
        So it is the identity on $B$.
    \end{answer}

    \textbf{Exercise 3}: Prove that the inverse of a bijection is a bijection and that the composition of two bijections is a bijection.

    \begin{answer}
        (Part I) We note that $f : A \rightarrow B$ is a bijection then it is both injective and surjective. What follows is that by definition, $f^{-1}: B \rightarrow A$ is such that $f \circ f^{-1} = \text{id}_{A}$ and $f^{-1} \circ f = \text{id}_{B}$. But by definition, $f^{-1}$ has a left and right inverse which is $f$. So $f^{-1}$ is a bijection. 

        (Part II) Suppose that we have two bijections $f, g$. A left-inverse for both exists so a left inverse of $f \circ g$ exists, namely $g^{-1} \circ f^{-1}$. The same for a right-inverse. And we're done.
    \end{answer}

    \textbf{Exercise 4}: Prove that `isomorphism' is an equivalence relation (on any set of sets).
 
    \begin{answer}
        This is asking that if we consider a set of sets, then two sets are considered isomorphic can define an equivalence relation. To do this, define the relation as
            \begin{equation*}
                A \sim B \iff \exists f (f : A \rightarrow B \land \exists f^{-1}(f \circ f^{-1} = I \land  f^{-1} \circ f = I))
            \end{equation*}
        So using this, we note that the relation is reflexive as a set is isomorphic to itself, take the identity function:
            \begin{align*}
                \text{id}_{A} &: A \rightarrow A \\
                \text{id}_{A} &:= a \mapsto a
            \end{align*}
        Now suppose that we have $A \sim B$. We note that $B \sim A$ since the bijection going from $B \rightarrow A$ is the inverse. We proved previously that inverse of bijections were bijective. For the transitive property, we also proved that the composition of bijections were bijections. We are done.
    \end{answer}

    \textbf{Exercise 5}: Formulate a notion of \textit{epimorphism}, in the style of the notion of \textit{monomorphism} seen in 2.6, and prove a result analogous to Proposition 2.3, for epimorphisms and surjections.
    \begin{definition}[]{Epimorphism}
        A function $f : A \rightarrow B$ is an epimorphism if for all functions $a^{\prime\prime}: Z \rightarrow B$, there exists a function $a^{\prime}: Z \rightarrow A$ such that
            \begin{equation*}
                f \circ a^{\prime} = a^{\prime\prime}
            \end{equation*}
    \end{definition}

    \textbf{Proposition}: A function $f$ is surjective iff it is an epimorphism.
        \begin{proof}
            (Part I) Suppose that $f$ is a surjection. Then we have that for any $b \in B$, there exists an $a \in A$ such that $f(a) = b$. Notice that each function $a^{\prime\prime}$ is determined by individual mappings of $z_{i} \mapsto b_{i}$. We consider just one. We observe that there exists an $a_{i}$ such that $f(a_{i}) = b_{i}$. Therefore, we construct our $a^{\prime}$ by the mapping of $z_{i} \mapsto a_{i}$.

            (Part II) Suppose that $f$ is an epimorphism. We take an arbitrary $a^{\prime\prime} : Z \rightarrow B$. We know there exists a mapping $a^{\prime} : Z \rightarrow A$. Consider a single mapping $z \mapsto b$ by $a^{\prime\prime}$. We know that $z \mapsto a_{0}$ since
                \begin{equation*}
                    a^{\prime\prime} = f \circ a^{\prime}
                \end{equation*}
            So that means that we must have $f(a_{0}) = z$. Therefore, our $f$ is surjective.
        \end{proof}

    \textbf{Exercise 6}: With notation as in Example 2.4, explain how any function $f: A \rightarrow B$ determines a section of $\pi_{A}$.
        \begin{answer}
            We have that the function can be decomposed into a surjective, then bijective, then injective map:
                \begin{center}
                    \begin{tikzcd}
                        A\ar[r, "\pi_{A}"']\ar[rrr, "f: A \rightarrow B", bend left = 20] & A/\sim \ar[r, "\sim "'] & \Im{f}\ar[r, ""] & B   
                    \end{tikzcd}
                \end{center}
            We note that $a^{\prime} \sim a^{\prime\prime}$ if and only if $f(a^{\prime}) = f(a^{\prime\prime})$. So we have that the equivalence classes are determined by the `injectivity' of $f$.
        \end{answer}

    \textbf{Exercise 7}: Let $f: A \rightarrow B$ be any function. Prove that the graph $\Gamma_{f}$ of $f$ is isomorphic to $A$.
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 8}: Describe as explicitly as you can all terms in the canonical decomposition of the function $\mathbb{R} \rightarrow \mathbb{C}$ defined by $r \mapsto e^{2\pi ir}$. 
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 9}: Show that if $A^{\prime} \cong A^{\prime\prime}$ and $B^{\prime} \cong B^{\prime\prime}$, and further $A^{\prime} \cap B^{\prime} = \emptyset$ and $A^{\prime\prime} \cap B^{\prime\prime} = \emptyset$, then $A^{\prime}\cup B^{\prime} \cong A^{\prime\prime} \cup B^{\prime\prime}$. Conclude that the operation $A \coprod B$ is well-defined up to isomorphism.
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 10}: Show that if $A$ and $B$ are finite sets, then $\lvert B^{A} \rvert = \lvert B \rvert^{\lvert A \rvert}$.
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 11}: In view of Exercise 2.10, it is not unreasonable to use $2^{A}$ to denote the set of functions from an arbitrary set $A$ to a set with 2 elements (say $\{0, 1\}$). Prove that there is a bijection between $2^{A}$ and the \textit{power set} of $A$.
        \begin{answer}
            Leave for later.
        \end{answer}
\end{exercises}

\newpage
\begin{topic}
    \section{Categories}
\end{topic}

A category is a collection of objects and the morphisms between them. Such a definition is vague to expand the number of objects that can be categories. For example, there is no such thing as a set of all sets which we found a counterexample to in Russell's paradox. 

It is possible to define a universe which contains all categories and objects. What will be worked with is a class, which is a category and sometimes a set.

\begin{definition}[\label{def:1.3.1}]{Category}
    A category $C$ consists of 
    \begin{itemize}
        \item a class $\text{Obj}(C)$ of objects of the category, and

        \item for every two objects $A, B$ of $C$, a set $\text{Hom}_{C}(A, B)$ of morphisms, with the properties listed below.
    \end{itemize}

    You can think of the objects as sets and the morphisms as functions. Here are the properties of morphisms:

    \begin{itemize}
        \item For every object $A$ of $C$, there exists a morphism $1_{A} \in \text{Hom}_{C}(A, A)$, the identity on $A$.

        \item One can compose morphisms: two morphisms $f \in \text{Hom}_{C}(A, B)$ and $g \in \text{Hom}_{C}(B, C)$ determine a morphism $gf \in \text{Hom}_{C}(A, C)$. So for every triple of objects $A, B, C$ of $C$ there is a function (of sets)
            \begin{equation*}
                \text{Hom}_{C}(A, B) \times \text{Hom}_{C}(B, C) \rightarrow \text{Hom}_{C}(A, C)
            \end{equation*}
            and the image of the pair $(f, g)$ is denoted $gf$.

        \item This `composition law' is associative: if $f \in \text{Hom}_{C}(A, B), g \in \text{Hom}_{C}(B, C)$, and $h \in \text{Hom}_{C}(C, D)$, then 
            \begin{equation*}
                (hg)f = h(gf)
            \end{equation*}

        \item The identity morphisms are identities with respect to composition: that is for all $f \in \text{Hom}_{C}(A, B)$ we have
            \begin{equation*}
                f1_{A} = f, \, 1_{B}f = f
            \end{equation*}
    \end{itemize}
\end{definition}

It is also required that $\text{Hom}_{C}(A, B)$ and $\text{Hom}_{C}(C, D)$ are disjoint unless $A = C, B = D$. We define an endomorphism to be $\text{Hom}_{C}(A, A) = \text{End}_{C}(A)$, the set of morphisms from an object to itself. This set is closed under composition.

A commutative diagram is that between morphisms such that any traversal direction of compositions will lead to the same result. A diagram is the set of objects of a category and the morphisms between these objects.

\begin{examples}
    \begin{example}[\label{exm:1.3.2}]
        To denote a category of sets, this notation will be used: Set
            \begin{itemize}
                \item $\text{Obj}(\text{Set})= $ the class of all sets;

                \item for $A, B$ in $\text{Obj}(\text{Set})$ (that is, for $A, B$ sets) $\text{Hom}_{Set}(A, B) = B^{A}$.
            \end{itemize}
    \end{example}

    \begin{example}[\label{exm:1.3.3}]
        Another example is where $S$ is a set, $\sim $ is relation on $S$ which is reflexive and transitive. The category is defined as follows:
            \begin{itemize}
                \item \textit{objects}: the elements of $S$;

                \item \textit{morphisms}: if $a, b$ are objects (that is, if $a, b \in S$), then let $\text{Hom}(a, b)$ be the set consisting of the element $(a, b) \in S \times S$ if $a \sim b$, and let $\text{Hom}(a, b) = \emptyset$ otherwise. 
            \end{itemize}
        In this category, there are not many morphism: one for any pair of objects and none for unrelated objects. We would then define a composition of morphisms and see if the conditions in \Fref{def:1.3.1} are satisfied. We require that there is the identity morphism in every $\text{End}_{C}(A)$ set, compositions exist in the category, the composition is associative, and that the identity works with composition:
        \begin{itemize}
            \item For the first condition, if $a \in S$ we require that
                \begin{equation*}
                    1_{a} \in \text{Hom}(a, a).
                \end{equation*}
                But since $a \sim a$ as our relation is reflexive, this is satisfied, therefore, we conclude that
                \begin{equation*}
                    1_{a} = (a, a) \in \text{Hom}(a, a)
                \end{equation*}

            \item Now for composition, we require that if we have 
                \begin{equation*}
                    f \in \text{Hom}(a, b), \, g \in \text{Hom}(b, c);
                \end{equation*}
                then there is $gf \in \text{Hom}(a, c)$. Since $f \in \text{Hom}(a, b)$, the set $\text{Hom}(a, b) \neq \emptyset$, therefore, $f = (a, b)$, and $g = (b, c)$. But looking at equivalence relations, we find that
                \begin{equation*}
                    a \sim b \land b \sim c \implies a \sim c
                \end{equation*}
                since we noted that $\sim $ was transitive. Therefore, $(a, c) \in \text{Hom}(a, c)$ and
                \begin{equation*}
                    gf = (a, c) \in \text{Hom}(a, c)
                \end{equation*}

            \item We now check associativity, that is if $f \in \text{Hom}(a, b), g \in \text{Hom}(b, c)$, and $h \in \text{Hom}(c, d)$, then
                \begin{equation*}
                    f = (a, b), \, g = (b, c), \, h = (c, d)
                \end{equation*}
                so we consider
                \begin{equation*}
                    f(gh) = (fg)h
                \end{equation*}
                which we solve for:
                \begin{equation*}
                    gh = (b, d), \, fg = (a, c)
                \end{equation*}
                and upon observation, our morphisms are indeed associative.

            \item To show that the identity morphisms are identities wrt composition, we suppose that $f \in \text{Hom}(a, b)$ and that the identity morphisms are $1_{a} = (a, a), 1_{b} = (b, b)$:
                \begin{equation*}
                    1_{b}f := a \mapsto b \mapsto b
                \end{equation*}
                which goes the same for $1_{a}$. 
        \end{itemize}

        One example of such a category is the equivalence relation $=$, where all morphisms are exactly the identity morphisms. Such categories are called \textit{discrete}.

        Another example is the relation $\leq $, to which we obtain
        \begin{center}
            \begin{tikzcd}
                2\ar[r, ""]\ar[dr, ""] & 3\ar[d, "1_{3}"]\ar[r, ""] & 5\ar[r, ""]  & 7 \\
                                       & 3\ar[urr, ""]\ar[r, ""]    & 4\ar[ur, ""] &   
            \end{tikzcd}
        \end{center}
        These categories are special in that all diagrams that are drawn from them are commutative. This is not often the case, such as in Set.
    \end{example}

    \begin{example}[\label{exm:1.3.4}]
        Another category is where if $S$ be a set, the category $\hat{S}$:
            \begin{itemize}
                \item $\text{Obj}(\hat{S}) = \mathcal{P}(S)$, the power set $S$

                \item for $A, B$ objects of $\hat{S}$ (that is, $A \subseteq S$ and $B \subseteq S$) let $\text{Hom}_{\hat{S}}(A, B)$ be the pair $(A, B)$ if $A \subseteq B$, and let $\text{Hom}_{\hat{S}}(A, B) = \emptyset$ otherwise. 
            \end{itemize}
            The identity $1_{A}$ consists of the pair $(A, A)$, which is the only morphism from $A$ to $A$. We also have composition as the relation $\subseteq $ is transitive. The same goes for associative, as the order in which we view the subset relation does not change the subset relationship. Now let $1_{B}$ be $B \rightarrow B$ and $f \in \text{Hom}_{\hat{S}}(A, B)$. We must have that $f = (A, B)$. So we look at 
            \begin{equation*}
                1_{B}f = A \rightarrow B \rightarrow B
            \end{equation*}
            which is just 
            \begin{equation*}
                A \rightarrow B = f
            \end{equation*}
            The same argument can be made for $1_{A}$.
    \end{example}

    \begin{example}[\label{exm:1.3.5}]
        This example will be abstract but will recur a lot. Let $C$ be a category and let $A$ be an object of $C$. The category $C_{A}$ will have objects which are certain morphisms of $C$ and the morphisms are certain diagrams of $C$.
            \begin{itemize}
                \item $\text{Obj}(C_{A}) = $ all morphisms form any object of $C$ to $A$. Therefore, any object of $C_{A}$ will be a morphism $f \in \text{Hom}_{C}(Z, A)$ for some $Z \in C$. An object of $C_{A}$ is an arrow $Z \rightarrow A$ in $C$ which are draw top down. 
                    \begin{center}
                        \begin{tikzcd}
                            Z\ar[d, "f"] \\
                            A            
                        \end{tikzcd}
                    \end{center}
            \end{itemize}
            What will the morphisms of $C_{A}$ look like? My idea: If we take two objects in $C_{A}$ and the other in $C_{B}$, then we have $f : Z_{1} \rightarrow A$ and $g : Z_{2} \rightarrow B$. If we want to map one to the other, it makes sense to have some morphism $m$ such that $m(f) = g$, where $m : A \rightarrow Z_{2}$. In actuality, the book takes into consideration simpler cases to deduce from there. In fact, my construction was incorrect because $C_{A}$ consists of objects that specifically send stuff to $A$, and not any other object in $C$. My option was close, but it lacked also the basic symmetry that the book presents:
            \begin{itemize}
                \item Let $f_{1}, f_{2}$ be objects of $C_{A}$, that is, two arrows
                \begin{center}
                    \begin{tikzcd}
                        Z_{1}\ar[d, "f_{1}"] & Z_{2}\ar[d, "f_{2}"] \\
                        A                    & A                    
                    \end{tikzcd}
                \end{center}
                in $C$. Morphisms $f_{1} \rightarrow f_{2}$ are defined to be \textit{commutative diagrams}
                \begin{center}
                    \begin{tikzcd}
                        Z_{1}\ar[rr, "\sigma"]\ar[dr, "f_{1}"] &   & Z_{2}\ar[dl, "f_{2}"] \\
                                                               & A &                       
                    \end{tikzcd}
                \end{center}
                in the `ambient' category $C$.
            \end{itemize}
            That is, morphisms $f \rightarrow g$ correspond precisely to those morphisms $\sigma : Z_{1} \rightarrow Z_{2}$ in $C$ \textit{such} that $f_{1} = f_{2}\sigma$. Instead of thinking of these morphisms as a set, we think of these as a collection of diagrams that satisfy the commutative property. To check for the identity, we have the diagram which we get from $C$:
            \begin{center}
                \begin{tikzcd}
                    Z\ar[rr, "1_{Z}"]\ar[dr, "f"'] &   & Z\ar[dl, "f"] \\
                                                   & A &               
                \end{tikzcd}
            \end{center}
            which commutes because $C$ is a category. If we look at composition, $f_{1} \rightarrow f_{2} \rightarrow f_{3}$, we can put two diagrams side by side to get $f_{1} \rightarrow f_{3}$:
            \begin{center}
                \begin{tikzcd}
                    Z_{1}\ar[r, "\sigma"]\ar[dr, "f_{1}"'] & Z_{2}\ar[d, "f_{2}"]\ar[r, "\tau"] & Z_{3}\ar[dl, "f_{3}"] \\
                                                           & A                                  &                       
                \end{tikzcd}
            \end{center}
            and then since $C$ is a category, the diagram commutes, so we have
            \begin{center}
                \begin{tikzcd}
                    Z_{1}\ar[rr, "\tau\sigma"]\ar[dr, "f_{1}"'] &   & Z_{3}\ar[dl, "f_{2}"] \\
                                                                & A &                       
                \end{tikzcd}
            \end{center}
            So the composition exists. To check associativity, we consider morphisms $\sigma, \tau, \pi$ defined as $Z_{1} \rightarrow Z_{2}$, $Z_{2} \rightarrow Z_{3}$, $Z_{3} \rightarrow Z_{4}$ respectively and we draw a commutative diagram each:
            \begin{center}
                \begin{tikzcd}
                    Z_{1}\ar[rr, "\sigma"]\ar[dr, "f_{1}"']       &   & Z_{2}\ar[dl, "f_{2}"] \\
                                                                  & A &                       
                \end{tikzcd}
                \begin{tikzcd}
                    Z_{2}\ar[rr, "\tau"]\ar[dr, "f_{2}"'] &   & Z_{3}\ar[dl, "f_{3}"] \\
                                                          & A &                       
                \end{tikzcd}
                \begin{tikzcd}
                    Z_{3}\ar[rr, "\pi"]\ar[dr, "f_{3}"'] &   & Z_{4}\ar[dl, "f_{4}"] \\
                                                         & A &                       
                \end{tikzcd}
            \end{center}
            Piecing this all together, we get
            \begin{center}
                \begin{tikzcd}
                    Z_{1}\ar[r, "\sigma"]\ar[dr, "f_{1}"'] & Z_{2}\ar[r, "\tau"]\ar[d, "f_{2}"'] & Z_{3}\ar[r, "\pi"]\ar[dl, "f_{3}"'] & Z_{4}\ar[dll, "f_{4}"] \\
                                                           & A                                   &                                    &                        
                \end{tikzcd}
            \end{center}
            As this is a commutative diagram, we note that the grouping of the morphisms do not matter. Such a category is called a \textit{slice category}.
    \end{example}

    \begin{example}[\label{exm:1.3.6}]
        Let us apply this construction of a category to that in Example 3.3, where $S = \mathbb{Z}$ and $\sim $ is the relation $\leq $. Choose $A = 3$, an integer and the objects of $C_{A}$ are the morphisms in $C$ which send stuff to 3. So we have the pairs $(n, 3)$ for $n \leq 3$. We have the morphism
                \begin{equation*}
                    (m, 3) \rightarrow (n, 3)
                \end{equation*}
            if and only if $m \leq n$. In this case, $C_{A}$ may be considered as the subcategory of integers $\leq 3$, with the `the same' morphisms as in $C$.
    \end{example}

    \begin{example}[\label{exm:1.3.7}]
        Another example are the morphisms obtained from morphisms in a category $C$ which go from a fixed object $A$ to any element in $C$. These are called \textit{coslice categories}.
    \end{example}

    \begin{example}[\label{exm:1.3.8}]
        A concrete example of a category for $3.7$ is where $C = \text{Set}$ and $A = $ a fixed singleton $\{*\}$. Call the resulting category $\text{Set}^{*}$.
            \begin{itemize}
                \item an object of $\text{Set}^{*}$ is therefore a morphism $f : \{*\} \rightarrow S \in \text{Set}$. The object in Set must be nonempty and is the element $f(*)$.  So the objects can be denoted as $(S, s)$ where $S$ is the set and $s$ is any object in $S$. Now a morphism between two objects $(S, s) \rightarrow (T, t)$ is a set-function $\sigma : S \rightarrow T$ such that $\sigma(s) = t$. Verify this:

                The goal is to map a given morphism defined as $(S, s)$ to $(T, t)$. We also note that our singleton is fixed. So we have two given objects denoted as 
                \begin{center}
                    \begin{tikzcd}
                        \{*\}\ar[d, "f_{1}"] &  & \{*\}\ar[d, "f_{2}"] \\
                        S                    &  & T                      
                    \end{tikzcd}
                \end{center}
                So we require a morphism that maps $f_{1} \mapsto f_{2}$ and considering the diagram, we attempt to construct a composition:
                \begin{center}
                    \begin{tikzcd}
                                           & \{*\}\ar[dr, "f_{2}"']\ar[dl, "f_{1}"] &   \\
                        S\ar[rr, "\sigma"] &                                        & T   
                    \end{tikzcd}
                \end{center}
                So following the commutative diagram, we have that the mapping is such that $\sigma$, as desired.
            \end{itemize}
            The objects of $\text{Set}^{*}$ are called `pointed sets'. One example is that a group $G$ has an identity $e_{G}$, and that the group homomorphisms will be functions which send identities to identities meaning that they are morphisms of pointed sets.
    \end{example}

    \begin{example}[\label{exm:1.3.9}]
        We will define a new category given a category $C$, objects $A, B$ by
            \begin{itemize}
                \item $\text{Obj}(C_{A, B}) = $ diagrams
                    \begin{center}
                        \begin{tikzcd}
                                                      & A \\
                            Z\ar[ur, "f"]\ar[dr, "g"] &   \\
                                                      & B 
                        \end{tikzcd}
                    \end{center}
                    in $C$; and

                \item morphisms
                    \begin{center}
                        \begin{tikzcd}
                                                                  & A  &              &    &                                       & A \\
                            Z_{1}\ar[ur, "f_{1}"]\ar[dr, "g_{1}"] & {} & {}\ar[r, ""] & {} & Z_{2}\ar[ur, "f_{2}"]\ar[dr, "g_{2}"] &   \\
                                                                  & B  &              &    &                                       & B 
                        \end{tikzcd}
                    \end{center}
                are commutative diagrams
                    \begin{center}
                        \begin{tikzcd}
                                                                                                                      &                                       & A \\
                            Z_{1}\ar[urr, "f_{1}", bend left = 20]\ar[r, "\sigma"]\ar[drr, "g_{1}"', bend right = 20] & Z_{2}\ar[ur, "f_{2}"]\ar[dr, "g_{2}"] &   \\
                                                                                                                      &                                       & B   
                        \end{tikzcd}
                    \end{center}
            \end{itemize}
            Observe that this is an application of $C_{A}$ and $C_{B}$ categories. Instead of considering singleton functions as objects, considering multiple gives us a diagram, and the morphisms are the commutative diagrams such that $\sigma$ satisfies the commutative condition. We first consider if an identity exists from a diagram to itself. This would be an identity on functions, and the morphism on sets that we desire is the identity from our category, namely, the identity diagram is
            \begin{center}
                \begin{tikzcd}
                                                                               & A &                                        \\
                    Z_{1}\ar[ur, "f_{1}"]\ar[rr, "I_{Z_{1}}"]\ar[dr, "g_{1}"'] &   & Z_{1}\ar[ul, "f_{1}"']\ar[dl, "g_{1}"] \\
                                                                               & B &                                          
                \end{tikzcd}
            \end{center}

            What would the composition of diagrams look like? Suppose we have one diagram such that $\sigma : f_{2} \mapsto f_{1}$ and another such that $\tau : f_{3} \mapsto f_{2}$, the same for $g_{1}$ and $g_{2}$:
            \begin{center}
                \begin{tikzcd}
                                                                                                              &                                         & A            &                                                                                         &                                        & A \\
                    Z_{1}\ar[urr, "f_{1}", bend left = 20]\ar[drr, "g_{1}"', bend right = 20]\ar[r, "\sigma"] & Z_{2}\ar[ur, "f_{2}"]\ar[dr, "g_{2}"']  & {}\ar[r, ""] & Z_{2}\ar[urr, "f_{2}", bend left = 20]\ar[r, "\tau"]\ar[drr, "g_{2}"', bend right = 20] & Z_{3}\ar[ur, "f_{3}"]\ar[dr, "g_{3}"'] &   \\
                                                                                                              &                                         & B            &                                                                                         &                                        & B   
                \end{tikzcd}
            \end{center}
            So the composition of morphisms is now clear:
            \begin{center}
                \begin{tikzcd}
                                                                                                              &                                                      & A                                    \\
                    Z_{1}\ar[urr, "f_{1}", bend left = 20]\ar[drr, "g_{1}"', bend right = 20]\ar[r, "\sigma"] & Z_{2}\ar[ur, "f_{2}"]\ar[dr, "g_{2}"']\ar[r, "\tau"] & Z_{3}\ar[u, "f_{3}"']\ar[d, "g_{3}"] \\
                                                                                                              &                                                      & B                                      
                \end{tikzcd}
            \end{center}
            What does it mean to be associative? If we generalize this to a commutative diagram of three morphisms:
            \begin{center}
                \begin{tikzcd}
                                                                                                                &                                                                                         &                                                     & A                                    \\
                    Z_{1}\ar[urrr, "f_{1}", bend left = 30]\ar[r, "\sigma"]\ar[drrr, "g_{1}"', bend right = 30] & Z_{2}\ar[urr, "f_{2}", bend left = 15]\ar[r, "\tau"]\ar[drr, "g_{2}"', bend right = 15] & Z_{3}\ar[ur, "f_{3}"]\ar[dr, "g_{3}"']\ar[r, "\pi"] & Z_{4}\ar[u, "f_{4}"']\ar[d, "g_{4}"] \\
                                                                                                                &                                                                                         &                                                     & B                                      
                \end{tikzcd}
            \end{center}
            We can try to understand by traversing the diagram. Notice that our end goal is to get the mapping $f_{1} : Z_{1} \rightarrow A$. We follow the first two commutative diagrams, first by $\sigma$ then by $\tau$, and finally, with our input, $f_{3}$. So we have such that $f_{3} \mapsto f_{1}$. Now if we do the innermost function, we have $\pi(f_{4}) = f_{3}$. So we get $(\sigma\tau)(\pi(f_{4})) = f_{1}$. Notice that if we do the same the other direction, it works also.

        It is clear that if we compose the identity diagram above, that we would get the same commutative diagram. We have verified that this is a category.

        Cool note: if we flip the arrows on the diagrams, we would get the alternate \textit{coslice diagram}. This is observed by how instead of the diagram requiring simultaneously that
            \begin{align*}
                f_{1} &= f_{2}\sigma & g_{1} &= g_{2}\sigma   
            \end{align*}
            we instead reverse the composition to obtain
            \begin{align*}
                f_{1} &= \sigma f_{2} & g_{1} &= \sigma g_{2}   
            \end{align*} 
        which is exactly as seen in the \textit{coslice} example.
    \end{example}

    \begin{example}[\label{exm:1.3.10}]
        We will consider the \textit{fibered} version of $C_{A, B}$ and $C^{A, B}$. The ultimate test of understanding! Start with a given category $C$, and choose two fixed \textit{morphisms} $\alpha : A \rightarrow C$, $\beta : B \rightarrow C$ with the same target $C$. We then consider a category $C_{\alpha, \beta}$ as follows:
            \begin{itemize}
                \item $\text{Obj}(C_{\alpha,\beta}) = $ \textit{commutative diagrams}
                    \begin{center}
                        \begin{tikzcd}
                                                       & A\ar[dr, "\alpha"] &   \\
                            Z\ar[ur, "f"]\ar[dr, "g"'] &                    & C \\
                                                       & B\ar[ur, "\beta"'] &     
                        \end{tikzcd}
                    \end{center}

                \item morphisms correspond to commutative diagrams
                    \begin{center}
                        \begin{tikzcd}
                                                                                                                       &                                       & A\ar[dr, "\alpha"] &   \\
                            Z_{1}\ar[urr, "f_{1}", bend left = 20]\ar[r, "\sigma"]\ar[drr, "g_{1}"', bend right = 20]  & Z_{2}\ar[ur, "f_{2}"]\ar[dr, "g_{2}"'] &                    & C \\
                                                                                                                       &                                       & B\ar[ur, "\beta"'] &     
                        \end{tikzcd}
                    \end{center}
            \end{itemize}
        The identities and compositions of such a morphism should be considered. We observe that if there are two objects that are the same, the morphism between them that is the identity would just be a mapping of the identity on $Z$. In the composition of morphisms, we just extend leftward with another set $Z_{0}$ and write the outermost composition function $\tau$.

        Now left to the reader is the construction of the mirror of this from two morphisms $\alpha : C \rightarrow A, \beta : C \rightarrow B$ with common source.

        We define the fibered version of $C^{\alpha, \beta}$ as follows:
            \begin{itemize}
                \item $\text{Obj}(C_{\alpha, \beta}) = $\textit{commutative diagrams}
                    \begin{center}
                        \begin{tikzcd}
                                                                & A\ar[dr, "f"] &   \\
                            C\ar[ur, "\alpha"]\ar[dr, "\beta"'] &               & Z \\
                                                                & B\ar[ur, "g"'] &     
                        \end{tikzcd}
                    \end{center}

                \item morphisms correspond to commutative diagrams
                    \begin{center}
                        \begin{tikzcd}
                                                               & A\ar[dr, "f_{2}"]\ar[drr, "f_{1}", bend left = 20]    &                       &       \\
                            C\ar[ur, "\alpha"]\ar[dr, "\beta"] &                                                       & Z_{2}\ar[r, "\sigma"] & Z_{1} \\
                                                               & B\ar[ur, "g_{2}"']\ar[urr, "g_{1}"', bend right = 20] &                       &         
                        \end{tikzcd}
                    \end{center}
            \end{itemize}
        This is essentially a reversing of the arrows on the previous fibered diagram.
    \end{example}
\end{examples}

\begin{exercises}{\textbf{Problem Sets}}
    \textbf{Exercise 1}: Let $C$ be a category. Consider a structure $C^{op}$ with 
        \begin{itemize}
            \item $\text{Obj}(C^{op}) := \text{Obj}(C)$;

            \item for $A, B$ objects of $C^{op}$ (hence objects of $C$), $\text{Hom}_{C^{op}}(A, B) := \text{Hom}_{C}(B, A)$. 
        \end{itemize}
        Show how to make this into a category (that is, define composition of morphisms in $C^{op}$ and verify the properties listed in \Fref{def:1.3.1}).
            \begin{answer}
                If we have two morphisms $f \in \text{Hom}_{C^{op}}(A, B), f \in \text{Hom}_{C^{op}}(B, C)$, then we note that $f \in \text{Hom}_{C}(B, A)$ and $g \in \text{Hom}_{C}(C, B)$. Therefore, we define the composition of morphisms to be
                \begin{equation*}
                    f \circ g : C \rightarrow A \in \text{Hom}_{C^{op}}(A, C)
                \end{equation*}
                What happens with the identity morphisms? We know that $C$ is a category, so since $\text{Hom}_{C^{op}}(A, A) := \text{Hom}_{C}(A, A)$, the identity exists as the same identity from $C$. Note that the morphisms are associative as associativity is inherited from the associativity of $C$. Now to check that the identity is the identity with respect to composition, it is trivially inherited from $C$ also.
            \end{answer}
        The category $C^{op}$ is essentially the category obtained by reversing the arrows on $C$. This tells us that $C$ is a category iff $C^{op}$ is.

    \textbf{Exercise 2}: If $A$ is a finite set, how large is $\text{End}_{\text{Set}}(A)$?
        \begin{answer}
            If $A$ is finite, we simply have all the morphisms that go between $A$ and itself. So from one of the previous exercises, this is $\lvert A \rvert!$. 
        \end{answer}

    \textbf{Exercise 3}: Formulate precisely what it means to say that $1_{a}$ is an identity with respect to composition in Example 3.3, and prove this assertion.
        \begin{proof}
            For $1_{a}$ to be the identity, we require that for any morphism $f \in \text{Hom}(a, b)$, we have that
            \begin{equation*}
                f \circ 1_{a} = f
            \end{equation*}
            This is true as since $1_{a}$ is the identity, we have that $a \mapsto a$ and for $f$, this is also a singleton map $a \mapsto b$ as $a \sim b$ and $a \sim a$. We see that
            \begin{equation*}
                f \circ 1_{a} = a \mapsto a \mapsto b = a \mapsto b = f
            \end{equation*}
            So it has been proven.
        \end{proof}

    \textbf{Exercise 4}: Can we define a category in the style of Example 3.3 using the relation $< $ on the set $\mathbb{Z}$?
        \begin{answer}
            Consider the category $C$ defined as follows:
                \begin{itemize}
                    \item $\text{Obj}(C) = $ elements in $\mathbb{Z}$,

                    \item morphisms of $C$ are elements of $\text{Hom}(a, b)$, $a, b \in \mathbb{Z}$. This is the set defined as $(a, b) \in \text{Hom}(a, b) \iff a < b$, and $\text{Hom}(a, b) = \emptyset$ otherwise.
                \end{itemize}
            But this does not work as it is not reflexive, that is, that $a \not< a$.
        \end{answer}

    \textbf{Exercise 5}: Explain in what sense Example 3.4 is an instance of the categories considered in Example 3.3.
        \begin{answer}
            If we consider $\subseteq $ as the relation, we note that it is indeed reflexive and that it is transitive. There is also the fact that each $\text{Hom}(A, B)$ group is of size either $1$ or $0$, with the same conditions as $\text{Hom}(a, b)$.
        \end{answer}

    \textbf{Exercise 6}: Define a category $V$ by taking $\text{Obj}(V) = \mathbb{N}$. (We will leave the reader the task of making sense of a matrix with 0 rows or columns.) Use product of matrices to define composition. Does this category `feel' familiar?
        \begin{answer}
            First, to consider matrices with 0 rows or columns, we define matrices in bijection with the set of linear operators from an $n$ dimensional space to that of a $m$ dimensional space. So to have $0$ rows is to go to a zero dimensional space, which is the $0$ mapping. To have 0 columns is to map from a $0$ dimensional space which means that our function is defined solely by 0: $M(0) = 0$. This is also the 0 map.

            The identity map is where we just take the matrix:
                \begin{align*}
                    M = 
                        \begin{bmatrix}
                            1      & 0      & \ldots & 0      \\
                            0      & 1      & \ldots & 0      \\
                            \vdots & \vdots & \ddots & \vdots \\
                            0      & 0      & \ldots & 1        
                        \end{bmatrix}
                \end{align*}
            The identity map is indeed a identity with respect to composition. We already know that the composition exists in the category, and the composition is associative.
        \end{answer}

    \textbf{Exercise 7}: Define carefully objects and morphisms in , and draw the diagram corresponding to composition.
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 8}: A \textit{subcategory} $C^{\prime}$ of a category $C$ consists of a collection of objects of $C$, with morphisms $\text{Hom}_{C^{\prime}}(A, B) \subseteq \text{Hom}_{C}(A, B)$ for all objects $A, B$ in $\text{Obj}(C^{\prime})$, such that identities and compositions in $C$ make $C^{\prime}$ into a category. A subcategory $C^{\prime}$ is \textit{full} if $\text{Hom}_{C^{\prime}}(A, B) = \text{Hom}_{C}(A, B)$ for all $A, B$ in $\text{Obj}(C^{\prime})$. Construct a category of \textit{infinite sets} and explain how it may be viewed as a full subcategory of $\text{Set}$.
        \begin{answer}
            Define our category of infinite sets as follows:
                \begin{itemize}
                    \item $\text{Obj}(C^{\prime}) = $ Sets $S$ such that $\lvert S \rvert = \infty$,

                    \item morphisms in the set, for any two sets $A, B$ in $C^{\prime}$, $\text{Hom}(A, B)$ such that $f : A \rightarrow B \iff f \in \text{Hom}(A, B)$.
                \end{itemize}
            Notice that this is the same definition as defined in $\text{Set}$. So this means that if a morphism is in $\text{Hom}_{C}(A, B)$, such that $A, B$ are infinite sets, then it is also in $\text{Hom}_{C^{\prime}}(A, B)$. We just need to check that this is a proper category. The identity is indeed inherited from $\text{Hom}_{C}(A, A)$ for any infinite set $A$ as we just map every element to itself. Suppose that we have two morphisms from arbitrary infinite sets $X, Y, Z$: $f \in \text{Hom}_{C^{\prime}}(X, Y)$ and $\text{Hom}_{C^{\prime}}(Y, Z)$. Then it must be that $f : X \rightarrow Y$ and that $g : Y \rightarrow Z$. Our composition is defined as $g \circ f : X \rightarrow Y \rightarrow Z$, which exists in $\text{Hom}_{C^{\prime}}(X, Z)$ by definition. Associativity is inherited from $C$, and the identity property also. We are done.
        \end{answer}

    \textbf{Exercise 9}: An alternative to the notion of \textit{multiset} is obtained by considering sets endowed with equivalence relations; equivalent elements are taken to be multiple instances of elements `of the same kind'. Define a notion of morphism between such enhanced sets, obtaining a category $\text{MSet}$ containing (a `copy' of) $\text{Set}$ as a full subcategory. Which objects in $\text{MSet}$determine ordinary multisets as defined in 2.2 and how? 
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 10}: Since the objects of a category $C$ are not (necessarily) sets, it is not clear how to make sense of a notion of `subobject' in general. In some situations it does make sense to talk about subobjects, and the subobjects of any given objects $A$ in $C$ are in one-to-one correspondence with the morphisms of $A \rightarrow \Omega$ for a fixed, special object $\Omega$ of $C$, called a \textit{subobject classifier}. Show that $\text{Set}$ has a subobject classifier.
        \begin{answer}
            Leave for later.
        \end{answer}

    \textbf{Exercise 11}: Draw the relevant diagrams and define composition and identities for the category $C^{A, B}$ mentioned in Example 3.9. Do the same for the category $C^{\alpha\beta}$ mentioned in Example 3.10.
\end{exercises}

\newpage
\begin{topic}
    \section{Morphisms}
\end{topic}

Just as in $\text{Set}$, we highlight certain types of function (injective, surjective, bijective), and we should do the same for morphisms. Note that defining morphisms by actions on `elements' is not an option when we are looking at the general case, as objects of categories might not always have `elements'.

\begin{definition}[\label{def:1.4.1}]{Isomorphisms}
    A morphisms $f \in \text{Hom}_{C}(A, B)$ is an \textit{isomorphism} if it has a two sided inverse under composition, that is $\exists g \in \text{Hom}_{C}(B, A)$ such that
    \begin{equation*}
        gf = 1_{A}, \, fg = 1_{B}
    \end{equation*}
\end{definition}

Recall that when the inverse of of a function such as a left-sided inverse is not necessarily unique. Yet, we can guarantee uniqueness through the definition of an isomorphism. 

\textbf{Proposition 4.2}: The inverse of an isomorphism is unique.
    \begin{proof}
        We must show that if we have two inverses $g_{1}$ and $g_{2} : B \rightarrow A$, then $g_{1} = g_{2}$. This is done by 
        \begin{equation*}
            g_{1} = g_{1}1_{B} = g_{1}(fg_{2}) = (g_{1}f)g_{2} = 1_{A}g_{2} = g_{2}
        \end{equation*}
        as desired.
    \end{proof}
This is needed so that there is no ambiguity of the left-sided inverse vs right-sided inverse when talking about $f^{-1}$.

\textbf{Proposition 4.3}: Based on the notation above,
    \begin{itemize}
        \item Each identity $1_{A}$ is an isomorphism and is its own inverse.

        \item If $f$ is an isomorphism, then $f^{-1}$ is an isomorphism and further $(f^{-1})^{-1} = f$.

        \item If $f \in \text{Hom}_{C}(A, B)$, $g \in \text{Hom}_{C}(B, C)$ are isomorphisms, then the composition $gf$ is an isomorphism and $(gf)^{-1} = f^{-1}g^{-1}$.
    \end{itemize}
These are verification exercises.

We can create a definition that two objects $A, B$ in a category are isomorphic if they contain a bijection $f : A \rightarrow B$. This defines an equivalence relation.

\begin{examples}
    \begin{example}[\label{exm:1.4.4}]
        Notice that the isomorphisms in the category $\text{Set}$ are the bijections.
    \end{example}

    \begin{example}[\label{exm:1.4.5}]
        Identities are isomorphisms and they can be the only isomorphisms in a category. In the category $C$ from the relation $\leq $ on $\mathbb{Z}$ in \Fref{exm:1.3.3}, we realize the only isomorphic objects are that when comparing the same object $a = a$. We require the morphism $f: a \rightarrow b$ and the morphism $g : b \rightarrow a$, which implies that $a = b$. So for such an isomorphism to exist, it must be the identity morphism: $1_{a}$.
    \end{example}

    \begin{example}[\label{exm:1.4.6}]
        There are also categories where every morphism is an isomorphism, in which we call the categories \textit{groupoids}. 

        An \textit{automorphism} of an object $A$ of a category $C$ is an isomorphism from $A$ to itself. The set of automorphisms of $A$ is denoted $\text{Aut}_{C}(A)$, which is a subset of $\text{End}_{C}(A)$. From proposition 4.3, there is structure to be talked about in $\text{Aut}_{C}(A)$:
            \begin{itemize}
                \item the composition of two elements $f, g \in \text{Aut}_{C}(A)$ is an element $gf \in \text{Aut}_{C}(A)$, 

                \item composition is associative,

                \item $\text{Aut}_{c}(A)$ contains the element $1_{A}$, which is an identity for composition 

                \item every element $f \in \text{Aut}_{C}(A)$ has an inverse $f^{-1} \in \text{Aut}_{C}(A)$.
            \end{itemize}
        This tells us that $\text{Aut}_{C}(A)$ is a \textit{group}, for all objects $A$ in any category $C$.
    \end{example}
\end{examples}

Since the objects in a category are varying, there is no way of talking about injectivity and surjectivity as we have previously as set-functions. The previous definitions required the concept of an element. So instead we define \textit{monomorphisms} and \textit{epimorphisms}:

\begin{definition}[\label{def:1.4.7}]{Monomorphism}
    Let $C$ be a category. A morphism $f \in \text{Hom}_{C}(A, B)$ is a \textit{monomorphism} if the following holds:
    \begin{center}
        \textit{for all objects $Z$ of $C$ and all morphisms $a^{\prime}, a^{\prime\prime} \in \text{Hom}_{C}(Z, A)$,} \\
        $f \circ a^{\prime} = f \circ a^{\prime\prime} \implies a^{\prime} = a^{\prime\prime}$
    \end{center}
\end{definition}
and now for epimorphisms:
\begin{definition}[\label{def:1.4.8}]{Epimorphism}
    Let $C$ be a category. A morphism $f \in \text{Hom}_{c}(A, B)$ is an \textit{epimorphism} if the following holds:
        \begin{center}
            \textit{for all objects $Z$ of $C$ and all morphisms $\beta^{\prime}, \beta^{\prime\prime} \in \text{Hom}_{C}(B, Z),$}\\
            $\beta^{\prime} \circ f = \beta^{\prime\prime} \circ f \implies \beta^{\prime} = \beta^{\prime\prime}$
        \end{center}
\end{definition}
\textit{Note}: My definition of epimorphism was incorrect in one of the homework problems. I can see why this definition of epimorphism is used. The idea of monomorphism was looking at `injectivity' of the morphisms while looking at morphisms as elements. So for epimorphisms, a plausible idea is to flip this and look at injectivity the `other way'. It refers to flipping the arrows on a commutative diagram to produce a \textit{coslice category}.

\begin{examples}
    \begin{example}[\label{exm:1.4.9}]
        What was proven in Proposition $2.3$ was that the monomorphisms are the injective functions. Now check that the epimorphisms are the surjective functions.
            \begin{proof}
                ($\rightarrow $) Suppose that $f$ is an epimorphism. Suppose that $f$ is not surjective for contradiction. Then there is some $b \in Z$ such that $f(a) \neq b$ for all $a \in B$. Now let the functions $\beta^{\prime}, \beta^{\prime\prime}$ be such that they map all elements in $\Im{f}$ to the same thing but $b$ to different things in $Z$. This gives us the contradiction that $f$ is an epimorphism.

                ($\leftarrow $) Since $f$ is surjective, we know that there is a left-inverse. Therefore, taking the left inverse of both sides shows that $f$ is an epimorphism.
            \end{proof}
        These definitions for morphisms work as the counterparts to injectivity and surjectivity in categories.
    \end{example}

    \begin{example}[\label{exm:1.4.10}]
        In the categories in \Fref{exm:1.3.3}, every morphism was a monomorphism and epimorphism. But there was at most one morphism between any two objects in the category, therefore, the condition for defining monomorphisms and epimorphisms was vacuous.
    \end{example}
\end{examples}

Consider \Fref{exm:1.4.10}. For example, in $\text{Set}$, the category which contains sets, an isomorphism is a morphism that is both surjective and injective, or is both a monomorphism and an epimorphism. In the category defined by $\leq $ on $\mathbb{Z}$, every morphism is both a monomorphism and an epimorphism. The only isomorphisms are identities (\Fref{exm:1.4.5}). This is a feature of $\text{Set}$, which might not be true in every category. It will not hold in the category of rings, but it will in every abelian category. In $\text{Set}$, a function is an epimorphism iff it has a right-inverse. This might not be true in general such as the category $\text{Grp}$ of groups.

\begin{exercises}{Problem Sets}
    \textbf{Exercise 4.1}: Composition is defined for \textit{two} morphisms. If more than two morphisms are give, for example:
        \begin{center}
            \begin{tikzcd}
                A\ar[r, "f"] & B\ar[r, "g"] & C\ar[r, "h"] & D\ar[r, "i"] & E,
            \end{tikzcd}
        \end{center}
    then one may compose them in several ways, for example:
        \begin{equation*}
            (ih)(gh), \, (i(hg))f, \, i((hg)f), \, \text{etc}
        \end{equation*}
    so that every step one is only composing two morphisms. Prove that the result of any such nested composition is independent of the placement of the parentheses. (Hint: Use induction on $n$ to show that any such choice for $f_{n}f_{n - 1}\cdots f_{1}$ equals
        \begin{equation*}
            ((\cdots((f_{n}f_{n - 1})f_{n - 2})\cdots)f_{1})
        \end{equation*}
    Carefully working out the case $n = 5$ is helpful)
        \begin{proof}
            \textbf{Base Case}: We will consider the base case with the composition of three morphisms. The statement holds, as we just need to check that $((fg)h) = (f(gh))$ which is true as composition is associative.

            \textbf{Inductive Case}: Suppose that we have $f_{n}f_{n - 1}\cdots f_{1}$ such that $n > 3$. Consider the form:
                \begin{equation*}
                    ((\cdots((f_{n}f_{n - 1})f_{n - 1})\cdots)f_{1}).
                \end{equation*}
            Notice that we can treat $f_{n}f_{n - 1}$ as just one morphism $g_{n - 1}$. But by the inductive hypothesis, we claim that we can rearrange the parentheses of this product to our liking. So we wrap $g_{n - 1}$ with the morphism immediately to the right of it:
                \begin{equation*}
                    ((\cdots((g_{n}f_{n - 2})f_{n - 2})\cdots)f_{1})
                \end{equation*}
            And by the associative law, we now compose
                \begin{equation*}
                    ((\cdots((f_{n}(f_{n - 1}f_{n - 2})))\cdots)f_{1})
                \end{equation*}
            Notice that we can repeat this process however many times to get the innermost composition between any arbitrary $f_{i}f_{i - 1}$. So since $(f_{i}f_{i - 1})$ is just 1 morphism after composition, we have reduced it to the previous case. We are done.
        \end{proof}

    \textbf{Exercise 2}: In \Fref{exm:1.3.3} we have seen how to construct a category from a set endowed with a relation, provided this latter is reflexive and transitive. For what types of relations is the corresponding category a groupoid (every morphism is an isomorphism)?
        \begin{answer}
            In the category with the set $\mathbb{Z}$ from which we draw the elements of $C$ and the morphisms defined by $\leq $, we notice that some morphisms were not necessarily isomorphisms, specifically the ones that are not identities. This is because our relation is not symmetric. That is, in general, we do not have
                \begin{equation*}
                    a \leq b \land b \leq a
                \end{equation*}
            So what happens if we include symmetry as a requirement? The we must have that $a \sim b$ and $b \sim a$. This tells us that any morphism $f$ is invertible. So $f$ is an isomorphism. We can probably also conclude the converse. Every set $\text{Hom}_{C}(a, b)$, has either 1 or 0 elements. We will prove the contrapositive. Suppose that there exists a morphism that does not have an inverse. That must mean that if $f \in \text{Hom}_{C}(a, b)$, the set $\text{Hom}_{C}(b, a)$ must be empty. This means that $a \sim b$ but $b \not\sim a$. So there exists two elements that do not have the symmetric property. So we have proven the contrapositive.
        \end{answer}

    \textbf{Exercise 4.3}: Let $A, B$ be objects of a category $C$, and let $f \in \text{Hom}_{C}(A, B)$ be a morphism. 
        \begin{itemize}
            \item Prove that if $f$ has a right-inverse, then $f$ is an epimorphism.
                \begin{proof}
                    Suppose that $f \in \text{Hom}_{C}(A, B)$ and consider arbitrary morphisms $a^{\prime}, a^{\prime\prime} \in \text{Hom}_{C}(Z, A)$. Now since $f$ has a right-inverse, we have
                        \begin{gather*}
                            a^{\prime} \circ f = a^{\prime\prime} \circ f \\
                            a^{\prime} \circ f \circ f^{-1} = a^{\prime\prime} \circ f \circ f^{-1} \\
                            a^{\prime} \circ 1_{B} = a^{\prime\prime} \circ 1_{B} \\
                            a^{\prime} = a^{\prime\prime}
                        \end{gather*}
                    as desired.
                \end{proof}

            \item Show that the converse does not hold, by giving an explicit example of a category and an epimorphism without a right-inverse. 
                \begin{answer}
                    We try finding one through a non symmetry equivalence relation that represents the morphisms of a category. Specifically, we have gone over the category with the morphisms corresponding to equivalence relations $\sim $ or $\leq $ on $\mathbb{Z}$. We take two arbitrary functions $a^{\prime}, a^{\prime\prime} \in \text{Hom}_{C}(b, z)$. Therefore, we have $f \in \text{Hom}_{C}(a, b)$ for $a \neq b$. Now observe that indeed, 
                        \begin{equation*}
                            a^{\prime} \circ f = a^{\prime\prime} \circ f \implies a^{\prime} = a^{\prime\prime}
                        \end{equation*}
                    which is vacuously true. But there is no right inverse for $f$ as $b \not\leq a$. This concludes the proof.
                \end{answer}
        \end{itemize}

    \textbf{Exercise 4.4}: Prove that the composition of two monomorphisms is a monomorphism. Deduce that one can define a subcategory $C_{\text{mono}}$ of a category $C$ by taking the same objects as in $C$ and defining $\text{Hom}_{C_{\text{mono}}}(A, B)$ to be the subset of $\text{Hom}_{C}(A, B)$ consisting of monomorphisms, for all objects $A, B$. Do the same for epimorphisms. Can you define a subcategory $C_{\text{nonmono}}$ of $C$ by restricting to morphisms that are \textit{not} monomorphisms?
        \begin{answer}
            (Part I) Suppose that $f \in \text{Hom}_{C}(B, C)$, $g \in \text{Hom}_{C}(C, D)$ and that for arbitrary morphisms $a^{\prime}, a^{\prime\prime} \in \text{Hom}_{C}(A, B)$, $b^{\prime}, b^{\prime\prime} \in \text{Hom}_{C}(A, C)$ we have
                \begin{align*}
                    f \circ a^{\prime} = f \circ a^{\prime\prime} &\implies a^{\prime} = a^{\prime\prime}                               \\
                    g \circ b^{\prime} = g \circ b^{\prime\prime} &\implies                               b^{\prime} = b^{\prime\prime}   
                \end{align*}
            So we now compose $g \circ f$ and verify:
                \begin{align*}
                    (g \circ f) \circ a^{\prime} = (g \circ f) \circ a^{\prime\prime} &\implies  f \circ a^{\prime} = f \circ a^{\prime\prime} \\
                    f \circ a^{\prime} = f \circ a^{\prime\prime}                     &\implies  a^{\prime} = a^{\prime\prime}                   
                \end{align*}
            So $g \circ f$ is a monomorphism. Interestingly, we can take all elements in $C$ and define the subcategory consisting of the morphisms as only the monomorphisms of $C$, because composition of monomorphisms is possible and that the composition is also a monomorphism and is therefore a morphism in $C_{\text{mono}}$. Identity is indeed a monomorphism. Identity with respect to composition is inherited from $C$. 

            (Part II) Suppose that $f \in \text{Hom}_{C}(A, B)$, $g \in \text{Hom}_{C}(B, C)$ and that for arbitrary morphisms $a^{\prime}, a^{\prime\prime} \in \text{Hom}_{C}(C, D)$, $b \in \text{Hom}_{C}(B, D)$ we have
                \begin{align*}
                    b^{\prime} \circ f = b^{\prime\prime} \circ f &\implies  b^{\prime} = b^{\prime\prime} \\
                    a^{\prime} \circ g = a^{\prime\prime} \circ g &\implies  a^{\prime} = a^{\prime\prime}   
                \end{align*}
            So we now compose $g \circ f$ and verify:
                \begin{align*}
                    a^{\prime} \circ (g \circ f) = a^{\prime\prime} \circ (g \circ f) &\implies  a^{\prime} \circ g = a^{\prime\prime} \circ g \\
                    a^{\prime} \circ g = a^{\prime\prime} \circ g                     &\implies  a^{\prime} = a^{\prime\prime}                   
                \end{align*}
            This shows that $g \circ f$ is an epimorphism also. By the same reasoning in the previous part, we have justified the composition part. The identity is definitely an epimorphism. So this is indeed a subcategory $C_{\text{epi}}$.

            We cannot define such a category $C_{\text{nonmono}}$ since the identity would not exist in it.
        \end{answer}

    \textbf{Exercise 4.5}: Give a concrete description of monomorphisms and epimorphisms in the category $\text{MSet}$ you constructed in Exercise 3.9. (You answer will depend on the notion of morphism you defined in that exercise!)
        \begin{answer}
            Leave for Later.
        \end{answer}
\end{exercises}

\newpage
\begin{topic}
    \section{Universal Properties}
\end{topic}

Categories give us an overview to the reasons for the constructions in algebra. Upcoming are several important constructions satisfying universal properties. One will be how products and disjoint unions will have universal properties relating to $C^{A, B}$ and $C^{A, B}$ in \Fref{exm:1.3.9}.

The later definitions will contain an explicitly description followed by a description of its universal property. The `explicit' description will help in computation and arguments, while the universal property will show the nature of the construction. The description will seem to depend on an arbitrary choice while the universal property will not be arbitrary.

Universal properties will help us view the relationship between concepts, such as how products and disjoint unions of sets are actually mirror constructions.

\begin{definition}[\label{def:1.5.1}]{Initial and Final Objects}
    Let $C$ be a category. We say that an object $I$ of $C$ is \textit{initial} in $C$ if for every object $A$ or $C$ there exists \textit{exactly one} morphism $I \rightarrow A$ in $C$:
        \begin{equation*}
            \forall A \in \text{Obj}(C): \text{Hom}_{C}(I, A) \text{ is a singleton}.
        \end{equation*}
    We say that an object $F$ of $C$ is \textit{final} in $C$ if for every object $A$ or $C$ there exists \textit{exactly one morphism} $A \rightarrow F$ in $C$:
        \begin{equation*}
            \forall A \in \text{Obj}(C): \text{Hom}_{C}(A, F) \text{ is a singleton}.
        \end{equation*}
\end{definition}

You can also use \textit{terminal} to denote either possibility, but in general, it is important to consider which `end' of $C$ that is being considered. Categories do not need to have initial and final objects.

\begin{examples}
    \begin{example}[\label{exm:1.5.2}]
        The category obtained by endowing $\mathbb{Z}$ with the relation $\leq $ from \Fref{exm:1.3.3} has no initial or final object. There is no such integer where $i \in \mathbb{Z}$ is $i \leq a$ for all integers $a$. Similarly, for a final object, we require an object larger than every integer in the set of integers which is impossible. In contrast, the category in \Fref{exm:1.3.6} does have a final object which is the pair $(3, 3)$, but has no initial object. Initial and final objects might not be unique.
    \end{example}

    \begin{example}[\label{exm:1.5.3}]
        In $\text{Set}$, the empty set $\emptyset$ is initial and is the unique set that is initial. For the final objects, there is the singleton set $\{p\}$ which is essentially the constant function. Although they may not be unique, we claim that they are unique up to \textit{isomorphism}. This will be important so here is the proof.

        \textbf{Proposition 5.4}: Let $C$ be a category.
            \begin{itemize}
                \item If $I_{1}, I_{2}$ are both initial objects in $C$, then $I_{1} \cong I_{2}$.

                \item If $F_{1}, F_{2}$ are both final objects in $C$, then $F_{1} \cong F_{2}$. 
            \end{itemize}
        These isomorphisms are also uniquely determined.
            \begin{proof}
                Recall that for every object $A$ of $C$, there is at least one element in $\text{Hom}_{C}(A, A)$, which is the identity $1_{A}$. If $I$ is initial, then there is a unique morphism $I \rightarrow I$, which is the identity $1_{I}$. 

                Now suppose that $I_{1}$ and $I_{2}$ are both initial. There there is a unique morphism $f : I_{1} \rightarrow I_{2}$ in $C$ and likewise, there is a unique morphism $g : I_{2} \rightarrow I_{1}$. Consider $gf : I_{1} \rightarrow I_{1}$. So we must have that $gf = 1_{I_{1}}$ by the top observation, and similarly, $fg = 1_{I_{2}}$. This shows that $f$ is an isomorphism.
            \end{proof}
        The proposition shows that in fact, no initial or final object is more special than the other. Although it may seem like $\{\emptyset\}$ is a natural choice, it does not influence the nature of the interactions between the objects.
    \end{example}
\end{examples}

The notion of universal properties requires the understanding of functors, which will be introduced much later. So the current definition will suffice:

A construction satisfies a universal property when it can be seen as a terminal object of a category. We can say that it is the solution to a universal problem. In simple cases, it could be the statement such as $\emptyset$ is universal with respect tot he property of mapping to sets, which is also the assertion that $\emptyset$ is initial in the category of $\text{Set}$.

The situation is often more complicated, as being initial and final means that there is the existence and uniqueness of certain morphisms. So the explanation of universal follows that object $X$ is universal with respect to the following property: for any $Y$ such that ..., there exists a unique morphism $Y \rightarrow X$ such that ...

It is not uncommon to disregard part of the information about the solution to a universal problem, as this information can be implicitly in a given set-up.

\begin{definition}[\label{def:1.5.3}]{Quotients}
    Let $\sim $ be an equivalence relation defined on a set $A$. Consider the assertion
        \begin{center}
            ``The quotient $A/\sim $ is universal with respect to the property of mapping $A$ to a set in such a way that equivalent elements have the same image.''
        \end{center}
    What does this mean? The assertion talks about functions so we can consider
        \begin{equation*}
            \varphi: A \rightarrow Z
        \end{equation*}
    with $Z$ any set that has the property
        \begin{equation*}
            a^{\prime} \sim a^{\prime\prime} \implies \varphi(a^{\prime}) = \varphi(a^{\prime\prime})
        \end{equation*}
\end{definition}

These morphisms are objects of a category similar to those defined in \Fref{exm:1.3.7}, so we can denote these objects like $(\varphi, Z)$. We should define morphisms $(\varphi_{1}, Z_{1}) \rightarrow (\varphi_{2}, Z_{2})$ as commutative diagrams.
    \begin{center}
        \begin{tikzcd}
            Z_{1}\ar[rr, "\sigma"] &                                               & Z_{2} \\
                                   & Z\ar[ul, "\varphi_{1}"]\ar[ur, "\varphi_{2}"'] &         
        \end{tikzcd}
    \end{center}
This is the same definition as in \Fref{exm:1.3.7}. Does this category have initial objects?

\textbf{Claim 5.5}: Denoting $\pi$ by `canonical projection' defined in \Fref{exm:1.2.6}, the pair $(\pi, A/\sim )$ is an initial object of this category.

\begin{proof}
    Consider any $(\varphi, Z)$. We have to prove that there exists a unique morphism $(\pi, A/\sim ) \rightarrow (\varphi, Z)$ or a unique commutative diagram
        \begin{center}
            \begin{tikzcd}
                A/\sim \ar[rr, "\overline{\varphi}"] &                                    & Z \\
                                                     & A\ar[ul, "\pi"]\ar[ur, "\varphi"'] &     
            \end{tikzcd}
        \end{center}
    Consider an arbitrary element of $A/\sim $. If the diagram is to commute, we require that
        \begin{equation*}
            \overline{\varphi}([a]_{\sim }) = \varphi(a);
        \end{equation*}
    This means that $\overline{\varphi}$ is unique. We now have to check that $\overline{\varphi}$ is well-defined. That is, if $[a_{1}]_{\sim } = [a_{2}]_{\sim }$, then $\varphi(a_{1}) = \varphi(a_{2})$. We have
        \begin{equation*}
            [a_{1}]_{\sim } = [a_{2}]_{\sim } \implies a_{1} \sim a_{2} \implies \varphi(a_{1}) = \varphi(a_{2})
        \end{equation*}
    This is the condition that morphisms of our category satisfy.
\end{proof}

In the assertion above, it does not tell us what category to consider, nor if we should look at the initial objects of the category. Also, the universal problem is not even $A/\sim $, it is actual $\pi: A \rightarrow A/\sim $. It is important to practice the skill of translating the loose assertions to precise statements. 

The reason for the loose assertion is that there is no other morphism to be considered $A \rightarrow A/\sim $ other than the canonical projection, and additionally, the final object of the category is not interesting or of significance. Maybe the object $(p, A)$ with morphism $\varphi^{\prime} : Z \rightarrow \{p\}$? For the commutative diagram:
    \begin{center}
        \begin{tikzcd}
            \{p\} &                                  & Z\ar[ll, "\varphi^{\prime}"'] \\
                  & A\ar[ul, "p"]\ar[ur, "\varphi"'] &                                 
        \end{tikzcd}
    \end{center}
Since for any $Z$, there is only one mapping to the set $\{p\}$.

So what is to gain by viewing quotients in terms of their universal property? Suppose that $\sim $ is defined from a function $f : A \rightarrow B$. We will see that $\Im{f}$ also satisfies the universal property that is given above. This tells us that $A/\sim $ is isomorphic to $\Im{f}$. This shows an abstraction from the canonical decomposition that was done.

It is important to be able to see a universal property from a construction. Now is a good time to stop and consider the idea of a product of two sets and see if there is a universal property that jumps out.

The universal property:
    \begin{center}
        \begin{tikzcd}
                                                             & A \\
            A \times B \ar[ur, "\pi_{A}"]\ar[dr, "\pi_{B}"'] &   \\
                                                             & B   
        \end{tikzcd}
    \end{center}
Where $A, B$ are sets with product $A \times B$ and the two natural projections. Then for every set $Z$ and morphisms
    \begin{center}
        \begin{tikzcd}
                                               & A \\
            Z\ar[ur, "f_{A}"]\ar[dr, "f_{B}"'] &   \\
                                               & B   
        \end{tikzcd}
    \end{center}
there exists a unique morphism $\sigma : A \times B$ such that the diagram
    \begin{center}
        \begin{tikzcd}
                                                                                                  &                                                 & A \\
            Z\ar[urr, "f_{A}", bend left = 20]\ar[r, "\sigma"]\ar[drr, "f_{B}"', bend right = 20] & A \times B\ar[ur, "\pi_{A}"]\ar[dr, "\pi_{B}"'] &   \\
                                                                                                  &                                                 & B   
        \end{tikzcd}
    \end{center}
commutes, where we usually denote $\sigma$ by $f_{A} \times f_{B}$.

\begin{proof}
    Define $\forall z \in Z$
        \begin{equation*}
            \sigma(z) = (f_{A}(z), f_{B}(z))
        \end{equation*}
    This function makes the diagram commute because
        \begin{equation*}
            \pi_{A}\sigma(z) = \alpha_{A}(f_{A}(z), f_{B}(z)) = f_{A}(z)
        \end{equation*}
    showing that $\pi_{A}\sigma = f_{A}$ and similarly, $\pi_{B}\sigma = f_{B}$. Since this definition is forced by the commutativity of the diagram, the morphism $\sigma$ is unique.
\end{proof}
Therefore, the product of sets with their natural projections are the final objects of the category $C_{A, B}$.

Why view the product as this? This allows us to say that this is a universal property in any category, but the definition only makes sense in the category $\text{Set}$. We say that a category $C$ has finite products if for all objects $A, B$ in $C$ the category $C_{A, B}$ has final objects.

Note that the product does not always look like a product. Consider the category from $\leq $ on $\mathbb{Z}$. The objects of the category are $a, b \in \mathbb{Z}$ and call $a \times b$ a categorical product. The universal property now becomes \textit{for all} $z \in \mathbb{Z}$ \textit{such that} $z \leq a$ \textit{and} $z \leq b$, \textit{we have} $z \leq a \times b$.

This universal problem does have a solution $\forall a, b$: it is not called $a \times b$ but rather $\min(a, b)$. We can see that $\min(a, b)$ satisfies this property, therefore, the category has products, and the products are a familiar operation on two integers.

There is a connection between the cartesian product of two sets and the minimum of two integers. Both are products taken from two different categories but they satisfy the same universal property in a different context.

Overview is that what it means to be a universal property is that it holds for all morphisms in a category. That is to say that these morphisms are relations between the objects of a category.

Next are coproducts, and co- often means that there is a reversing of all the arrows. Where products are final objects in the categories $C_{A, B}$ by considering morphisms in $C$ from a common source with targets $A, B$, coproducts are the initial objects in the categories $C^{A, B}$ with common target, who source from $A, B$. Dear reader, look away and spell this universal property out before we do.

Let $A, B$ be objects of a category $C$. A coproduct $A \coprod B$ of $A$ and $B$ will be an object of $C$, endowed with two morphisms $i_{A} : A \rightarrow A \coprod B$, $i_{B} : B \rightarrow A \coprod B$ and satisfying the universal property such that for all objects $Z$ and morphisms
    \begin{center}
        \begin{tikzcd}
            A\ar[dr, "f_{A}"]  &   \\
                               & Z \\
            B\ar[ur, "f_{B}"'] &     
        \end{tikzcd}
    \end{center}
there exists a unique morphism $\sigma : A \coprod B \rightarrow Z$ such that the diagram
    \begin{center}
        \begin{tikzcd}
            A\ar[drr, "f_{A}", bend left = 20]\ar[dr, "i_{A}"]    &                            &   \\
                                                                  & A\coprod B\ar[r, "\sigma"] & Z \\
            B\ar[ur, "i_{B}"']\ar[urr, "f_{B}"', bend right = 20] &                            &     
        \end{tikzcd}
    \end{center}
commutes. There is symmetry with the universal property of products and we say that a category $C$k has coproducts if this universal problem has a solution for all pairs of objects $A$ and $B$. Here is a familiar coproduct:

\textbf{Proposition 5.6}: The disjoint union is a coproduct in $\text{Set}$.

\begin{proof}
    Recall that the disjoint union $A \coprod B$ is defined to be the union of two disjoint isomorphic copies $A^{\prime}, B^{\prime}$ of $A, B$, respectively. We may have $A^{\prime} = \{0\} \times A, B^{\prime} = \{1\} \times B$. Here, the functions $i_{A}, i_{B}$ are 
        \begin{equation*}
            i_{A}(a) = (0, a), \, i_{B}(b) = (1, b)
        \end{equation*}
    where we see these elements as elements of $(\{0\} \times A) \cup (\{1\} \times B)$.

    Now let $f_{A} : A \rightarrow Z$, $f_{B} : B \rightarrow Z$ be arbitrary morphisms to a common target. Define
        \begin{equation*}
            \sigma : A \coprod B = (\{0\} \times A) \cup (\{1\} \times B) \rightarrow Z
        \end{equation*}
    by
        \begin{equation*}
            \sigma(c) = 
                \begin{cases}
                    f_{A}(a) & \text{if } c = (0, a) \in \{0\} \times A\\
                    f_{B}(b) & \text{if } c = (1, b) \in \{1\} \times B
                \end{cases}
        \end{equation*}
    this definition makes the diagram commute and it is forced on us, therefore, this morphism is unique.
\end{proof}

This shows that the category $\text{Set}$ has coproducts and also shows us what disjoint unions are. For example, there were arbitrary choices to be made for a disjoint union, but any choice would lead to isomorphic relations. This is because terminal objects of a category are not unique but they are unique up to isomorphism.

There is also an unexpected symmetry between the products and disjoint unions between sets that becomes apparent when considering universal properties. The reader should also contemplate the notion of coproduct in other categories such as the one from $\leq $ on $\mathbb{Z}$, which does have coproducts. The coproduct of two objects $a, b$ is the maximum of them.

\begin{exercises}{Problem Sets}
    \textbf{Exercise 1}: Prove that a final object in a category $C$ is initial in the opposite category $C^{\text{op}}$.
        \begin{proof}
            Suppose that $f$ is a final object of $C$. This means that for all other objects $g \in C$, we have that there is exactly one morphism in $\text{Hom}_{C}(g, f)$. This means that if we consider the category $C^{\text{op}}$ with $\text{Hom}_{C^{\text{op}}}(f, g) := \text{Hom}_{C}(g, f)$, we have that for all $g \in C^{\text{op}}$, the set $\text{Hom}_{C^{\text{op}}}(f, g)$ is a singleton. This says that $f$ is initial in $C^{\text{op}}$.
        \end{proof}

    \textbf{Exercise 2}: Prove that $\emptyset$ is the \textit{unique} initial object in $\text{Set}$.
        \begin{proof}
            We know that $\emptyset$ is an initial object in $\text{Set}$. This is because there is one morphism from $\emptyset \rightarrow A$ for $A \in \text{Set}$. We also know that initial objects are unique up to isomorphism. This means that $\lvert \emptyset \rvert = \lvert I \rvert$ for any initial object $I$ in $C$. But we know that $\emptyset$ is a subset of all sets. Therefore, $\emptyset \subseteq I$ and $\lvert \emptyset \rvert = \lvert I \rvert$ so $\emptyset = I$.
        \end{proof}

    \textbf{Exercise 3}: Prove that final objects are unique up to isomorphism.
        \begin{proof}
            Suppose that $F_{1}$ is a final object. This means that $\lvert \text{Hom}_{C}(F_{1}, F_{1}) \rvert = 1$ and that implies that the element in the set must be $1_{F_{1}}$.

            Suppose that $F_{1}, F_{2}$ are final objects. Then to prove isomorphism, we have to prove a bijection between them. By the fact that they are final objects, we have that there is one mapping $\varphi : F_{1} \rightarrow F_{2}$ and another which is $\varphi^{\prime} : F_{2} \rightarrow F_{1}$. Now we compose them and observe that:
                \begin{align*}
                    \varphi\varphi^{\prime}  &\in  \text{Hom}_{C}(F_{2}) \\
                    \varphi^{\prime}\varphi  &\in  \text{Hom}_{C}(F_{1})   
                \end{align*}
            This means that either morphisms are both left-sided and right-sided inverses. Therefore, the mapping $\varphi$ is a bijection. We conclude that final objects are isomorphic.
        \end{proof}

    \textbf{Exercise 4}: What are initial and final objects in the category of `pointed sets' (\Fref{exm:1.3.8})? Are they unique?
        \begin{proof}
            For the initial objects in $\text{Set}^{*}$, we note that the morphisms $\sigma$ of $\text{Set}^{*}$ are such that $\sigma : S \rightarrow T$ with $\sigma(s) = t$ for objects $(S, s), (T, t)$. Note that the number of morphisms between two sets of size $m$ and $n$ is $n^{m}$ where $n$ is the cardinality of the codomain set. 
                \begin{align*}
                    n^{m} & =  1 \\
                    m     & =  0
                \end{align*}
            Therefore, it is forced that the size of the initial object have cardinality $0$ which is therefore $\emptyset$. This is the unique initial object. As for the final object, we do the same thing:
                \begin{align*}
                    n^{m} & =  1 \\
                    n     & =  1   
                \end{align*}
            Which shows us that the final objects are singletons. These are not unique, but they are unique up to isomorphism.
        \end{proof}

    \textbf{Exercise 5}: What are the final objects in the category considered in 5.3?
        \begin{proof}
            What's 5.3?
        \end{proof}

    \textbf{Exercise 6}: Consider the category corresponding to endowing (as in \Fref{exm:1.3.3}) the set $\mathbb{Z}^{+}$ of positive integers with the \textit{divisibility} relation. Thus there is exactly one morphism $d \rightarrow m$ in this category if and only if $d$ divides $m$ without remainder; there is no morphism between $d$ and $m$ otherwise. Show that this category has products and coproducts. What are their `conventional' names?
        \begin{proof}
            The product and coproducts do exist in this category. For the product, we have that it is known by the lcm while for the coproduct, it is known as the gcd:
                \begin{center}
                    \begin{tikzcd}
                                                                                                      &                                    & b \\
                        a\ar[r, "\sigma"]\ar[urr, "f", bend left = 20]\ar[drr, "g"', bend right = 20] & \gcd(b, c)\ar[ur, ""]\ar[dr, ""] &   \\
                                                                                                      &                                    & c   
                    \end{tikzcd}
                \end{center}
                \begin{center}
                    \begin{tikzcd}
                        b\ar[dr, ""]\ar[drr, "f", bend left = 20]   &                              &   \\
                                                                    & \lcm(b, c)\ar[r, "\sigma"] & a \\
                        c\ar[ur, ""]\ar[urr, "g"', bend right = 20] &                              &     
                    \end{tikzcd}
                \end{center}
        \end{proof}
    \textbf{Exercise 7}: Redo Exercise $2.9$, this time using Proposition 5.4.
        \begin{proof}
            Later.
        \end{proof}

    \textbf{Exercise 8}: Show that in every category $C$ the products $A \times B$ and $B \times A$ are isomorphic, if they exist.
        \begin{proof}
            Because they both satisfy the universal property for the product of $A, B$, we have that they are both final objects and are therefore isomorphic.
        \end{proof}

    \textbf{Exercise 10}: Let $C$ be a category with products. Find a reasonable candidate for the universal property that the product $A \times B \times C$ of \textit{three} objects of $C$ ought to satisfy, and prove that both $(A \times B) \times C$ and $A \times (B \times C)$ satisfy this universal property. Deduce that $(A \times B) \times C$ and $A \times (B \times C)$ are necessarily isomorphic.
        \begin{proof}
            The product $A \times B \times C$ solves the universal problem of mappings from a set $Z$ to three other sets $A, B, C$. Observe that the morphism that goes to our final object is determined uniquely by the elements $a_{i}, b_{i}, c_{i}$ such that $z_{i} \mapsto a_{i}, z_{i} \mapsto b_{i}, z_{i} \mapsto c_{i}$. This means that the structure of the sets $(A \times B) \times C$ and $A \times (B \times C)$ does not change the fact that they are both final objects and are therefore isomorphic.
        \end{proof}

    \textbf{Exercise 10}: Push the envelope a little further still, and define products and coproducts for \textit{families}(i.e., indexed sets) of objects of a category.
        \begin{enumerate}
            \item Do these exist in $\text{Set}$?
                \begin{answer}
                    Try Later.
                \end{answer}

            \item It is common to denote the product $\underbrace{A \times \cdots \times A}_{n \text{ times}}$ by $A^{n}$.
                \begin{answer}
                    Try Later.
                \end{answer}
        \end{enumerate}

    \textbf{Exercise 11}: Let $A$, resp. $B$ be the set, endowed with an equivalence relation $\sim_{A}$, resp. $\sim_{B}$. Define a relation $\sim $ on $A \times B$ by setting
        \begin{equation*}
            (a_{1}, b_{1}) \sim (a_{2}, b_{2}) \iff a_{1} \sim_{A} a_{2} \text{ and } b_{1} \sim_{B} b_{2}
        \end{equation*}
        \begin{itemize}
            \item Use the universal property for quotients in \Fref{def:1.5.3} to establish that there are functions $(A \times B)/ \sim \rightarrow A/ \sim_{A}$, $(A \times B) / \sim \rightarrow B/ \sim_{B}$.
                \begin{proof}
                    We start off by declaring $\varphi_{1}$ to be the function such that:
                        \begin{equation*}
                            a \sim a^{\prime} \land b \sim b^{\prime} \iff (a, b) \sim (a^{\prime}, b^{\prime})
                        \end{equation*}
                    tells us that:
                        \begin{equation*}
                            \varphi_{1}([a, b]) = [a] = \varphi_{1}([a^{\prime}, b^{\prime}])
                        \end{equation*}
                    This mapping is well-defined, which we can tell by glancing. We define this analogously for the function $(A \times B)/\sim \rightarrow B/\sim_{B}$.
                \end{proof}

            \item Prove that $(A \times B) / \sim $, with these two functions, satisfies the universal property for the product of $A/\sim_{A}$ and $B/\sim_{B}$.
                \begin{proof}
                    What is to show is that the object:
                        \begin{center}
                            \begin{tikzcd}
                                                               & A/\sim_{A} \\
                                (A \times B)/\sim \ar[ur, "\varphi_{1}"]\ar[dr, "\varphi_{2}"'] &            \\
                                                                                                & B/\sim_{B}   
                            \end{tikzcd}
                        \end{center}
                    is a final object. Suppose that we have an arbitrary set $Z$. We define the functions $\tau_{1}: Z \rightarrow A/\sim_{A}$ and $\tau_{2}: Z \rightarrow A/\sim_{B}$. Now we define a function $\sigma: Z \rightarrow (A \times B)/\sim $ by the following:
                        \begin{equation*}
                            \sigma(z_{i}) = [(a_{i}, b_{i})]
                        \end{equation*}
                    for $\tau_{1}(z_{i}) = [a_{i}]$ and $\tau_{2}(z_{i}) = [b_{i}]$. Observe that this function $\sigma$ is unique as it is forced by definition based on the $\varphi_{1}$ and $\varphi_{2}$ we defined.
                \end{proof}

            \item Conclude (without further work) that $(A \times B)/\sim \cong (A/\sim_{A}) \times (B/\sim_{B})$. 
                We conclude that the object:
                    \begin{center}
                        \begin{tikzcd}
                                                           & A/\sim_{A} \\
                            (A \times B)/\sim \ar[ur, "\varphi_{1}"]\ar[dr, "\varphi_{2}"'] &            \\
                                                                                            & B/\sim_{B}   
                        \end{tikzcd}
                    \end{center}
                is a final object while
                    \begin{center}
                        \begin{tikzcd}
                                                                      & A/\sim_{A} \\
                            (A/\sim_{A}) \times (B/\sim_{B})\ar[ur, "\pi_{1}"]\ar[dr, "\pi_{2}"'] &            \\
                                                                                                  & B/\sim_{B}   
                        \end{tikzcd}
                    \end{center}
                is also a final object by the standard projection that we first used to identify products. Therefore, there is a commutative diagram to represent an isomorphism between them:
                    \begin{center}
                        \begin{tikzcd}
                                                                               & A/\sim_{A} &                                             \\
                            (A \times B)/\sim \ar[ur, "\varphi_{1}"]\ar[dr, "\varphi_{2}"']\ar[rr, "\sigma"] &                               & (A/\sim_{A}) \times (B/\sim_{B})\ar[ll, ""]\ar[dl, "\pi_{2}"]\ar[ul, "\pi_{1}"'] \\
                                                                                                            & B/\sim_{B} &                                               
                        \end{tikzcd}
                    \end{center}
                since there is a bijection $\sigma$ between the sets, that means that $(A \times B)/\sim \cong (A/\sim_{A}) \times (B/\sim_{B})$.
        \end{itemize}

    \textbf{Exercise 12}: Define the notions of \textit{fibered products} and \textit{fibered coproducts}, as terminal objects of the categories $C_{\alpha, \beta}, C^{\alpha, \beta}$ considered in \Fref{exm:1.3.10} by stating carefully the corresponding universal properties.
        \begin{answer}
            We define a fibered product as the object in $C_{\alpha, \beta}$ such that for any object $A$, commutative diagram in $C_{\alpha, \beta}$, there exists a unique morphism from $A$ to the fibered product to which 
        \end{answer}

    As it happens, $\text{Set}$ has both fibered products and coproducts. Define these objects `concretely', in terms of naive set theory.
\end{exercises}

\chapter{Groups, First Encounter}

We will study the category $\text{Grp}$ in which we look at monomorphisms and epimorphisms. We will also look at equivalence relations and quotients of a group and how the decomposition theorem works in $\text{Grp}$. For Chapter III, we will study rings and in Chapter IV, we will look at Sylow theorems, `composition series', and the classification of finite abelian groups.

\begin{topic}
    \section{Definition of Group}
\end{topic}

\textbf{Joke 2.1.1} Definition: A group is a groupoid with a single object.

This is true in which we defined a groupoid in \Fref{exm:1.4.6}. Looking closely at the definition, this says that if $*$ is the single object of a groupoid $G$,
    \begin{equation*}
        \text{Hom}_{G}(*, *) = \text{Aut}_{G}(*)
    \end{equation*}
since $G$ is a groupoid. Call this set $G$. By the definition of a category, there is associativity in $G$, there is an identity, and for every $g \in G$, there is an inverse $g^{-1} \in G$. So this is the definition of a group:

\begin{definition}[\label{def:2.1.2}]{Group}
    Suppose that $G$ is a nonempty set with a binary operation, with a multiplication map:
        \begin{equation*}
            \cdot : G \times G \rightarrow G
        \end{equation*}
    the notation is
        \begin{equation*}
            \cdot(g, h) := g \cdot h
        \end{equation*}
    or simply 
        \begin{equation*}
            gh
        \end{equation*}
    Then $G$ is a group if 
        \begin{enumerate}
            \item the operation $\cdot$ is \textit{associative}:
                \begin{equation*}
                    (\forall g, h, k \in G): (gh)k = g(hk)
                \end{equation*}

            \item there exists an \textit{identity} element $e_{G}$ for $\cdot$:
                \begin{equation*}
                    (\exists e_{G} \in G)(\forall g \in G) : ge = g = eg
                \end{equation*}

            \item every element in $G$ has an \textit{inverse} with respect to $\cdot$:
                \begin{equation*}
                    (\forall g \in G)(\exists h \in G) : gh = e_{G} = hg
                \end{equation*}
        \end{enumerate}
\end{definition}

\begin{examples}
    \begin{example}[\label{exm:2.1.3}]
        Since $G$ is nonempty, at the very least, we have $G = \{e\}$. This is the trivial group.
    \end{example}

    \begin{example}[\label{exm:2.1.4}]
        It should be checked that $(\mathbb{Z}, +)j$, $(\mathbb{Q}, +)$, $(\mathbb{R}, +)$, $(\mathbb{C}, +)$ are all groups. These examples are not very interesting as they are common and are to specialized. They are known as commutative groups.
    \end{example}

    \begin{example}[\label{exm:2.1.5}]
        There is also a non-commutative group for the set of invertible $n \times n$ matrices for $n \geq 2$. The product of two matrices might not always commute.
    \end{example}
\end{examples}

We note that the identity is unique using the standard proof:
    \begin{proof}
        If $e_{G}$ and $h$ are identities:
            \begin{equation*}
                h = eh = e
            \end{equation*}
        which concludes the proof.
    \end{proof}

\textbf{Proposition 2.1.7}: Inverses are also unique. We use the same trick as the last proof:
    \begin{proof}
        Suppose that $h$ is in $G$ and that $h^{-1}, g^{-1}$ are inverses:
            \begin{equation*}
                hg^{-1} = e
            \end{equation*}
        therefore, 
            \begin{equation*}
                h^{-1}hg^{-1} = h^{-1} = eg^{-1} = g^{-1}
            \end{equation*}
    \end{proof}

Elements of a group in general do not commute. They will commute if they are the same element:
    \begin{equation*}
        g^{n} = \underbrace{g \cdots g}_{n \text{ times }}, \, \, g^{-n} = \underbrace{g^{-1} \cdots g^{-1}}_{n \text{ times }}
    \end{equation*}

Cancellation holds in groups because of the existence of inverses. We can only cancel on same side however:

\textbf{Proposition 2.1.8}: Let $G$ be a group. Then $\forall a, g, h \in G$:
    \begin{equation*}
        ga = ha \implies g = h, \, \, ag = ah \implies g = h
    \end{equation*}
the proof is left as an exercise for the reader.

Cancellation in general does not work in settings outside of groups, such as how the zero element in $\mathbb{R}$ does not have an inverse. To force a group, such as the multiplicative group on $\mathbb{R}$, we require:
    \begin{equation*}
        \mathbb{R}^{*} := \mathbb{R} \backslash \{0\}
    \end{equation*}

\begin{definition}[\label{def:2.1.5}]{Commutative Groups}
    We say that a group is commutative if its binary operation is commutative:
        \begin{equation*}
            gh = hg
        \end{equation*}
    Commutative groups arise in instances such as `modules over the ring $\mathbb{Z}$'. These commutative groups are simply called abelian groups.
\end{definition}

\begin{definition}[\label{def:2.1.9}]{Order of an Element}
    An element $g \in G$ has finite order if $g^{n} = e$ for some positive integer $n$. The order is the smallest $n$.
\end{definition}

\textbf{Lemma 2.1.10}: If $g^{n} = e$ for some positive integer $n$, then $\text{ord}(g) \divides n$.

We can use a proof by contradiction and division algorithm.

\textbf{Corollary 2.1.11}: Let $g$ be an element of finite order and $N \in \mathbb{Z}$. Then
    \begin{equation*}
        g^{N} = e \iff N \text{ is a multiple of } \text{ord}(g)
    \end{equation*}
this is an observation of the consequence of the proof in Lemma 2.1.10.

\begin{definition}[\label{def:2.1.12}]{Order of a Group}
    If $G$ is a finite set, we write $\lvert G \rvert$ for the number of elements in $G$ and $\lvert G \rvert = \infty$ otherwise.
\end{definition}

We note that the order of an element in $G$ cannot exceed $\lvert G \rvert$. Try proving this!

There is a stronger relation to be said between the order of an element and the group through Lagrange's Theorem. The elements of a group are not always predictable. We may have $f, g$ finite order but $\text{ord}(fg) = \infty$.

\textbf{Proposition 2.1.13}: Let $g \in G$ be an element of finite order. Then $g^{m}$ has finite order $\forall m \geq 0$, and also:
    \begin{equation*}
        \text{ord}(g^{m}) = \dfrac{\lcm(m, \text{ord}(g))}{m} = \dfrac{\lvert g \rvert}{\gcd(m, \text{ord}(g))}
    \end{equation*}

\begin{proof}
    We first establish the equality on the RHS. This is true because of the fact that:
        \begin{equation*}
            \dfrac{ab}{\gcd(a, b)} = \lcm(a, b)
        \end{equation*}
    Now to connect the equality to $\text{ord}(g^{m})$, we note that if
        \begin{equation*}
            g^{a} = e
        \end{equation*}
    We have that $m \divides a$, so what we really desire is the least common factor:
        \begin{equation*}
            g^{\lcm(m, a)} = e
        \end{equation*}
    Which is almost there. We perform the decomposition:
        \begin{equation*}
            (g^{m})^{\frac{\lcm(a, m)}{m}} = e
        \end{equation*}
    and we're done.

    We could also say that the order is the least $d$ such that:
        \begin{equation*}
            g^{md} = e
        \end{equation*}
    Therefore, we have $m \cdot \text{ord}(g) = \lcm(m, \text{ord}(g))$.
\end{proof}

\textbf{Proposition 2.1.14}: If $gh = hg$, then $\text{ord}(gh)$ divides $\lcm(\text{ord}(g), \text{ord}(h))$.
    \begin{proof}
        Suppose that elements $g, h$ commute. If we take both to the power off $\lcm(\text{ord}(a), \text{ord}(b))$, we get:
            \begin{equation*}
                gh^{\lcm(\text{ord}(a), \text{ord}(b))} = g^{\lcm(\text{ord}(a), \text{ord}(b))}h^{\lcm\text{ord}(a), \text{ord}(b)} = e
            \end{equation*}
        Therefore, the order divides this.
    \end{proof}

\begin{exercises}{Problem Sets}
    \textbf{Exercise 1}: Write a careful proof that every group is the group of isomorphisms of a groupoid. In particular, every group is the group of automorphisms of some object in some category.
        \begin{proof}
            Define the category with:
                \begin{itemize}
                    \item The object is the set $G$.

                    \item Morphisms in the group are in the set $\text{Hom}(G, G)$, which we call are the elements of $G$. These morphisms are defined as $f : G \rightarrow G$ where $f : g \in G \mapsto fg$.
                \end{itemize}
            This defines a category because the composition of morphisms is possible and associative. The identity function exists and so does the inverse, making every morphism an isomorphism. Therefore, the group is the collection of isomorphisms in this category.
        \end{proof}

    \textbf{Exercise 2}: Consider the `sets of numbers' listed in 1.1 and decide which are made into groups by conventional operations such as $+$ and $\cdot$. Even if the answer is negative (such as $(\mathbb{R}, \cdot)$), see if variations on the definitions of these sets lead to groups (for example, $(\mathbb{R}^{*}, \cdot)$).
        \begin{answer}
            Leave for Later.
        \end{answer}

    \textbf{Exercise 3}: Prove that $(gh)^{-1} = h^{-1}g^{-1}$ for all elements $g, h$ of a group $G$.
        \begin{proof}
            We are tasked with finding the inverse of $gh$. We denote that as $i$:
                \begin{equation*}
                    ghi = e \implies hi = g^{-1} \implies i = h^{-1}g^{-1}
                \end{equation*}
            So we are done.
        \end{proof}

    \textbf{Exercise 4}: Suppose that $g^{2} = e$ for all elements $g$ of a group $G$; prove that $G$ is commutative.
        \begin{proof}
            If $g^{2} = e$ for all elements, we consider $gh$ for $g, h \in G$. Notice that 
                \begin{equation*}
                    (gh)^{-1} = h^{-1}g^{-1}
                \end{equation*}
            But since $g^{2} = e$ for all elements, we have $gh = h^{-1}g^{-1}$ and therefore, $gh = hg$.
        \end{proof}
 
    \textbf{Exercise 5}: The `multiplication table' of a group is an array compiling the results of all multiplications $gh$:
        \begin{align*}
            \begin{array}{ c c c c c }
                \cdot  & e      & \ldots & h      & \ldots \\
                e      & e      & \ldots & h      & \ldots \\
                \ldots & \ldots & \ldots & \ldots & \ldots \\
                g      & g      & \ldots & gh     & \ldots \\
                \ldots & \ldots & \ldots & \ldots & \ldots   
            \end{array}   
        \end{align*}
    Prove that every row and every column of the table contains all elements of the group exactly once.
        \begin{proof}
            Suppose that a given row for $g \in G$ contains duplicate elements $gh_{1}$ and $gh_{2}$. Then we have:
                \begin{equation*}
                    gh_{1} = gh_{2} \implies h_{1} = h_{2}
                \end{equation*}
            This can be generalized to the columns also.
        \end{proof}

    \textbf{Exercise 6}: Prove that there is only one possible multiplication table for $G$ if $G$ has exactly 1, 2, or 3 elements. Analyze the possible multiplication tables for groups with exactly 4 elements, and show that there are two distinct tables, up to reordering the elements of $G$. Use these tables to prove that all groups with $\leq 4$ elements are commutative.
        \begin{proof}
            Not interested.
        \end{proof}

    \textbf{Exercise 7}: Prove Corollary 2.1.11.
        \begin{proof}
            Later.
        \end{proof}

    \textbf{Exercise 8}: Let $G$ be a finite group with exactly one element $f$ of order 2. Prove that $\prod_{g \in G}^{} g = f$.
        \begin{proof}
            We first note that since there is only one element of order 2, there can only be generating element of even order, for example, let $n$ be even and observe that if $g^{n} =e$ we have $g^{\frac{1}{2} = e}$. Now suppose we have the group $G\backslash \{g^{\frac{1}{2}}\}$. For all odd ordered elements $g$ and order $n$, we have
                \begin{equation*}
                    g, g^{2}, g^{3}, \ldots, g^{n - 1} \implies \prod_{i = 1}^{n - 1} g = g^{\frac{n(n - 1)}{2}}
                \end{equation*}
            But since $n$ is odd, we must have $n - 1$ even. Therefore, $n$ divides $\prod_{i = 1}^{n - 1} g$ which means that the product of this set of elements is just $e$. Now for the set generated by the element of even order:
                \begin{equation*}
                    g, g^{2}, \ldots, g^{\frac{n}{2} - 1}, g^{\frac{n}{2} + 1}, \ldots, g^{n - 1}
                \end{equation*}
            To which we have the order as $g^{\frac{(n)(n - 2)}{2}}$. Since $n - 2$ is even we have that $n$ also divides this product of elements. Therefore, 
                \begin{equation*}
                    \prod_{g \in G \backslash \{g^{\frac{n}{2}}\}}^{} g = e
                \end{equation*}
            So if we add back in $f$, we have the result as desired.
        \end{proof}

    \textbf{Exercise 9}: Let $G$ be a finite group, of order $n$, and let $m$ be the number of elements $g \in G$ of order exactly $2$. Prove that $n - m$ is odd. Deduce that if $n$ is even, then $g$ necessarily contains elements of order 2.
        \begin{proof}
            (Part I) Consider the generating element $g$ of odd order $n$. Observe that this element contributes an even number of elements to the order of $G$:
                \begin{equation*}
                    g, g^{2}, \ldots, g^{n - 1}
                \end{equation*}
            To which this list has $n - 1 - 1 + 1$ or $n - 1$ elements. Now consider the generating element $g$ of even order $n$.
                \begin{equation*}
                    g, g^{2}, \ldots, g^{n - 1}
                \end{equation*}
            Each such element has one element of order 2. Suppose we have $m$ such elements. Notice that the list above has an odd number of elements, $o$ Therefore, we have a contribution of an odd number of elements $o \cdot m$. This makes $n - m$ odd. 

            (Part II) From the first part that we concluded, observe that if there are no elements of order 2, then $n$ is odd. Therefore, by contrapositive, we have what we want.
        \end{proof}

    \textbf{Exercise 10}: Suppose that the order of $g$ is odd. What can you say about the order of $g^{2}$?
        \begin{proof}
            Recall that the order of $g^{2}$ is equal to the least common multiple of $2$ and $g$ divided by 2. Therefore, we have $\lcm(n, 2) = 2n$ and
                \begin{equation*}
                    \frac{\lcm(n, 2)}{2} = n
                \end{equation*}
        \end{proof}

    \textbf{Exercise 11}: Prove that for all $g, h$ in a group $G$, $\lvert gh \rvert = \lvert hg \rvert$.
        \begin{proof}
            Suppose that $\text{ord}(gh) = n$. Then we have:
                \begin{equation*}
                    (gh)^{n} = e
                \end{equation*}
            or in expanded form:
                \begin{equation*}
                    \underbrace{gh \cdots gh}_{n \text{ times }} = e
                \end{equation*}
            Now if we take $hg$ to the power of $n + 1$, observe that we have:
                \begin{equation*}
                    h \cdot \underbrace{gh \cdots gh}_{n \text{ times }} \cdot g = hg
                \end{equation*}
            Therefore, we have
                \begin{equation*}
                    (hg)^{n + 1} = hg
                \end{equation*}
            So the order of $gh$ divides the order of that of $hg$. We can show the same vice versa. Therefore, the orders are equal. *-We had to show that this $n$ was the least positive integer for $hg$. Apparently an alternate proof utilizes the hint: (Hint: Prove that $\lvert aga^{-1} \rvert = \lvert g \rvert$ for all $a, g$ in $G$).
        \end{proof}

    \textbf{Exercise 12}: In the group of invertible $2 \times 2$ matrices, consider
        \begin{equation*}
            g = \begin{bmatrix}
                0 & -1 \\
                1 & 0    
            \end{bmatrix}, \, 
            h = \begin{bmatrix}
                0  & 1  \\
                -1 & -1   
            \end{bmatrix}
        \end{equation*}
    Verify that $\text{ord}(g) = 4$, $\text{ord}(h) = 3$, and $\text{ord}(gh) = \infty$.
        \begin{proof}
            The first two problems are just verification. We check the last one:
                \begin{equation*}
                    gh = \begin{bmatrix}
                        0 & -1 \\
                        1 & 0    
                    \end{bmatrix}
                    \begin{bmatrix}
                        0  & 1  \\
                        -1 & -1   
                    \end{bmatrix} =
                    \begin{bmatrix}
                        1 & 1 \\
                        0 & 1   
                    \end{bmatrix}
                \end{equation*}
            Now we look at powers of this matrix:
                \begin{align*}
                    \begin{bmatrix}
                        1 & 1 \\
                        0 & 1   
                    \end{bmatrix}^{2} &= 
                    \begin{bmatrix}
                        1 & 2 \\
                        0 & 1   
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 2 \\
                        0 & 1   
                    \end{bmatrix} 
                    \begin{bmatrix}
                        1 & 1 \\
                        0 & 1   
                    \end{bmatrix}  &=
                    \begin{bmatrix}
                        1 & 3 \\
                        0 & 1   
                    \end{bmatrix}
                \end{align*}
            We notice that this might never reach the identity. The proof of this is a verification.
        \end{proof}

    \textbf{Exercise 13}: Give an example showing that $\text{ord}(gh)$ is not necessarily equal to $\lcm(\text{ord}(g), \text{ord}(h))$, even if $g$ and $h$ commute.
        \begin{proof}
            One example is if $\text{ord}(g) = 4$ and $h = g$. We have $gh = g^{2}$. But the order of $g^{2}$ is $2$, which is not $\lcm(4, 4)$.
        \end{proof}

    \textbf{Exercise 14}: As a counter point to Exercise 1.13, prove that if $g$ and $h$ commute and $\gcd(\text{ord}(g), \text{ord}(h)) = 1$, then $\text{ord}(gh) = \text{ord}(g)\text{ord}(h)$.
        \begin{proof}
            Suppose that $N$ is the order of $gh$. Then we have:
                \begin{equation*}
                    N = \lcm(\lvert g \rvert, \lvert h \rvert)
                \end{equation*}
            but by the fact that
                \begin{equation*}
                    \lcm(g, h) = \dfrac{gh}{\gcd(g, h)}
                \end{equation*}
            we have that $N = \lvert g \rvert\lvert h \rvert$
        \end{proof}

    \textbf{Exercise 15}: Let $G$ be a commutative group, and let $g \in G$ be an element of maximal \textit{finite} order, that is, such that if $h \in G$ has finite order, then $\lvert h \rvert \leq \lvert g \rvert$. Prove that in fact if $h$ has finite order in $G$, then $\lvert h \rvert$ \textit{divides} $\lvert g \rvert$.
    \begin{proof}
        Consider for contradiction that we have 
            \begin{align*}
                \lvert g \rvert &= p^{m}r & \lvert h \rvert &= p^{n}s   
            \end{align*}
        with $m < n$, $p$ a prime integer, and $r, s$ relatively prime to $p$. We then seen that the order of $g^{p^{m}}$ is just $r$ as $(g^{p^{m}})^{r}$. We apply the same reasoning to see that the order of $h^{s}$ is $p^{n}$. Since $\gcd(p^{n}, r) = 1$, we can conclude that the order of $g^{p^{m}}h^{s}$ is the product of $\lvert g^{p^{m}} \rvert\lvert h^{s} \rvert = p^{n}r$. But this is a contradiction because we have found and element with greater order than $g$ as $n > m$.
    \end{proof}
\end{exercises}

\begin{topic}
    \section{Examples of Groups}
\end{topic}

We have already seen that every object $A$ of every category $C$ determines a group called $\text{Aut}_{C}(A)$ which is the group of automorphisms of $A$. We will see that groups arise from these automorphisms in what will be discussed as group actions.

\begin{definition}[\label{def:2.2.1}]{Symmetric Group}
    The symmetric group is the group of permutations denoted $S_{A}$ is the group $\text{Aut}_{\text{Set}}(A)$. The group of permutations of the set $\{1, \ldots, n\}$ is denoted $S_{n}$.
\end{definition}

The group of automorphisms of $A$ is just the set of bijections between $A$ and itself. We have already shown that $\lvert S_{n} \rvert = n!$. Elements of $S_{A}$ are functions and therefore, the binary action should be the composition of functions. It is important to know the elements of $S_{n}$ for small $n$. In $S_{3}$, notice that the elements are not commutative. 

\begin{definition}[\label{def:2.2.2}]{Dihedral Groups}
    The dihedral groups can be thought of as the symmetries on a 2d regular polygon of $n$ sides. We denote this group as $D_{2n}$. These group elements consists of rotation and reflection. The precise definition of this group is:
        \begin{equation*}
            D_{2n} = \{r^{a}s^{b} : rsr^{-1} = s^{-1} \land r^{2} = e \land s^{n} = e\}
        \end{equation*}
\end{definition}

Notice that $D_{6} \cong S_{3}$. We can show this through a group homomorphism which is a morphism between groups that preserves the structure of composition of group elements.

\begin{definition}[\label{def:2.2.3}]{Cyclic Groups and Modular Arithmetic}
    Let $n$ be a positive integer and consider the equivalence relation on $\mathbb{Z}$ defined by
        \begin{equation*}
            (\forall a, b \in \mathbb{Z}) : a \equiv b \mod{n} \iff n \divides (b - a).
        \end{equation*}
    This is called congruence modulo $n$. We have seen this relation in \Fref{exm:1.1.3}. The set of equivalence classes $\mathbb{Z} / \sim $ is called $\mathbb{Z}_{n}$ or $\mathbb{Z} / n\mathbb{Z}$.

    These groups are also called cyclic groups with the notation $C_{n}$ where we say that the group $C_{n}$ is generated by one element $x$ where $x^{n} = e$. For $\mathbb{Z} / n\mathbb{Z}$, the element $[1]$ generates the group.
\end{definition}

It should be checked that $\mathbb{Z}/n\mathbb{Z}$ consists of exactly $n$ elements:
    \begin{equation*}
        [0]_{n}, [1]_{n}, \ldots, [n - 1]
    \end{equation*}
This is an abelian group with respect to addition:
    \begin{equation*}
        [a] + [b] := [a + b]
    \end{equation*}
which we have to check is well defined.

\textbf{Proposition 2.2.3}: The order of $[m]$ in $\mathbb{Z}/ n\mathbb{Z}$ is $1$ if $n \divides m$ and more generally, 
    \begin{equation*}
        \lvert [m]_{n} \rvert = \dfrac{n}{\gcd(m, n)}
    \end{equation*}
\begin{proof}
    We see that if $n \divides m$, then $[m] = [0]$. For the general statement, we see that $[m] = [1]^{m}$. Applying the past proposition, we have that 
        \begin{equation*}
            \lvert [m]_{n} \rvert = \frac{n}{\gcd(m, n)}
        \end{equation*}
\end{proof}

The proof shows that the order of every element of a cyclic group divides the order of the group.

\textbf{Corollary 2.2.5}: The class $[m]_{n}$ generates $\mathbb{Z}/n\mathbb{Z}$ if and only if $\gcd(m, n) = 1$.

This leads to the fact that any group $\mathbb{Z}/p\mathbb{Z}$ where $p$ is prime is generated by all of its elements except for $0$.

Also note that it should be checked that we have multiplication defined in the group. The operation:
    \begin{equation*}
        [a] \cdot [b] := [ab]
    \end{equation*}
is also well-defined but does not have a group structure unless we modify it:
    \begin{equation*}
        (\mathbb{Z}/n\mathbb{Z})^{*} := \{[m] \in \mathbb{Z}/n\mathbb{Z} : \gcd(m, n) = 1\}
    \end{equation*}
This subset is well-defined.

\textbf{Proposition 2.2.6}: Multiplication makes $(\mathbb{Z}/n\mathbb{Z})^{*}$ into a group.
    \begin{proof}
        We observe that the group is closed under multiplication as if $\gcd(m_{1}, n) = 1$ and $\gcd(m_{2}, n) = 1$, we have that $\gcd(m_{1}m_{2}, n) = 1$. We have associativity as a property of multiplication. The identity exists because $\gcd(m, n) = 1$ therefore, we have that $[m]$ generates the additive group and so $[1]$ must be in there:
            \begin{equation*}
                [m][a] = [1]
            \end{equation*}
        Therefore, multiplicative inverses exist. To verify that $\gcd(a, n) = 1$, we make the observation that if $[a][m] = [1]$, we have that $[am] = [1]$ and therefore, $am = bn + 1$. If $r \divides a$, we must have that $r \divides bn + 1$. But if $r$ divides $bn + 1$, it cannot divide $bn$, otherwise, we have $r \divides 1$. So $r$ is 1. Therefore, $\gcd(a, n) = 1$.
    \end{proof}

For $n = p$ where $p$ is a positive prime, the group $((\mathbb{Z}/ p\mathbb{Z})^{*}, \cdot)$ has order $p - 1$.

\begin{exercises}{Problem Sets}
    \textbf{Exercise 1}: One can associate an $n \times n$ matrix $M_{\sigma}$ with a permutation $\sigma \in S_{n}$ by letting the entry at $(i, \sigma(i))$ be $1$ and letting all other entries be $0$. For example, the matrix corresponding to the permutation
        \begin{equation*}
            \sigma = 
            \begin{bmatrix}
                1 & 2 & 3 \\
                3 & 2 & 1   
            \end{bmatrix} \in S_{3}
        \end{equation*}
    would be 
        \begin{equation*}
            \begin{bmatrix}
                0 & 0 & 1 \\
                0 & 1 & 0 \\
                1 & 0 & 0   
            \end{bmatrix}
        \end{equation*}
    Prove that, with this notation,
        \begin{equation*}
            M_{\sigma\tau} = M_{\sigma}M_{\tau}
        \end{equation*}
    for all $\sigma, \tau \in S_{n}$, where the product on the right is the ordinary product of matrices.
        \begin{proof}
            Leave for Later.
        \end{proof}

    \textbf{Exercise 2}: Prove that if $d \leq n$, then $S_{n}$ contains elements of order $d$.
        \begin{answer}
            We just need 
                \begin{equation*}
                    (1 \, \ldots \, d)
                \end{equation*}
        \end{answer}

    \textbf{Exercise 3}: For every positive integer $n$ find an element of order $n$ in $S_{n}$.
        \begin{answer}
            We just need
                \begin{equation*}
                    (1 \, \ldots \, n)
                \end{equation*}
        \end{answer}

    \textbf{Exercise 4}: Define a homomorphism $D_{8} \rightarrow S_{4}$ by labeling vertices of a square, as we did for a triangle in 2.2. List the 8 permutations in the image of this homomorphism.
        \begin{answer}
            We just label the vertices of the square by 1, 2, 3, 4 and use an identity map.
        \end{answer}

    \textbf{Exercise 5}: Describe generators and relations for all dihedral groups $D_{2n}$. 
        \begin{proof}
            The generators of the group are of the form $s, r, sr, s^{2}r, \ldots, s^{n - 1}r$. We can say that two permutations are related if you can reach them by a number of rotations.
        \end{proof}

    \textbf{Exercise 6}: For every positive integer $n$ construct a group containing two elements $g, h$ such that $\lvert g \rvert = 2$, $\lvert h \rvert = 2$, and $\lvert gh \rvert = n$.
        \begin{proof}
            We can use $D_{2n}$ where $r$ has order $2$ and $sr$ also has order $2$.
        \end{proof}

    \textbf{Exercise 7}: Find all elements of $D_{2n}$ that commute with every other element.
        \begin{proof}
            Notice that the elements that commute with every other element. If $n$ is even, we have that $s^{\frac{n}{2}}$ commutes with all other elements:
                \begin{align*}
                    s^{\frac{n}{2}}rs^{j} &= s^{\frac{n}{2}}s^{-j}r = s^{\frac{n}{2} - j}r \\
                    rs^{j}s^{\frac{n}{2}} &= rs^{\frac{n}{2} + j} = s^{\frac{n}{2} - j}r     
                \end{align*}
            We also have that $rs^{\frac{n}{2}}$ commutes with every element. There are no other elements that commute with every other element. Notice that this only works when $n$ is even.
        \end{proof}

    \textbf{Exercise 8}: Find the orders of the groups of symmetries of the five `platonic solids'.
        \begin{proof}
            The order of the symmetries group on the tetrahedron is the number of unique morphisms that are generated by those that fix one element and rotate the rest. So it is generated by 
                \begin{equation*}
                    \{(1 \, 2 \, 3), (1 \, 2 \, 4), (1 \, 3 \, 4), (2 \, 3 \, 4)\}
                \end{equation*}
            The order of this group would be 12. For the cube, we have rotation across the three axes, along with reflection across three planes. So the group of symmetries is generated by the set:
                \begin{equation*}
                    \longset{S}{(1 \, 2 \, 3 \, 4)(5 \, 6 \, 7 \, 8), (1 \, 2 \, 6 \, 5)(3 \, 4 \, 8 \, 7), (2 \, 3 \, 7 \, 6)(1 \, 4 \, 8 \, 5), (1 \, 5)(2 \, 6)(3 \, 7)(4 \, 8), (1 \, 2)(5 \, 6)(3 \, 4)(7 \, 8), (2 \, 3)(6 \, 7)(1 \, 4)(5 \, 8)}
                \end{equation*}
            
        \end{proof}

    \textbf{Exercise 9}: Verify carefully that `congruence mod $n$' is an equivalence relation.
        \begin{proof}
            Consider the relation where two elements are related if and only if $a \equiv b \pmod{n}$. We definitely have that $a \sim a$, since $n \divides a - a$. Also, we have that if $n \divides a - b$, then also, $n \divides -(a - b)$ or $n \divides b - a$. Therefore, the relation is symmetric. Finally, for transitivity, if $a \equiv b \pmod{n}$ and $b \equiv c \pmod{n}$, Then we have $n \divides a - b$ and $n \divides b - c$, therefore, $n \divides (a - b) + (b - c) = a - c$. Therefore, the relation is transitive and this means that it is an equivalence relation.
        \end{proof}

    \textbf{Exercise 10}: Prove that $\mathbb{Z} / n\mathbb{Z}$ consists of precisely $n$ elements.
        \begin{proof}
            We have that $\mathbb{Z}/ n\mathbb{Z}$ is the set of equivalence classes of $Z$ based on our equivalence relation. Observe that the number of unique remainders that we can have lies between $0, \ldots, n - 1$. Therefore, we have $n$ elements in $\mathbb{Z}/n\mathbb{Z}$.
        \end{proof}

    \textbf{Exercise 11}: Prove that the square of every odd integer is congruent to $1$ modulo $8$.
        \begin{proof}
            We just have to check the equivalence classes with odd representatives, because mod 8, the square of odd numbers are odd and therefore, odd - even = odd:
                \begin{equation*}
                    \mathbb{Z}/8\mathbb{Z} = \{[0], [1], [2], [3], [4], [5], [6], [7]\}.
                \end{equation*}
            So we have to just square the odd ones:
                \begin{equation*}
                    [1]^{2} = [1], \, [3]^{2} \equiv [1], \, [5]^{2} \equiv [1], \, [7]^{2} \equiv [1]
                \end{equation*}
            which concludes the proof.
        \end{proof}

    \textbf{Exercise 12}: Prove that there are no integers $a, b, c$ such that $a^{2} + b^{2} = 3c^{2}$.
        \begin{proof}
            We can try working in mod 3 so that the RHS cancels to just 0:
                \begin{equation*}
                    \mathbb{Z}/3\mathbb{Z} = \{[0], [1], [2]\}
                \end{equation*}
            So now we square all elements:
                \begin{equation*}
                    \{[0], [1]\}
                \end{equation*}
            So all possible sums of two square remainders are:
                \begin{equation*}
                    \{[0], [1]\}
                \end{equation*}
            This means that both $a, b$ must have either remainder $0$. But this is impossible since the $LHS$ is divisible by an even number of $3$'s but the RHS is divisible by an odd number of $3$'s.
        \end{proof}

    \textbf{Exercise 13}: Prove that if $\gcd(m, n) = 1$, then there exists integers $a$ and $b$ such that 
        \begin{equation*}
            am + bn = 1
        \end{equation*}
        \begin{proof}
            Observe that if $n = 1$, we have that $m = 1$, to which we use $a = 0$, $b = 1$. Now suppose that $n > 1$. Then we have by modding both sides:
                \begin{equation*}
                    am \equiv 1 \pmod{n}
                \end{equation*}
            Now observe that the order of the element $1^{m}$ in $\mathbb{Z}/n\mathbb{Z}$ is just 
                \begin{equation*}
                    \dfrac{\lcm(m, n)}{m} = \dfrac{n}{\gcd(m, n)} = n
                \end{equation*}
            Therefore, $[1]^{ma} = [1]$ for some $a$. So $m$ has an inverse, we call $a$. This concludes the proof.
        \end{proof}

    \textbf{Exercise 14}: State and prove an analog of Lemma 2.2, showing that the multiplication on $\mathbb{Z} / n\mathbb{Z}$ is a well-defined operation.
        \begin{proof}
            Hell no
        \end{proof}

    \textbf{Exercise 15}: Let $n > 0$ be an odd integer.
        \begin{itemize}
            \item Prove that if $\gcd(m, n) = 1$, then $\gcd(2m + n, 2n) = 1$.
                \begin{proof}
                    If we have what is above, due to the properties of the gcd, we have $\gcd(m, n) = 1 \implies \gcd(2m, 2n) = 2$. Since $n$ is odd, we have that $2m + n$ does not have a factor of $2$. Furthermore, the factors of $2m + n$ cannot divide those of $n$ because $\gcd(2m, n) = 1$. Therefore, we have $\gcd(2m + n, 2n) = 1$
                \end{proof}

            \item Prove that if $\gcd(r, 2n) = 1$, then $\gcd(\frac{r + n}{2}, n) = 1$.
                \begin{proof}
                    Leave for Later.
                \end{proof}

            \item Conclude that the function $[m]_{n} \rightarrow [2m + n]_{2n}$ is a bijection between $(\mathbb{Z} / n \mathbb{Z})^{*}$ and $(\mathbb{Z} / 2n\mathbb{Z})^{*}$.
        \end{itemize}
        The number $\varphi(n)$ of elements of $(\mathbb{Z}/ n\mathbb{Z})^{*}$ is \textit{Euler's $\varphi$-function}. The reader has just proved that if $n$ is odd, then $\varPhi(2n) = \varphi(n)$. Much more general formulas will be given later on.

    \textbf{Exercise 16}: Find the last digit of $1238237^{18238456}$ by working in $\mathbb{Z} / 10\mathbb{Z}$.
        \begin{proof}
            Too much work.
        \end{proof}

    \textbf{Exercise 17}: Show that if $m \equiv m^{\prime}$ mod $n$, then $\gcd(m, n) = 1$ if and only if $\gcd(m^{\prime}, n) = 1$.
        \begin{proof}
            This follows trivially from the proof on the last proposition in the section.
        \end{proof}

    \textbf{Exercise 18}: For $d \leq n$, define an injective function $\mathbb{Z}/d\mathbb{Z} \rightarrow S_{n}$ preserving the operation, that is, such that the sum of equivalence classes in $\mathbb{Z}/n\mathbb{Z}$ corresponds to the product of the corresponding permutations.
        \begin{proof}
            Just use an element of order $d$ in $S_{n}$:
                \begin{equation*}
                    (1 \, 2 \, \ldots \, d)
                \end{equation*}
            This is injective, we just name this element $g$ and we do the mapping such that:
                \begin{equation*}
                    [i] \mapsto g^{i}
                \end{equation*}
        \end{proof}

    \textbf{Exercise 19}: Both $(\mathbb{Z}/5\mathbb{Z})^{*}$ and $(\mathbb{Z}/12\mathbb{Z})^{*}$ consist of 4 elements. Write their multiplication tables, and prove that no re-ordering of the elements will make them match.
        \begin{proof}
            The elements of both groups are:
                \begin{equation*}
                    (\mathbb{Z}/5\mathbb{Z})^{*} = \{[1], [2], [3], [4]\} \text{ and } (\mathbb{Z}/12\mathbb{Z})^{*} = \{[1], [5], [7], [11]\}
                \end{equation*}
            and multiplication tables:
                \begin{align*}
                    \begin{array}{ c | c c c c }
                            & {[1]} & {[2]} & {[3]} & {[4]} \\
                    \hline
                        {[1]} & {[1]} & {[2]} & {[3]} & {[4]} \\
                        {[2]} & {[2]} & {[4]} & {[1]} & {[3]} \\
                        {[3]} & {[3]} & {[1]} & {[4]} & {[2]} \\
                        {[4]} & {[4]} & {[3]} & {[2]} & {[1]}   
                    \end{array}
                        \hspace{30pt} 
                    \begin{array}{ c | c c c c }
                             & {[1]}  & {[5]}  & {[7]}  & {[11]} \\
                    \hline
                        {[1]}  & {[1]}  & {[5]}  & {[7]}  & {[11]} \\
                        {[5]}  & {[5]}  & {[1]}  & {[11]} & {[7]}  \\
                        {[7]}  & {[7]}  & {[11]} & {[1]}  & {[5]}  \\
                        {[11]} & {[11]} & {[7]}  & {[5]}  & {[1]}   
                    \end{array}
                \end{align*}
            All elements in $(\mathbb{Z}/12\mathbb{Z})^{*}$ are of order 2. This is not true for the elements of $(\mathbb{Z}/5\mathbb{Z})^{*}$
        \end{proof}
\end{exercises}

\begin{topic}
    \section{The Category Grp}
\end{topic}

A group has two types of information. One is the set and the other, the binary operation:
    \begin{equation*}
        m_{G} : G \times G \rightarrow G
    \end{equation*}
satisfying certain properties. For two groups, a group homomorphism is:
    \begin{equation*}
        \varphi : (G, m_{G}) \rightarrow (H, m_{H})
    \end{equation*}
which is a function between sets. But we must incorporate the information of the binary operation in this somehow. We also have that the function $\varphi$ determines:
    \begin{equation*}
        (\varphi \times \varphi) : G \times G \rightarrow H \times H
    \end{equation*}
and using the universal property of products. Or we could just define the function using what we already know about sets:
    \begin{equation*}
        (\forall (a, b) \in G \times G): (\varphi \times \varphi)(a, b) = (\varphi(a), \varphi(b))
    \end{equation*}
with respect to the group operations $m_{G}, m_{H}$ on the groups $G, H$, we can define a diagram that incorporates all this information:
    \begin{center}
        \begin{tikzcd}
            G \times G\ar[r, "\varphi \times \varphi"]\ar[d, "m_{G}"] & H \times H\ar[d, "m_{H}"] \\
            G\ar[r, "\varphi"']                                       & H                           
        \end{tikzcd}
    \end{center}
a very natural requirement would make this diagram commute.

\begin{definition}[\label{def:2.3.1}]{Group Homomorphism}
    A function $\varphi : G \rightarrow H$ is a group homomorphism if the diagram above commutes. This reveals a simple property. What commutativity means is that we can travel the diagram in equal ways:
        \begin{center}
            \begin{tikzcd}
                (a, b)\ar[r, "", dashed, no head]\ar[d, "", maps to] & {}\ar[d, "", dashed, no head] \\
                a \cdot b \ar[r, "", maps to]                        & \varphi(a \cdot b)   
            \end{tikzcd}
                \hspace{30pt} 
            \begin{tikzcd}
                (a, b)\ar[r, "", maps to]\ar[d, "", dashed, no head] & (\varphi(a), \varphi(b))\ar[d, "", maps to] \\
                {}\ar[r, "", dashed, no head]                        & \varphi(a) \cdot \varphi(b)                   
            \end{tikzcd}
        \end{center}
    So we have that the order of operations does not matter where we can take $\varphi$ first then do the binary operation or take the binary operation then the $\varphi$. That is to say:
        \begin{equation*}
            \varphi(ab) = \varphi(a)\varphi(b)
        \end{equation*}
    which means that $\varphi$ `preserves the structure'.
\end{definition}

\begin{definition}[\label{def:2.3.2}]{Homomorphisms of Grp}
    For $G$, $H$ groups, we define 
        \begin{equation*}
            \text{Hom}_{\text{Grp}}(G, H)
        \end{equation*}
    to be the set of group homomorphisms $G \rightarrow H$. If $G, H, K$ are groups and $\varphi: G \rightarrow H$, $\psi : H \rightarrow K$ are group homomorphisms, we have that the composition is a group homomorphism with $\psi \circ \varphi: G \rightarrow K$:
    \begin{center}
        \begin{tikzcd}
            G \times G \ar[r, "\varphi \times \varphi"']\ar[rr, "(\psi \circ \varphi) \times (\psi \circ \varphi)", bend left = 20]\ar[d, "m_{G}"] & H \times H\ar[r, "\psi \times \varphi"']\ar[d, "m_{H}"'] & K \times K\ar[d, "m_{K}"] \\
            G \ar[r, "\varphi"]\ar[rr, "\psi \circ \varphi"', bend right = 20]                                                                     & H\ar[r, "\psi"]                                          & K                           
        \end{tikzcd}
    \end{center}
\end{definition}

We check that the outer rectangles commute:
    \begin{align*}
        (\psi \circ \varphi)(a \cdot b) &= \psi(\varphi(a \cdot b)) = \psi(\varphi(a) \cdot \varphi(b)) = \psi(\varphi(a)) \cdot \psi(\varphi(b)) \\
                                        &= (\psi \circ \varphi)(a) \cdot (\psi \circ \varphi)(b)                             
    \end{align*}
Since $\text{id}_{G} : G \rightarrow G$ is a group homomorphism, we have that $\text{Grp}$ is a category.

One additional thing to consider is the fact that a group keeps track of the existence of inverses and the identity element. This means that the morphisms of $\text{Grp}$ should keep track of this data. This is automatic in what we found out about the structure of a group homomorphism however:

\textbf{Proposition 2.3.2}: Let $\varphi: G \rightarrow H$ be a \textit{group homomorphism}. Then
    \begin{itemize}
        \item $\varphi(e_{G}) = e_{H}$;

        \item $\forall g \in G$, $\varphi(g^{-1}) = \varphi(g)^{-1}$
    \end{itemize}
In referencing a diagram, we get:
    \begin{center}
        \begin{tikzcd}
            G\ar[r, "\varphi"]\ar[d, "\text{id}_{G}"'] & H\ar[d, "\text{id}_{H}"] \\
            G\ar[r, "\varphi"']                        & H                          
        \end{tikzcd}
    \end{center}
must commute.
    \begin{proof}
        (Part I) Since $e_{H}e_{H} = e_{H}$:
            \begin{equation*}
                e_{H} \cdot \varphi(e_{G}) = \varphi(e_{G}) = \varphi(e_{G} \cdot e_{G}) = \varphi(e_{G}) \cdot \varphi(e_{G})
            \end{equation*}
        which means that $e_{G} = \varphi(e_{G})$.

        (Part II) For the second part, we have:
            \begin{equation*}
                \varphi(g^{-1}) \cdot \varphi(g) = \varphi(g^{-1} \cdot g) = \varphi(e_{G}) = e_{H} = \varphi(g)^{-1} \cdot \varphi(g)
            \end{equation*}
        which shows that $\varphi(g^{-1}) = \varphi(g)^{-1}$.
    \end{proof}
The categories $\text{Grp}$ and $\text{Set}$ look very similar, but there is the additional information about the binary operation in a group. Another difference is that $\text{Set}$ has a unique initial object $\emptyset$ which are not the same as final objects which are the singletons. 

\textbf{Proposition 2.3.3}: \textit{Trivial groups} are both initial and final in $\text{Grp}$. This means that trivial groups are zero objects of the category $\text{Grp}$.

\begin{proof}
    The trivial groups are final object because they contain one element and by the same reasoning as made in $\text{Set}$. To show that they are initial, we note that the homomorphisms of $\text{Grp}$ are group homomorphisms and that the identity must map to the identity. Therefore, this is the trivial map also. So the trivial group is a zero object.
\end{proof}

$\text{Grp}$ also has products and the product of two groups $G$, $H$ would be the based on $G \times H$. We would define multiplication component-wise $\forall g_{1}, g_{2} \in G$, $\forall h_{1}, h_{2} \in H$:
    \begin{equation*}
        (g_{1}, h_{1}) \cdot (g_{2}, h_{2}) := (g_{1}, g_{2}, h_{1}, h_{2})
    \end{equation*}
This defines a group structure on $G \times H$. The group $G \times H$ is called the \textit{direct product} of the groups $G$ and $H$. We also have the projections as group homomorphisms:
    \begin{center}
        \begin{tikzcd}
              & G \times H \ar[dr, "\pi_{H}"']\ar[dl, "\pi_{G}"] &   \\
            G &                                                  & H   
        \end{tikzcd}
    \end{center}

\textbf{Proposition 2.3.4}: With the operation defined component-wise, $G \times H$ is a product in $\text{Grp}$.

\begin{proof}
    To be a product means to solve the universal problem that for any $\varphi_{G}: A \rightarrow G$, $\varphi_{H} : A \rightarrow H$, there is a unique homomorphism $\varphi_{G} \times \varphi_{H}$ which makes:
        \begin{center}
            \begin{tikzcd}
                                                                                                                                          &                                                     & G \\
                A\ar[urr, "\varphi_{G}", bend left = 20]\ar[r, "\varphi_{G} \times \varphi_{H}"]\ar[drr, "\varphi_{H}"', bend right = 20] & G \times H\ar[ur, "\pi_{G}"]\ar[dr, "\varphi_{H}"'] &   \\
                                                                                                                                          &                                                     & H   
            \end{tikzcd}
        \end{center}
    commute. We know that it commutes because it is the same as the product of $G, H$ in $\text{Set}$. Now we check that $\varphi_{G} \times \varphi_{H}$ is a group homomorphism and exists in $\text{Grp}$:
        \begin{align*}
            \varphi_{G} \times \varphi_{H}(ab) &= (\varphi_{G}(ab), \varphi_{H}(ab)) = (\varphi_{G}(a)\varphi_{G}(b), \varphi_{H}(a)\varphi_{H}(b))                                         \\
                                               &= (\varphi_{G}(a), \varphi_{H}(a))(\varphi_{G}(b), \varphi_{H}(b)) = (\varphi_{G} \times \varphi_{H}(a))(\varphi_{G} \times \varphi_{H}(b))   
        \end{align*}
\end{proof}

Coproducts also exist in $\text{Grp}$, but their construction will be more challenging to see. These will show up in Exercise 3.8, 5.6, 5.7. Free groups are cases of coproducts. For now, it is sufficient to know that coproducts can be constructed with the disjoint union in $\text{Set}$, which we cannot do in $\text{Grp}$ because there is no group structure we have for disjoint union. The coproduct of $G$ and $H$ is called the free product of $G$ and $H$ which is $G * H$.

The category $\text{Ab}$ have objects as abelian groups with morphisms as group homomorphisms. These will be more important than the ones in $\text{Grp}$. $\text{Ab}$ is also a nicer category than $\text{Grp}$. Interestingly, the products in $Ab$ are also the coproducts. When working with coproducts, the product $G \times H$ of abelian groups is often called the direct sum or denoted as $G \oplus H$.

Notice that even if $G$ and $H$ are commutative, they may not solve the universal problem that makes $G \times H$ into a coproduct.

\begin{exercises}{Problem Sets}
    \textbf{Exercise 1}: Let $\varphi: G \rightarrow H$ be a morphism in a category $C$ with products. Explain why there is a unique morphism
        \begin{equation*}
            (\varphi \times \varphi) : G \times G \rightarrow H \times H.
        \end{equation*}
    (This morphism is defined explicitly for $C = \text{Set}$ in 3.1)

    \textbf{Exercise 2}: Let $\varphi: G \rightarrow H$, $\psi : H \rightarrow K$ be morphisms in a category with products, and consider morphisms between the product $G \times G$, $H \times H$, $K \times K$ as in Exercise 3.1. Prove that
        \begin{equation*}
            (\psi\varphi) \times (\psi\varphi) = (\psi \times \psi)(\varphi \times \varphi)
        \end{equation*}
    (This is part of the commutativity of the diagram displayed in 3.2).
        \begin{proof}
            From the commutative diagram, we get that:
                \begin{center}
                    \begin{tikzcd}
                        G \times G\ar[r, "\varphi \times \varphi"']\ar[rr, "(\psi \circ \varphi) \times (\psi \circ \varphi)", bend left = 20] & H \times H \ar[r, "\psi \times \varphi"'] & K \times K   
                    \end{tikzcd}
                \end{center}
            The top arrow is equal to the composition of the both two arrows. This concludes the proof.
        \end{proof}

    \textbf{Exercise 3}: Show that if $G$, $H$ are abelian groups, then $G \times H$ satisfies the universal property for coproducts in $\text{Ab}$.
        \begin{proof}
            We start by supposing that there is a problem to solve, the mapping from two groups to a group by a homomorphism:
                \begin{center}
                    \begin{tikzcd}
                        G\ar[dr, "\varphi_{1}"]      &   \\
                                                     & Z \\
                        H\ar[ur, "\varphi_{2}"']     &     
                    \end{tikzcd}
                \end{center}
            Now if we take the product, we can take two sensible mappings to the product to be the projection mappings:
                \begin{center}
                    \begin{tikzcd}
                        G \ar[dr, "\pi_{G}"]\ar[drr, "\varphi_{1}", bend left = 20]    &                             &   \\
                                                                                       & G \times H \ar[r, "\sigma"] & Z \\
                        H \ar[ur, "\pi_{H}"']\ar[urr, "\varphi_{2}"', bend right = 20] &                             &     
                    \end{tikzcd}
                \end{center}
            To find $\sigma$, we observe that it has to be a group homomorphism. So we observe what the action of $\varphi_{1}, \varphi_{2}$ does on two elements from both $G$ and $H$:
                \begin{align*}
                    \pi_{H}(h_{1})     &= (e, h_{1}) \\
                    \varphi_{2}(h_{1}) &= z_{1}        
                \end{align*}
            This means that we have $(e, h_{1}) \mapsto \varphi_{2}(h_{1})$ for all $h \in H$. We denote this as $\sigma: \{(e, h): h \in H\} \rightarrow Z$ to be $\sigma(e, h) := \varphi_{2}(e)\varphi_{2}(h) = \varphi_{2}(h)$. Likewise, we show the same for $\varphi_{1}$ on elements in $G$. Therefore, we get the forced mapping:
                \begin{equation*}
                    \sigma: G \times H \rightarrow Z \hspace{30pt} \sigma(g, h) := \varphi_{1}(g)\varphi_{2}(h)
                \end{equation*}
            Is this a homomorphism? Suppose that we have elements of $G \times H$ called $(g_{1}, h_{1}), (g_{2}, h_{2})$, which map to $z_{1}, z_{2}$ respectively. Then we have:
                \begin{align*}
                    \sigma((g_{1}, h_{1})(g_{2}, h_{2})) &= \sigma((g_{1}g_{2}, h_{1}h_{2}))                                         \\
                                                         &= \varphi_{1}(g_{1}g_{2})\varphi_{2}(h_{1}h_{2})                           \\
                                                         &= \varphi_{1}(g_{1})\varphi_{1}(g_{2})\varphi_{2}(h_{1})\varphi_{2}(h_{2})
                \end{align*}
            But since $Z$ is commutative, $\varphi_{1}(g_{2})$ and $\varphi_{2}(h_{1})$ commute! So:
                \begin{align*}
                    \sigma((g_{1}, h_{1})(g_{2}, h_{2})) &= \varphi_{1}(g_{1})\varphi_{2}(h_{1})\varphi_{1}(g_{2})\varphi_{2}(h_{2}) \\
                                                         &= \sigma((g_{1}, h_{1}))\sigma((g_{2}, h_{2}))                               
                \end{align*}
        \end{proof}

    \textbf{Exercise 4}: Let $G$, $H$ be groups, and assume that $G \cong H \times G$. Can you concludes that $H$ is trivial?
        \begin{proof}
            Later.
        \end{proof}

    \textbf{Exercise 5}: Prove that $\mathbb{Q}$ is not the direct product of two nontrivial groups.

    \textbf{Exercise 6}: Consider the product of the cyclic groups $C_{2}, C_{3} : C_{2} \times C_{3}$. By Exercise 3.3, this group is a coproduct of $C_{2}$ and $C_{3}$ in $\text{Ab}$. Show that it is \textit{not} a coproduct of $C_{2}$ and $C_{3}$ in $\text{Grp}$ as follows:
        \begin{itemize}
            \item find injective homomorphisms $C_{2} \rightarrow S_{3}, C_{3} \rightarrow S_{3}$:
                \begin{proof}
                    We have the inclusion maps from $C_{2}$ to $S_{3}$ and $C_{3}$ to $S_{3}$:
                        \begin{align*}
                            \iota_{C_{2}} &: C_{2} \rightarrow S_{3} \\
                            \iota_{C_{3}} &: C_{3} \rightarrow S_{3}
                        \end{align*}
                \end{proof}

            \item arguing by contradiction, assume that $C_{2} \times C_{3}$ is a coproduct of $C_{2}, C_{3}$ and deduce that there would be a group homomorphism $C_{2} \times C_{3} \rightarrow S_{3}$ with certain properties;
                \begin{proof}
                    Now suppose that $C_{2} \times C_{3}$ is a coproduct of $S_{3}$ and we have a group homomorphism $\sigma : C_{2} \times C_{3} \rightarrow S_{3}$. Note that the homomorphism is from a product of abelian groups. Therefore, the product $C_{2} \times C_{3}$ is abelian also. Now note that we have that $(1 \, 2) \mapsto (1 \, 2)$ and $(1 \, 2 \, 3) \mapsto (1 \, 2 \, 3)$. But since the elements of $C_{2} \times C_{3}$ commute, we must have that $(1 \, 2)(1 \, 2 \, 3) = (1 \, 2 \, 3)(1 \, 2)$ which is false.
                \end{proof}

            \item show that there is no such homomorphism.

        \end{itemize}

    \textbf{Exercise 7}: Show that there is a \textit{surjective} homomorphism $\mathbb{Z} * \mathbb{Z} \rightarrow C_{2} * C_{3}$ (* denotes coproduct in $\text{Grp}$)
        \begin{proof}
            We can draw two commutative diagrams to see the relationship between these objects:
                \begin{center}
                    \begin{tikzcd}
                        C_{2} \ar[rr, "m_{1}"]\ar[dr, "\pi_{1}"]   &                              & Z \ar[dr, "\rho_{1}"]  &       \\
                                                                   & C_{2} * C_{3}\ar[rr, "\tau"] &                                & Z * Z \\
                        C_{3} \ar[ur, "\pi_{2}"']\ar[rr, "m_{2}"'] &                              & Z \ar[ur, "\rho_{2}"'] &         
                    \end{tikzcd} \\
                    \begin{tikzcd}
                        Z \ar[dr, "\rho_{1}"]\ar[rr, "m^{\prime}_{1}"]   &                         & C_{2}\ar[dr, "\pi_{1}"]  &               \\
                                                                         & Z * Z \ar[rr, "\sigma"] &                          & C_{2} * C_{3} \\
                        Z \ar[ur, "\rho_{2}"']\ar[rr, "m^{\prime}_{2}"'] &                         & C_{3}\ar[ur, "\pi_{2}"'] &                 
                    \end{tikzcd}
                \end{center}
            Notice that $\sigma$ has a right inverse however which can be seen by putting the diagrams together:
                \begin{center}
                    \begin{tikzcd}
                        C_{2} \ar[rr, "m_{1}"]\ar[dr, "\pi_{1}"]   &                               & Z \ar[dr, "\rho_{1}"]\ar[rr, "m^{\prime}_{1}"]   &                         & C_{2} \ar[dr, "\pi_{1}"]  &               \\
                                                                   & C_{2} * C_{3} \ar[rr, "\tau"] &                                                  & Z * Z \ar[rr, "\sigma"] &                           & C_{2} * C_{3} \\
                        C_{3} \ar[ur, "\pi_{2}"']\ar[rr, "m_{2}"'] &                               & Z \ar[ur, "\rho_{2}"']\ar[rr, "m^{\prime}_{2}"'] &                         & C_{3} \ar[ur, "\pi_{2}"'] &                 
                    \end{tikzcd}
                \end{center}
            Notice that there is a surjective mapping for $m^{\prime}_{1}, m^{\prime}_{2}$ because $\lvert \mathbb{Z} \rvert > \lvert C_{3} \rvert > \lvert C_{2} \rvert$. So there is a right inverse, $m_{1}, m_{2}$ for the morphisms respectively with respect to the commutative diagram above. But if we traverse the mappings, we get $\sigma\tau\pi_{1} = \pi_{1}m^{\prime}_{1}m_{1}$ and $\sigma\tau\pi_{2} = \pi_{2}m^{\prime}_{2}m_{2}$. Or simply, $\sigma\tau\pi_{1} = \pi_{1}$ and $\sigma\tau\pi_{2} = \pi_{2}$. But because $\sigma$ and $\tau$ are unique, $\sigma\tau$ is the unique morphism such that the diagram commutes. But we know that the morphism from an initial object to itself is the identity. So that means that $\sigma\tau$ is the identity and that $\tau$ is surjective. To clarify, we have:
                \begin{center}
                    \begin{tikzcd}
                        C_{2} \ar[dr, "\pi_{1}"]\ar[drr, "\pi_{1}", bend left = 20]    &                                   &               \\
                                                                                       & C_{2} * C_{3} \ar[r, "\text{id}", "\sigma\tau"'] & C_{2} * C_{3} \\
                        C_{3} \ar[ur, "\pi_{2}"']\ar[urr, "\pi_{2}"', bend right = 20] &                                   &                 
                    \end{tikzcd}
                \end{center}
        \end{proof}

    \textbf{Exercise 8}: Define a group $G$ with two generators $x, y$, subject (only) to the relations $x^{2} = e_{G}, y^{3} = e_{G}$. Prove that $G$ is a co-product of $C_{2}$ and $C_{3}$ in $\text{Grp}$. (The reader will obtain an even more concrete description for $C_{2} * C_{3}$ in Exercise 9.14; it is called the modular group)

    \textbf{Exercise 9}: Show that \textit{fiber} products and coproducts exist in $\text{Ab}$.
\end{exercises}

\begin{topic}
    \section{Group Homomorphisms}
\end{topic}

\begin{examples}
    \begin{example}[\label{exm:2.4.1}]
        For any two groups $G, H$, the set $\text{Hom}_{\text{Grp}}(G, H)$ is not empty. We define a homomorphism by sending all elements in the group $G$ to the identity. Therefore, $\text{Hom}_{\text{Grp}}(G, H)$ is a pointed set.

        Notice that $\text{Grp}$ has zero-objects which are both final and initial objects $\{*\}$, so there are unique morphisms:
            \begin{equation*}
                G \rightarrow \{*\}, \, \{*\} \rightarrow H
            \end{equation*}
        The composition of these mappings is the identity element of $\text{Hom}_{\text{Grp}}(G, H)$.
    \end{example}
\end{examples}

Recall the example of the homomorphism $D_{6} \rightarrow S_{3}$ which we defined as a group homomorphism. Other examples will consider group actions, which is the action from a group $G$ to an object $A$ of a category $C$:
    \begin{equation*}
        G \rightarrow \text{Aut}_{C}(A);
    \end{equation*}
For example, if the category is $C$ = Set, then the group actions will determine the permutations of the sets in $C$. One example is the set of vertices and the action of $D_{6}$ on them.

Another example is the exponential function from $(\mathbb{R}, +)$ to the group $(\mathbb{R}^{> 0}, \cdot )$ where we have $e^{a + b} = e^{a}e^{b}$. Another example is where we let $G$ be any group and $g \in G$ be the element of $G$. We have the exponential map as $\varepsilon_{g} : \mathbb{Z} \rightarrow G$
    \begin{equation*}
        (\forall a \in \mathbb{Z}) : \varepsilon_{g}(a) := g^{a}
    \end{equation*}
We note that this is a group homomorphism and that $g$ generates $G$ if and only if the mapping is surjective.

Another example of a homomorphism are the quotient functions:
    \begin{equation*}
        a \mapsto a \cdot [1]_{n} = [a]_{n}:
    \end{equation*}
with the notation introduced above, this is $\varepsilon_{[1]_{n}}$. The function is surjective so $[a]_{n}$ generates $\mathbb{Z}/n\mathbb{Z}$. We already noted that $[m]_{n}$ generates $\mathbb{Z}/n\mathbb{Z}$ if and only if $\gcd(m, n) = 1$.

If $m \divides n$, there is a homomorphism
    \begin{equation*}
        \pi^{n}_{m} : \mathbb{Z}/n\mathbb{Z} \rightarrow \mathbb{Z}/m\mathbb{Z}
    \end{equation*}
by making the diagram
    \begin{center}
        \begin{tikzcd}
            \mathbb{Z}\ar[d, "\pi_{n}"']\ar[dr, "\pi_{m}"] &                        \\
            \mathbb{Z}/n\mathbb{Z}\ar[r, "\pi^{n}_{m}"']   & \mathbb{Z}/m\mathbb{Z}   
        \end{tikzcd}
    \end{center}
commute or basically:
    \begin{equation*}
        \pi^{n}_{m}([a]_{n}) = [a]_{m}
    \end{equation*}
It should be checked that this is well-defined. Furthermore, if $m_{1}$ and $m_{2}$ are divisors of $n$, then there are homomorphisms $\pi_{m_{1}}^{n}$ and $\pi_{m_{2}}^{n}$ from $\mathbb{Z}/n\mathbb{Z}$ to $\mathbb{Z}/m_{1}\mathbb{Z}$ and $\mathbb{Z}/m_{2}\mathbb{Z}$ and therefore to their direct product. For instance, we have the homomorphisms:
    \begin{equation*}
        \mathbb{Z}/6\mathbb{Z} \rightarrow \mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/3\mathbb{Z}
    \end{equation*}
which we have by:
    \begin{align*}
        \begin{array}{ c c c }
            {[0]}_{6} \mapsto ({[0]}_{2}, {[0]}_{3}), & {[1]}_{6} \mapsto ({[1]}_{2}, {[1]}_{3}), & {[2]}_{6} \mapsto ({[0]}_{2}, {[2]}_{3}), \\
            {[3]}_{6} \mapsto ({[1]}_{2}, {[0]}_{3}), & {[4]}_{6} \mapsto ({[0]}_{2}, {[1]}_{3}), & {[5]}_{6} \mapsto ({[1]}_{2}, {[2]}_{3})    
        \end{array}
    \end{align*}
This is an isomorphism and we see that the $C_{6} \cong C_{2} \times C_{3}$. We can define a homomorphism $\mathbb{Z}/n\mathbb{Z} \rightarrow \mathbb{Z}/m\mathbb{Z}$ if $n \divides m$. But what about when $m \ndivides n$? Try to deduce a proof before it is shown.
    \begin{proof}
        The only homomorphism from $\mathbb{Z}/n\mathbb{Z} \rightarrow \mathbb{Z}/m\mathbb{Z}$ is the trivial homomorphism. Observe that all homomorphisms are of the form $\varphi : n \mapsto an$ for some $0 \leq a \leq m$. This is because all morphisms are determined by what the generator maps to. Then we must have that $n \mapsto an$ and $0 \mapsto 0$. But observe that $an \in \mathbb{Z}/m\mathbb{Z}$ is not 0. This is because $n \ndivides m$ so $an \neq m$ for any $a \in \mathbb{Z}$.
    \end{proof}
Considering that group homomorphisms must preserve the identity and the order, it must be that elements of finite order are sent to those of finite order also. So we have that
    \begin{equation*}
        \varphi(g)^{n} = \varphi(g^{n}) = \varphi(e_{G}) = e_{H}
    \end{equation*}
which gives us a more precise statement:

\textbf{Proposition 2.4.1}: Let $\varphi: G \rightarrow H$ be a group homomorphism, and let $g \in G$ be an element of finite order. Then $\lvert \varphi(g) \rvert$ divides $\lvert g \rvert$.
    \begin{proof}
        We observed that $\varphi(g)^{\lvert g \rvert} = e_{H}$. So we know that the order of $\varphi(g)$ divides the exponent.
    \end{proof}

\begin{examples}
    \begin{example}[\label{exm:2.4.2}]
        There are no nontrivial homomorphisms $\mathbb{Z}/n\mathbb{Z} \rightarrow \mathbb{Z}$. The image of every element of $\mathbb{Z}/n\mathbb{Z}$ must have finite order and the only element of finite order in $(\mathbb{Z}, +)$ is 0.

        There are also no nontrivial homomorphisms $\varphi: C_{4} \rightarrow C_{7}$. The orders of the elements in $C_{4}$ divide $4$ and the orders of the elements in $C_{7}$ divide $7$. Therefore, the order of each $\varphi(g)$ divides both 4 and 7 which means that $\lvert \varphi(g) \rvert = 1$.

        The order is not preserved in general such as $1 \in \mathbb{Z}$ while $[1]_{n} \in \mathbb{Z}/n\mathbb{Z}$ has finite order. Order is, however, preserved through isomorphism.
    \end{example}
\end{examples}

An isomorphism of groups $\varphi: G \rightarrow H$ is an isomorphism in $\text{Grp}$ which is a group homomorphism with an inverse:
    \begin{equation*}
        \varphi^{-1} : H \rightarrow G
    \end{equation*}
which is a group homomorphism. If we look at $\text{Set}$, if a homomorphism is an isomorphism, then it must be a bijection between the sets. So the converse holds:

\textbf{Proposition 2.4.3}: Let $\varphi : G \rightarrow H$ be a group homomorphism. Then $\varphi$ is an isomorphism of groups if and only if it is a bijection.
    \begin{proof}
        The first implication is immediate. Now suppose that $\varphi : G \rightarrow H$ is a bijective homomorphism. Then since it is a bijection, it has an inverse $\varphi$ in $\text{Set}$.
            \begin{equation*}
                \varphi^{-1} : H \rightarrow G
            \end{equation*}
        is this a group homomorphism? Suppose that $h_{1}, h_{2}$ are elements of $H$, then we have $g_{1} = \varphi^{-1}(h_{1}), g_{2} = \varphi^{-1}(h_{2})$ as the elements of $G$. Now we check:
            \begin{equation*}
                \varphi^{-1}(h_{1}h_{2}) = \varphi^{-1}(\varphi(g_{1})\varphi(g_{2})) = \varphi^{-1}(\varphi(g_{1}g_{2})) = g_{1}g_{2} = \varphi^{-1}(h_{1}) \varphi^{-1}(h_{2})
            \end{equation*}
        as desired.
    \end{proof}

\begin{examples}
    \begin{example}[\label{exm:2.4.4}]
        The function $D_{6} \rightarrow S_{3}$ is an isomorphism of groups since it is a bijective homomorphism. The exponential function $(\mathbb{R}, +) \rightarrow (\mathbb{R}^{> 0}, \cdot )$ is also an isomorphism. If the exponential function $\varepsilon_{g} : \mathbb{Z} \rightarrow G$ is determined by an element $g \in G$ is an isomorphism, we say that $G$ is an infinite cyclic group.

        The function $\pi^{6}_{2} \times \pi^{6}_{3} : C_{6} \rightarrow C_{2} \times C_{3}$ is an isomorphism.
    \end{example}
\end{examples}

\begin{definition}[\label{def:2.4.5}]{Isomorphic Groups}
    Two groups $G, H$ are isomorphic if they are isomorphic in $\text{Grp}$ or in other words, if there is a bijective group homomorphism $G \rightarrow H$.
\end{definition}

We note that the isomorphic condition is an equivalence relation. We write that $G \cong H$ if $G$ and $H$ are isomorphic. Automorphisms of a group $G$ are isomorphisms $G \rightarrow G$. These form a group $\text{Aut}_{\text{Grp}}(G)$ or $\text{Aut}(G)$.

\begin{examples}
    \begin{example}[\label{exm:2.4.6}]
        The concept of isomorphisms allow us to give a formal definition of cyclic groups that we last discussed in \Fref{def:2.2.3}.
    \end{example}
\end{examples}

\begin{definition}[\label{def:2.4.7}]{Cyclic Group}
    A group $G$ is cyclic if it is isomorphic to $\mathbb{Z}$ or to $C_{n} = \mathbb{Z}/n\mathbb{Z}$ for some $n$.
\end{definition}

We have that $C_{2} \times C_{3}$ is cyclic because it is isomorphic to $C_{6}$. In general $C_{m} \times C_{n}$ is cyclic if $\gcd(m, n) = 1$.

We know that $D_{6}$ and $S_{3}$ are isomorphic, but what about $C_{6}$ and $S_{3}$? We will look at this soon.

We can also see that if $p$ is prime, the group $((\mathbb{Z}/p\mathbb{Z})^{*}, \cdot )$ is cyclic. This is a deep fact and we see that $(\mathbb{Z}/12\mathbb{Z})^{*}$ is not cyclic. What it means to be cyclic is that there is an $a$ where every non-multiple of $p$ is congruent to a power of $a$. The usual proofs are not constructive. There is also a connection between the order of an element of a cyclic group $(\mathbb{Z}/p\mathbb{Z})^{*}$ which are called `cyclotomic polynomials' which will be introduced after some field theory.

Isomorphic objects should be indistinguishable in a category so they share the same group structure.

\textbf{Proposition 2.4.8}: Let $\varphi : G \rightarrow H$ be an isomorphism.
    \begin{itemize}
        \item $(\forall g \in G) : \lvert \varphi(g) \rvert = \lvert g \rvert$;

        \item $G$ is commutative if and only if $H$ is commutative. 
    \end{itemize}
    \begin{proof}
        (Part I) The first part we get from the fact that the order of $\varphi(g)$ divides that of $g$. If we take the inverse, the order of $g$ which is $\varphi^{-1}(\varphi(g))$ divides that of $\varphi(g)$. So the orders are equal.

        (Part II) The second part is left to the reader.
    \end{proof}

\begin{examples}
    \begin{example}[\label{exm:2.4.9}]
        $C_{6} \not\cong S_{3}$ since one is commutative and the other is not. We can also say that the number of distinct orders in $C_{6}$ does not match that of $S_{3}$, so the groups are not isomorphic.

        \textit{Note}: Two finite commutative groups are isomorphic if and only if they have the same number of elements of any given order. This will be proved in the next chapter. The general statement for non-commutative groups is not true.
    \end{example}
\end{examples}

We will look at homomorphisms in abelian groups because in general, those in $\text{Ab}$ are more well-behaved. We note that $\text{Hom}_{\text{Grp}}(G, H)$ is a pointed set for any two groups $G, H$, but in $\text{Ab}$, $\text{Hom}_{\text{Ab}}(G, H)$ is a group.

The operation in $\text{Hom}_{\text{Ab}}(G, H)$ is inherited from the operation in $H$ and if $\varphi, \psi: G \rightarrow H$ are group homomorphisms, let $\varphi + \psi$ be the function:
    \begin{equation*}
        (\forall a \in G) : (\varphi + \psi)(a) := \varphi(a) + \psi(a).
    \end{equation*}
Would $\varphi + \psi$ be a group homomorphism? For all $a, b \in G$:
    \begin{align*}
        (\varphi + \psi)(a + b) &= \varphi(a + b) + \psi(a + b) = (\varphi(a) + \varphi(b)) + (\psi(a) + \psi(b)))              \\
                                &= (\varphi(a) + \psi(a)) + (\varphi(b) + \psi(b)) = (\varphi + \psi)(a) + (\varphi + \psi)(b).   
    \end{align*}
The $+$ signs are used to denote commutative operations in this book. The operation makes $\text{Hom}_{\text{Ab}}(G, H)$ into a group. The properties of $+$ we get from a homomorphism such as associativity, identity, and inverses are defined as:
    \begin{equation*}
        (\forall a \in G) : (-\varphi)(a) = -\varphi(a).
    \end{equation*}
Note that these conclusions only require that $H$, the codomain is commutative. We can also say that $H^{A} = \text{Hom}_{\text{Set}}(A, H)$ is a group for all sets $A$, which is a group that will be studied later.

\begin{exercises}{Problem Sets}
    \textbf{Exercise 4.1}: Check that the function $\pi_{m}^{n}$ defined in 4.1 is well-defined and makes the diagram commute. Verify that it is a group homomorphism. Why is the hypothesis $m \divides n$ necessary?
        \begin{proof}
            Suppose that $[a]_{n} = [b]_{n}$. We must show that $\pi_{m}^{n}([a]_{n}) = \pi_{m}^{n}([b]_{n})$. Since $[a]_{n} = [b]_{n}$, we have that $a - b = cn$ for some $c \in \mathbb{Z}$. But since $m \divides n$, we must have that $c^{\prime}m = n$. Therefore, $a - b = cc^{\prime}m$. Therefore, we have as desired.
        \end{proof}

    \textbf{Exercise 4.2}: Show that the homomorphism $\pi_{2}^{4} \times \pi_{2}^{4} : C_{4} \rightarrow C_{2} \times C_{2}$ is not an isomorphism. In fact, is there any nontrivial isomorphism $C_{4} \rightarrow C_{2} \times C_{2}$?
        \begin{proof}
            It is not an isomorphism because it is not injective. We have
                \begin{align*}
                    0 &\mapsto  (0, 0) \\
                    1 &\mapsto  (1, 1) \\
                    2 &\mapsto  (0, 0) \\
                    3 &\mapsto  (1, 1)   
                \end{align*}
            There are no isomorphisms because the number of order 2 and 4 elements in $C_{4}$ do not match with the number of 2 and order 4 elements in $C_{2} \times C_{2}$ respectively.
        \end{proof}

    \textbf{Exercise 4.3}: Prove that a group of order $n$ is isomorphic to $\mathbb{Z}/n\mathbb{Z}$ if and only if it contains an element of order $n$.
        \begin{proof}
            ($\rightarrow $) Suppose that $G$ is a group of order $n$ with an element of order $n$ called $g$. Then we can list out all the elements of $G$ as $g^{k}$ for some $k$. Therefore, the group is cyclic and has order $n$. Therefore, $G \cong \mathbb{Z}/n\mathbb{Z}$.

            ($\leftarrow $) Suppose that $G \cong \mathbb{Z}/n\mathbb{Z}$. Since isomorphisms preserve order, we must have an element of order $n$.
        \end{proof}

    \textbf{Exercise 4.4}: Prove that no two of the groups $(\mathbb{Z}, +)$, $(\mathbb{Q}, +)$, $(\mathbb{R}, +)$ are isomorphic to one another. Can you decide whether $(\mathbb{R}, +)$, $(\mathbb{C}, +)$ are isomorphic to one another?
        \begin{proof}
            Notice that all homomorphisms from $\mathbb{Z}$ to any other group must be of the form 
                \begin{equation*}
                    \varphi := n \mapsto an
                \end{equation*}
            for some $a$ in the codomain. So we must have for $\varphi : \mathbb{Z} \rightarrow \mathbb{Q}$ as: 
                \begin{equation*}
                    0 \mapsto 0 \hspace{30pt} 1 \mapsto a
                \end{equation*}
            But observe that $\frac{a}{2} \notin \Im{\varphi}$. We can show this by induction on the positive integers. So the homomorphism is not surjective, and therefore not bijective. So it is not an isomorphism. We just need to prove that $\mathbb{R} \not\cong \mathbb{Q}$. We do this by noting that $\lvert \mathbb{Z} \rvert = \lvert \mathbb{Q} \rvert$. So we are done. The cardinalities do not match up. We cannot decide whether $(\mathbb{R}, +), (\mathbb{C}, +)$ are isomorphic because we do not know how to compare their cardinalities, nor do we know what a homomorphism would look like.
        \end{proof}

    \textbf{Exercise 4.5}: Prove that the groups $(\mathbb{R} \backslash \{0\}, \cdot)$ and $(\mathbb{C} \backslash \{0\}, \cdot)$ are not isomorphic.
        \begin{proof}
            We have that the only elements of finite order in $\mathbb{R}$ are $1, -1$ which have order 1 and 2 respectively. But in $\mathbb{C} \backslash \{0\}$, there are elements $i, -i$ which both have order 4. Since isomorphism preserves order, the groups are not isomorphic.
        \end{proof}

    \textbf{Exercise 4.6}: We have seen that $(\mathbb{R}, +)$ and $(\mathbb{R}^{> 0}, \cdot )$ are isomorphic in \Fref{exm:2.4.4}. Are there groups $(\mathbb{Q}, +)$ and $(\mathbb{Q}^{> 0}, \cdot )$ isomorphic?
        \begin{proof}
            Yes the groups are isomorphic. We can define a homomorphism $\varphi : q \mapsto x^{q}$, for some variable $x$. This is a group homomorphism which can be checked. Additionally, the map is both injective and surjective, when we take $x$ to be $>$ 0. If $\varphi(q_{1}) = \varphi(q_{2})$, we have $\frac{\varphi(q_{1})}{\varphi(q_{2})} = x^{0}$. Therefore, we have that $q_{1} - q_{2} = 0$ and $q_{1} = q_{2}$. The mapping is surjective as it is essentially an identity function.
        \end{proof}

    \textbf{Exercise 4.7}: Let $G$ be a group. Prove that the function $G \rightarrow G$ defined by $g \mapsto g^{-1}$ is a homomorphism if and only if $G$ is abelian. Prove that $g \mapsto g^{2}$ is a homomorphism if and only if $G$ is abelian.
        \begin{proof}
            (Part I)($\rightarrow $) Suppose that $\varphi : G \rightarrow G$ and that $\varphi := g \mapsto g^{-1}$. Then we have that for arbitrary $g_{1}, g_{2}$:
                \begin{equation*}
                    \varphi(g_{1}g_{2}) = g_{2}^{-1}g_{1}^{-1} = g_{1}^{-1}g_{2}^{-1} = \varphi(g_{1})\varphi(g_{2})
                \end{equation*}
            We can rearrange terms to show that the elements commute:
                \begin{equation*}
                    g_{2}^{-1}g_{1}^{-1} = g_{1}^{-1}g_{2}^{-1} \implies g_{1}g_{2} = g_{2}g_{1}
                \end{equation*}
            ($\leftarrow $) Suppose that $G$ is abelian. Then we can check with two elements that it is a homomorphism using the abelian property. This can be seen in the first part of the proof.

            (Part II)($\rightarrow $) Suppose that $\varphi : G \rightarrow G$ is a homomorphism such that $\varphi := g \mapsto g^{2}$. Again with the verification:
                \begin{equation*}
                    \varphi(g_{1}g_{2}) = g_{1}g_{2}g_{1}g_{2} = g_{1}g_{1}g_{2}g_{2} = \varphi(g_{1})\varphi(g_{2})
                \end{equation*}
            By taking inverses of both sides, we get:
                \begin{equation*}
                    g_{1}g_{2}g_{1}g_{2} = g_{1}g_{1}g_{2}g_{2} \implies g_{1}g_{2} = g_{2}g_{1}
                \end{equation*}
            ($\leftarrow $) The other way can be seen by how the previous part was done.
        \end{proof}

    \textbf{Exercise 4.8}: Let $G$ be a group, and let $g \in G$. Prove that the function $\gamma_{g} : G \rightarrow G$ defined by $(\forall a \in G) : \gamma_{g}(a) = gag^{-1}$ is an automorphism of $G$. (The automorphisms $\gamma_{g}$ are called `inner' automorphisms of $G$). Prove that the function $G \rightarrow \text{Aut}(G)$ defined by $g \mapsto \gamma_{g}$ is a homomorphism. Prove that this homomorphism is trivial if and only if $G$ is abelian.
        \begin{proof}
            (Part I) This function is a group homomorphism because if we have $g_{1}, g_{2}$:
                \begin{equation*}
                    \gamma_{g}(g_{1}g_{2}) = gg_{1}g_{2}g^{-1} = gg_{1}g^{-1}gg_{2}g^{-1} = \gamma_{g}(g_{1})\gamma_{g}(g_{2})
                \end{equation*}
            Also, it has an inverse homomorphism which we have as $\gamma_{g^{-1}}(a) := g^{-1}ag$.

            (Part II) The second part is a verification. Let $\sigma : G \rightarrow \text{Aut}(G)$ be defined as $\sigma(g) := g \mapsto \gamma_{g}$. Then we have for two elements in $G$:
                \begin{equation*}
                    \sigma(g_{1}g_{2})(a) = \gamma_{g_{1}g_{2}}(a) = g_{1}g_{2}ag_{2}^{-1}g_{1} = \gamma_{g_{1}}\gamma_{g_{2}}(a) = \sigma(g_{1})\sigma(g_{2})(a)
                \end{equation*}
            Now if the group is abelian, we have that $\gamma_{g} := g \mapsto gag^{-1} = agg^{-1} = a$. Therefore, the function $\gamma_{g} = eae^{-1}$, so the morphism $\sigma : G \rightarrow \text{Aut}(G)$ is a trivial homomorphism.
        \end{proof}

    \textbf{Exercise 4.9}: Prove that if $m, n$ are positive integers such that $\gcd(m, n)$ = 1, then $C_{mn} \cong C_{m} \times C_{n}$.
        \begin{proof}
            Let $\varphi : C_{mn} \rightarrow C_{m} \times C_{n}$ be the morphism such that $\varphi := z \rightarrow ([z]_{m}, [z]_{n})$. We prove that it is a homomorphism: 
                \begin{equation*}
                    \varphi(g_{1}g_{2}) = ([g_{1}g_{2}]_{m}, [g_{1}g_{2}]_{n}) = ([g_{1}]_{m}, [g_{1}]_{m})([g_{2}]_{n}, [g_{2}]_{n}) = \varphi(g_{1})\varphi(g_{2})
                \end{equation*}
            Now to prove surjectivity, we require that for arbitrary $z_{1}, z_{2}$, we have a $z$ such that:
                \begin{equation*}
                    \varphi(z) = ([z_{1}]_{m}, [z_{2}]_{n})
                \end{equation*}
            So we require that 
                \begin{align*}
                    z &\equiv  z_{1} \pmod{m} & z &\equiv  z_{2} \pmod{n}   
                \end{align*}
            So this comes down to the Chinese Remainder Theorem:
                \begin{align*}
                    z &\equiv  1 \pmod{m} & z &\equiv  0 \pmod{m} \\
                    z &\equiv  0 \pmod{n} & z &\equiv  1 \pmod{n}   
                \end{align*}
            Since we know that $\gcd(n, m) = 1$, $n$ is a generator in $\mathbb{Z}/m\mathbb{Z}$. We can pick an $0 \leq a_{1} < m$ such that $a_{1}n \equiv 1 \pmod{m}$ and simultaneously, $a_{1}n \equiv 0 \pmod{n}$. Furthermore, the same can be said for the RHS system of equations. Therefore, we have $z_{1}a_{1}n + z_{2}a_{2}m \equiv z_{1} \pmod{m}$ and $z_{1}a_{1}n + z_{2}a_{2}m \equiv z_{2} \pmod{n}$. Therefore, the mapping is surjective. Since the cardinalities of the groups are equal, it is bijective and therefore, an isomorphism.
        \end{proof}

    \textbf{Exercise 4.10}: Let $p \neq q$ be odd prime integers; show that $(\mathbb{Z}/pq\mathbb{Z})^{*}$ is not cyclic.
        \begin{proof}
            We can start by looking at an example of the group $(\mathbb{Z}/15\mathbb{Z})^{*}$:
                \begin{equation*}
                    (\mathbb{Z}/15\mathbb{Z})^{*} = \{[1], [2], [4], [7], [8], [11], [13], [14]\}
                \end{equation*}
            Notice that we remove all multiples of 3 and 5. In other words, we consider the order of the groups $(\mathbb{Z}/3\mathbb{Z})^{*}$ and $(\mathbb{Z}/5\mathbb{Z})^{*}$ to see how many multiples we remove:
                \begin{align*}
                    (\mathbb{Z}/3\mathbb{Z})^{*} &= \{[1], [2]\}                \\
                    (\mathbb{Z}/5\mathbb{Z})^{*} &= \{[1], [2], [3], [4]\}   
                \end{align*}
            Notice that we can remove exactly $\lvert (\mathbb{Z}/3\mathbb{Z})^{*} \rvert$ multiples of $5$ and $\lvert (\mathbb{Z}/5\mathbb{Z})^{*} \rvert$ multiples of $3$ in the set 
                \begin{equation*}
                    \{[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14]\}
                \end{equation*}
            This gives us the formula for the cardinality of $(\mathbb{Z}/15\mathbb{Z})^{*}$ or $(\mathbb{Z}/pq\mathbb{Z})^{*}$ in general:
                \begin{equation*}
                    \lvert (\mathbb{Z}/pq\mathbb{Z})^{*} \rvert = (\mathbb{Z}/pq\mathbb{Z})^{+} - 1 - \lvert (\mathbb{Z}/p\mathbb{Z})^{*} \rvert - \lvert (\mathbb{Z}/q\mathbb{Z})^{*} \rvert
                \end{equation*}
            We know that both $(\mathbb{Z}/p\mathbb{Z})^{*}$ and $(\mathbb{Z}/q\mathbb{Z})^{*}$ are cyclic because $p, q$ are prime. Therefore, their orders are $p - 1$ and $q - 1$ respectively. So this gives:
                \begin{align*}
                    \lvert (\mathbb{Z}/pq\mathbb{Z})^{*} \rvert &= pq - 1 - (p - 1) - (q - 1) \\
                                                                &= pq - 1 - p + 1 - q + 1     \\
                                                                &= (p - 1)(q - 1)               
                \end{align*}
            Now that we have found the order of $(\mathbb{Z}/pq\mathbb{Z})^{*}$, we can prove that it is not cyclic by showing that no element in it has order $(p - 1)(q - 1)$. Consider the fact that the order of an element $g$ divides the order of the group. Therefore, we have $\text{ord}(g) \divides (p - 1)(q - 1)$. Now suppose that we did have an element with order $(p - 1)(q - 1)$:
            \begin{equation*}
                g^{(p - 1)(q - 1)} = e
            \end{equation*}
            Notice that this is actually impossible. We note that $\gcd(g, p) = 1$ and $\gcd(g, q) = 1$. This gives us:
            \begin{align*}
                g^{(p - 1)} &\equiv  e \pmod{p} \\
                g^{(q - 1)} &\equiv   e \pmod{q}   
            \end{align*}
            Therefore, we choose the lcm of $(p - 1)$, $(q - 1)$, since then we will have both factors: $\gcd(g^{\lcm((p - 1), (q - 1))}, pq) = 1$! But since $p - 1$ and $q - 1$ are even, as both $p, q$ are odd, they share a factor of 2. So
            \begin{equation*}
                \lcm((p - 1), (q - 1)) \neq (p - 1)(q  - 1)
            \end{equation*}
        \end{proof}

    \textbf{Exercise 4.11}: In due time we will prove the easy fact that if $p$ is a prime integer, then the equation $x^{d} = 1$ can have at most $d$ solutions in $\mathbb{Z}/p\mathbb{Z}$. Assume this fact, and prove that the multiplicative group $G = (\mathbb{Z}/p\mathbb{Z})^{*}$ is cyclic.
        \begin{proof}
            Suppose that we have an element of maximal order $g$ with order $\lvert g \rvert$. Since multiplication is commutative, our group $(\mathbb{Z}/p\mathbb{Z})^{*}$ is commutative. This means that for all $h \in G$, $h^{\lvert g \rvert} = e$. Using the fact that the equation $x^{d} = 1$ has at most $d$ solutions in $\mathbb{Z}/p\mathbb{Z}$, we conclude that $\lvert g \rvert \geq \lvert G \rvert$. We also know that the order of $g$ cannot be greater than $\lvert G \rvert$ because if we consider the chain:
            \begin{equation*}
                g, g^{2}, g^{3}, \ldots, g^{\lvert G \rvert}, g^{\lvert G \rvert + 1}
            \end{equation*}
        the number of unique elements of this listing cannot exceed the order of the group or $\lvert G \rvert$. Therefore, we have $\lvert g \rvert \leq \lvert G \rvert$ also. Thus, $\lvert g \rvert = \lvert G \rvert$ and we can conclude that the group is isomorphic to $\mathbb{Z}/(p - 1)\mathbb{Z}$
        \end{proof}

    \textbf{Exercise 4.12}: Compute the order of $[9]_{31}$ in the group $(\mathbb{Z}/31\mathbb{Z})^{*}$. Does the equation $x^{3} - 9 = 0$ have solutions in $\mathbb{Z}/31\mathbb{Z}$?
        \begin{proof}
            (Part I) We know that $31$ is prime. Therefore, the order of $[9]_{31}$ is a factor of $30$, the order of the group. So the order is $2, 3, 5, 6, 10$, or $15$:
            \begin{align*}
                9^{2}  &= 81 \cong 19  \\
                19 * 9 &= 171 \cong 16 
            \end{align*}
            So the order is not $2$ or $3$. For $6$, we take $16^{2} = 256 \cong 8$, so it cannot be 5 either. Now for $10, 15$:
            \begin{align*}
                16 * 9 &= 144 \cong 20 \\
                20 * 9 &= 180 \cong 25 \\
                25^{2} &= 625 \cong 5  \\
                25 * 5 &= 125 \cong 1    
            \end{align*}
            The second line calculates $9^{5}$, the third, $9^{10}$, the fourth $9^{15}$. Therefore, the order is $15$.

            (Part II) To solve the equation we rewrite it to 
                \begin{equation*}
                    x^{3} = 9
                \end{equation*}
            Then we see that $x^{3}$ has order $15$. But we know that the order of $x^{3}$ is the lcm of the order of $\lvert x \rvert$ and $3$. Therefore, we have $15 = \frac{\lcm(3, d)}{3}$, where $d$ is the order of $x$. This means that $d = 45$, which is impossible. 
        \end{proof}

    \textbf{Exercise 4.13}: Prove that $\text{Aut}_{\text{Grp}}(\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}) \cong S_{3}$.
        \begin{proof}
            Consider the group $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ represented as the set:
                \begin{equation*}
                    \mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z} = \{e, r, s, rs\}
                \end{equation*}
            where $r, s$ have order 2 and $rs = sr$. Notice that if we consider the set $\{r, s, rs\}$, we have that the product of any two elements is equal to the third element. This can be checked by casework. Now if we consider the bijections between this set and itself, we would like to consider specifically the homomorphisms, so it is required that:
                \begin{equation*}
                    e \mapsto e
                \end{equation*}
            Now the other mappings are determined by what two elements map to. For $r$ and $s$, choose an arbitrary but different mapping from $r$ and $s$ to something in $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$. This will be a homomorphism. We must show that for any $g_{1}, g_{2}$ in the set, we have:
                \begin{equation*}
                    \varphi(g_{1}g_{2}) = \varphi(g_{1})\varphi(g_{2})
                \end{equation*}
            Certainly, this is true. Since $g_{1} \neq g_{2}$, we can say that $\varphi(g_{1}) = h_{1}$ and $\varphi(g_{2}) = h_{2}$ where $h_{1} \neq h_{2}$. Since $g_{1}g_{2} \neq g_{1} \lor g_{2}$, we have that it must be the third element of the set $g_{3}$. This means that $g_{3}$ must be sent to $\varphi(h_{3})$. Now we consider what $h_{1}h_{2}$ is. It is just $h_{3}$ because as we have established from the group, it cannot be equal to $h_{1} \lor h_{2}$ nor can it be the identity since $h_{1} \neq h_{2}$. We have a way to find all homomorphisms. Since these are also bijective, we have the isomorphisms and therefore the elements of $\text{Aut}_{\text{Grp}}(\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z})$. Notice that this shows the isomorphism to $S_{3}$. The identity is fixed, and we find all ways to permute the mappings of the three elements to themselves. So we can label each element of $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ as $1, 2, 3$ and consider it as the permutations of these three. Since the orders of the sets are equal and there is an injective mapping, it must be a bijection. In fact, we have established that this is an isomorphism because of the similar nature of the construction.
        \end{proof}

    \textbf{Exercise 4.14}: Prove that the order of the group of automorphisms of a cyclic group $C_{n}$ is the number of positive integers $r < n$ that are relatively prime to $n$. (This is called Euler's $\varphi-$ function).
        \begin{proof}
            Suppose that we have a cyclic group $C_{n}$ denoted as:
                \begin{equation*}
                    C_{n} = \{0, 1, 2, 3, \ldots, n - 1\}
                \end{equation*}
            Notice now that all homomorphisms are determined by what the $1$ element is sent to, as it generates the group. So therefore, we have $n$ options. Clearly, we cannot have $1 \mapsto 0$, as we must have $0 \mapsto 0$ and this would not form a bijection. Suppose we have $1 \mapsto p$ where $\gcd(p, n) \neq 1$. Then we observe that the equation:
                \begin{equation*}
                    pp^{\prime} \cong 1 \pmod{n}
                \end{equation*}
            has no solution because of Bezout's Theorem. This means that the range of our homomorphism does not contain 1. Therefore, it cannot be a bijection. Now what about when $\gcd(k, n) = 1$? Observe that by Bezout's Theorem, there is an inverse $m$ such that:
                \begin{equation*}
                    mk \cong 1 \pmod{n}
                \end{equation*}
            So we do in fact have a surjective mapping. Is it injective? Since the size of the sets are equal, it must be bijective. It is also a homomorphism. We need to verify that for $g_{1}, g_{2} \in C_{n}$, we have that $\varphi(g_{1}g_{2}) = \varphi(g_{1})\varphi(g_{2})$. We first note that $1 \mapsto k$. So We have:
                \begin{equation*}
                    \varphi(g_{1}g_{2}) = (g_{1} + g_{2})k = g_{1}k + g_{2}k = \varphi(g_{1})\varphi(g_{2})
                \end{equation*}
            Therefore, we have an isomorphism, as desired. 
        \end{proof}

    \textbf{Exercise 4.15}: Compute the group of automorphisms of $(\mathbb{Z}, +).$ Prove that if $p$ is prime, then $\text{Aut}_{\text{Grp}}(C_{p}) \cong C_{p - 1}$.
        \begin{proof}
            We must have $0 \mapsto 0$. Now we must have all automorphisms of the form:
                \begin{equation*}
                    a \mapsto an
                \end{equation*}
            for some $n \in Z$. We consider the fact that every automorphism has a positive smallest element in the codomain. We let this be $n$ as seen above. If not all morphisms are not of that form, we must have some $a \mapsto an + r$ where $0 < r \leq n - 1$. We can perform the division algorithm because we have inverses. But then we must have that $r$ is in the codomain. This leads to a contradiction.
        \end{proof}

    \textbf{Exercise 4.16}: Prove \textit{Wilson's theorem}: a positive integer $p$ is prime if and only if
        \begin{equation*}
            (p - 1)! \equiv -1 \pmod{p}.
        \end{equation*}

    \textbf{Exercise 4.17}: For a few small (but not too small) primes $p$, find a generator of $(\mathbb{Z}/p\mathbb{Z})^{*}$.

    \textbf{Exercise 4.18}: Prove the second part of Proposition 2.4.8.
\end{exercises}

\begin{topic}
    \section{Free Groups}
\end{topic}














\end{document}
