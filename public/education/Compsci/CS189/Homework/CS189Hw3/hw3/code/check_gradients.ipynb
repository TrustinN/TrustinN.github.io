{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would highly recommend looking at `neural_networks.grad_check.check_gradients` and making sure you understand how numerical gradient checking is being carried out. This function is used in the notebook to check the gradients of the neural network layers you write. Make sure to check the gradient of a layer after finishing its implementation.\n",
    "\n",
    "The function returns the relative error of the numerical gradient (approximated using finite differences) with respect to the analytical gradient (computed via backpropagation). Correct implementations should get very small errors, usually less than `1e-8` for 64-bit float matrices (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from neural_networks.utils import check_gradients\n",
    "from neural_networks.layers import FullyConnected\n",
    "from neural_networks.activations import Linear, ReLU, SoftMax\n",
    "from neural_networks.losses import CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for linear activation: 1.7257824629296002e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "linear_activation = Linear()\n",
    "_ = linear_activation.forward(X)\n",
    "grad = linear_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for linear activation:\",\n",
    "    check_gradients(\n",
    "        fn=linear_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for relu activation: 2.0922241164025358e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "relu_activation = ReLU()\n",
    "out = relu_activation.forward(X)\n",
    "grad = relu_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for relu activation:\",\n",
    "    check_gradients(\n",
    "        fn=relu_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for softmax activation: 1.3479786999838347e-10\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "softmax_activation = SoftMax()\n",
    "_ = softmax_activation.forward(X)\n",
    "grad = softmax_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for softmax activation:\",\n",
    "    check_gradients(\n",
    "        fn=softmax_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Full Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer (Linear Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for W: 1.9973623964560514e-11\n",
      "Relative error for b: 9.456720680851424e-12\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer (ReLU Activation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m fc_layer \u001b[38;5;241m=\u001b[39m FullyConnected(n_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m _ \u001b[38;5;241m=\u001b[39m fc_layer\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m----> 8\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mfc_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLdY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# check the gradients w.r.t. each parameter\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m fc_layer\u001b[38;5;241m.\u001b[39mparameters:\n",
      "File \u001b[0;32m~/Downloads/Berkeley/Compsci/CS189/Homework/CS189Hw3/hw3/code/neural_networks/layers.py:172\u001b[0m, in \u001b[0;36mFullyConnected.backward\u001b[0;34m(self, dLdY)\u001b[0m\n\u001b[1;32m    169\u001b[0m dG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m], dLdY)\n\u001b[1;32m    171\u001b[0m dX \u001b[38;5;241m=\u001b[39m dLdY \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 172\u001b[0m dW \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdLdY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdG\u001b[49m\n\u001b[1;32m    173\u001b[0m db \u001b[38;5;241m=\u001b[39m dLdY\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mones(dLdY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m dG\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# store the gradients in `self.gradients`\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# the gradient for self.parameters[\"W\"] should be stored in\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# self.gradients[\"W\"], etc.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 4)"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"relu\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26767904 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768166 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211302 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211649 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422079 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422484 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54013577]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.5401451 ]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984874  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984896  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60649963 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650329 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807359 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807602 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601675]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49602003]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40376936 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40379149 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705263 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705466 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75743719 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75746871 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.0442331 ]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424855]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59291126 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.592938   -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54269837 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270772 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08335856 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336446 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82340764]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82342003]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90319932 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320426 -0.81215962 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215737 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81216187 -0.27920956 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920824 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27921088 -1.1795523 ]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.17954904]]\n",
      "[[-0.26768035 -0.55211475 -0.70422281 -1.54014043]\n",
      " [-0.0984885  -0.60650146 -0.19807481 -0.49601839]\n",
      " [-2.40378043 -0.01705365 -2.75745295 -2.04424083]\n",
      " [-2.59292463 -1.54270305 -1.08336151 -1.82341383]\n",
      " [-0.90320179 -0.81215962 -0.27920956 -1.17955555]]\n",
      "Relative error for relu activation: 2.9032304435200326e-11\n"
     ]
    }
   ],
   "source": [
    "random_indices = np.random.randint(0, 4, size=5)\n",
    "Y = np.eye(4)[random_indices]\n",
    "Y_hat = np.random.rand(5, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "cross_entropy = CrossEntropy(\"cross_entropy\")\n",
    "_ = cross_entropy.forward(Y, Y_hat)\n",
    "grad = cross_entropy.backward(Y, Y_hat)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for relu activation:\",\n",
    "    check_gradients(\n",
    "        fn=lambda x: cross_entropy.forward(Y, x),  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=Y_hat,    # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=1,     # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
